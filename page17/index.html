<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Page 17 of 25 for Tom Ordonez | A blog about analytics, data science, software engineering, and other thoughts.</title>
<meta name="generator" content="Jekyll v4.1.0" />
<meta property="og:title" content="Tom Ordonez" />
<meta name="author" content="Tom Ordonez" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A blog about analytics, data science, software engineering, and other thoughts." />
<meta property="og:description" content="A blog about analytics, data science, software engineering, and other thoughts." />
<link rel="canonical" href="https://www.tomordonez.com/page17/" />
<meta property="og:url" content="https://www.tomordonez.com/page17/" />
<meta property="og:site_name" content="Tom Ordonez" />
<link rel="prev" href="https://www.tomordonez.com/page16" />
<link rel="next" href="https://www.tomordonez.com/page18" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tom Ordonez" />
<script type="application/ld+json">
{"@type":"WebPage","headline":"Tom Ordonez","url":"https://www.tomordonez.com/page17/","author":{"@type":"Person","name":"Tom Ordonez"},"description":"A blog about analytics, data science, software engineering, and other thoughts.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://www.tomordonez.com/feed.xml" title="Tom Ordonez" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-56050409-28', 'auto');
  ga('send', 'pageview');
}
</script>

<link rel="apple-touch-icon" sizes="180x180" href="/assets/fav/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/fav/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/fav/favicon-16x16.png">
  <link rel="manifest" href="/assets/fav/site.webmanifest">
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Tom Ordonez</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="https://www.linkedin.com/in/tomordonez/">Linkedin</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <script src="/assets/js/lunr.js"></script>

<script>

var documents = [{
    "id": 0,
    "url": "https://www.tomordonez.com/404.html",
    "title": "Not Found",
    "body": "Black Hole: Sorry. You fell into a black hole. The page you are looking for was divided by zero.  "
    }, {
    "id": 1,
    "url": "https://www.tomordonez.com/",
    "title": "",
    "body": ""
    }, {
    "id": 2,
    "url": "https://www.tomordonez.com/sourcing-with-google-maps.html",
    "title": "Sourcing with Google Maps API",
    "body": "Google has a cornucopia of data services. Someone said I should have more powerful words in my vocabulary, so there you go Grammarly. I yet have to find a way to use “rubric” in a sentence. I prefer the term Rubik as in Rubik’s cube. Google API is a Rubik’s cube. This year I took a 3-mo Bootcamp in big data and data visualization. It had Java, Python, Scala, JavaScript, Tableau, Hadoop, Spark, AWS, and Azure. It wasn’t a programming class. I was supposed to know already these technologies to be able to do the assignments. I spent countless hours every week of my free time reading, studying, and practicing. Then during the summer, I learned Power BI, GCP, and became a Certified ScrumMaster. Now that I bragged about my insatiable desire for learning, here is a hint. You don’t have to learn all these technologies. Examples: I love it when tutorials start with examples. Let’s skip the setup for now, so I don’t lose you. I am using the Google Maps API to find some data. Just to test a response. Let’s search the id that corresponds to an address and store the result in a variable: &gt;&gt;&gt; place = gmaps. find_place(input= 1600 Amphitheatre Parkway, Mountain View, CA , input_type= textquery )Now let’s see what’s stored in this variable: &gt;&gt;&gt; place{'candidates': [{'place_id': 'ChIJtYuu0V25j4ARwu5e4wwRYgE'}], 'status': 'OK'}It returns the ID that corresponds to this address. Not impressed, right? Let’s search for a steakhouse within 2000 meters of Google’s HQ office. This is the command: &gt;&gt;&gt; place = gmaps. find_place(input= steakhouse , input_type= textquery , fields=[ name ,  formatted_address ,  business_status ,  geometry ,  photos ,  types ], location_bias= circle:2000@37. 4222339,-122. 0854804 , language= en )It returns the following: &gt;&gt;&gt; place{'candidates': [{'business_status': 'CLOSED_TEMPORARILY', 'formatted_address': 	'545 San Antonio Rd Suite 31, Mountain View, CA 94040, United States', 'geometry': {'location': {'lat': 37. 4032079, 'lng': -122. 1118804}, 'viewport': {'northeast': 	{'lat': 37. 40471237989271, 'lng': -122. 1105114701073}, 'southwest': {'lat': 37. 40201272010727, 'lng': -12. 1132111298927}}}, 'name':  Paul Martin's America Mountain View , 'photos': [{'height': 4048, 'html_attributions': ['&lt;a href= https://maps. google. com/maps/contrib/11203600442766896285 &gt;Casey DuBose&lt;/a&gt;'], 'photo_reference': 'CmRaAAAAit6MjPA4tMxwkAx61ZquIzBYndTl5zAcCV-bjUPhl0dm0S3giXjEANdqAvxvxsJvCehIChMcOCPVJwxIzHAQWW9Igv01P_R-gilhmU52I0MSRgBgWXh4g5N7wRQDPQKEhC2y0uoOD03_XFjS6o7xi0UGhTiLFIHq8rbIF68PRZaCoEjumKy_Q', 'width': 3036}], 'types': ['restaurant', 'food', 'point_of_interest', 'establishment']}], 'status': 'OK'}It found a place called “Paul Martin’s America Mountain View. ” Let’s confirm and google this place. On Google maps, the result shows:  Temporarily closed 545 San Antonio Rd Suite 31, Mountain View, CA 94040Ideas: Now let’s think of ideas and use cases. These are the API services available:  Places Directions Geocoding Geolocation RoadsIf you use “Places”, you can send these types of requests:  Find Place Nearby search Text searchWith “Find Place” requests, you can use a query parameter like name, address, or phone number. Think of the possibilities. You could try a request like: Find anything near anywhere to visualize the data on a map. With “Nearby Search” requests, you can search by keyword near a latitude and longitude. I tried this: &gt;&gt;&gt; place = gmaps. places_nearby(location=(37. 4222339,-122. 0854804), radius=5000, keyword= startup , language= en )After some Python code to extract the data, the result is this: ['BootUp Ventures: Startup Ecosystem Co-Working, Office Suites &amp; Event Space', 'Startup Capital Ventures', 'Startup Rabbit', 'Cab Startup', 'Startup Launchpad, Inc', 'BootUpWorld', 'Startup Realty', 'Plug and Play Tech Center', 'Mercury', 'Palo-Alto Startup House', 'Bay Area Startups Services, Inc', 'Nuro', 'The Hive', 'Starship Technologies', EquityBee', 'Fyde', 'HelloStartups', 'sFoundation Inc. ', 'Y Combinator', 'Osaka Innovation Hub Silicon Valley Office']These are the results for requesting anything with the keyword “startup” within 5000 meters of Google’s HQ. Dig a hole: I looked up “BootUpWorld,” and it seems like a hybrid Bootcamp/coworking space. The website has interesting directory pages with Linkedin profiles. This one is cool. “Palo-Alto Startup House. ” It looks just like the house from the Silicon Valley show. Always blue, always blue, always blue. Here is another one, “Plug and Play Tech Center. ” Mmm interesting. It has a Wikipedia page, and it says they were early investors in Google, Paypal, Dropbox, etc. Geocoding data: If you love maps as I do. You can convert an address to latitude and longitude. It’s pretty easy to do one at a time by looking up the lat/long directly into Google Maps. However, what if you want to get the data from hundreds of addresses. Try this command: &gt;&gt;&gt; place = gmaps_geo. geocode(address= 1600 Amphitheatre Parkway, Mountain View, CA )Here is what some of the result shows &gt;&gt;&gt; place. . . 'location': {'latitude': 37. 4213102, 'longitude': -122. 0852443}With an additional effort in Python you can requests streams of data and export them to CSV files. You can then use this data for insights, or to dig more holes to find treasures. If you found sourcing with Google Maps interesting or if you have questions about how to set this up. Feel free to message me on Linkedin "
    }, {
    "id": 3,
    "url": "https://www.tomordonez.com/previous-directory-vim-commands.html",
    "title": "",
    "body": ""
    }, {
    "id": 4,
    "url": "https://www.tomordonez.com/from-wordpress-to-pelican-python.html",
    "title": "",
    "body": ""
    }, {
    "id": 5,
    "url": "https://www.tomordonez.com/ms-data-science-coursera-udemy-udacity.html",
    "title": "",
    "body": ""
    }, {
    "id": 6,
    "url": "https://www.tomordonez.com/review-programming-for-everybody-coursera.html",
    "title": "",
    "body": ""
    }, {
    "id": 7,
    "url": "https://www.tomordonez.com/bash-script-tutorial.html",
    "title": "",
    "body": ""
    }, {
    "id": 8,
    "url": "https://www.tomordonez.com/test-your-site-before-it-goes-live.html",
    "title": "",
    "body": ""
    }, {
    "id": 9,
    "url": "https://www.tomordonez.com/disable-touchscreen-ubuntu.html",
    "title": "",
    "body": ""
    }, {
    "id": 10,
    "url": "https://www.tomordonez.com/disable-touchscreen-ubuntu.html",
    "title": "",
    "body": ""
    }, {
    "id": 11,
    "url": "https://www.tomordonez.com/ubuntu-wifi-network-disconnected-after-sleep.html",
    "title": "",
    "body": ""
    }, {
    "id": 12,
    "url": "https://www.tomordonez.com/automating-tasks-crontab.html",
    "title": "",
    "body": ""
    }, {
    "id": 13,
    "url": "https://www.tomordonez.com/crontab-selenium-chrome-driver.html",
    "title": "",
    "body": ""
    }, {
    "id": 14,
    "url": "https://www.tomordonez.com/generate-random-number-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 15,
    "url": "https://www.tomordonez.com/installing-ruby-on-ubuntu.html",
    "title": "",
    "body": ""
    }, {
    "id": 16,
    "url": "https://www.tomordonez.com/windows-10-to-ubuntu-virtualbox-shared-folder.html",
    "title": "",
    "body": ""
    }, {
    "id": 17,
    "url": "https://www.tomordonez.com/execute-script-different-directory-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 18,
    "url": "https://www.tomordonez.com/linux-execute-many-commands-one-line.html",
    "title": "",
    "body": ""
    }, {
    "id": 19,
    "url": "https://www.tomordonez.com/redirect-stdout-stderr-bash.html",
    "title": "",
    "body": ""
    }, {
    "id": 20,
    "url": "https://www.tomordonez.com/sourcing-twitter-api.html",
    "title": "",
    "body": ""
    }, {
    "id": 21,
    "url": "https://www.tomordonez.com/regular-expressions-tutorial-regex.html",
    "title": "",
    "body": ""
    }, {
    "id": 22,
    "url": "https://www.tomordonez.com/blackbelt-sourcing-github.html",
    "title": "",
    "body": ""
    }, {
    "id": 23,
    "url": "https://www.tomordonez.com/create-csv-from-linkedin-sent-invites.html",
    "title": "",
    "body": ""
    }, {
    "id": 24,
    "url": "https://www.tomordonez.com/touch-typing-z-type.html",
    "title": "",
    "body": ""
    }, {
    "id": 25,
    "url": "https://www.tomordonez.com/from-zero-to-hero-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 26,
    "url": "https://www.tomordonez.com/how-to-setup-vpn-linux-fedora.html",
    "title": "",
    "body": ""
    }, {
    "id": 27,
    "url": "https://www.tomordonez.com/top-5-python-scripts-sourcing.html",
    "title": "",
    "body": ""
    }, {
    "id": 28,
    "url": "https://www.tomordonez.com/terminate-ssh-connection.html",
    "title": "",
    "body": ""
    }, {
    "id": 29,
    "url": "https://www.tomordonez.com/create-a-regex-macro-in-sublime.html",
    "title": "",
    "body": ""
    }, {
    "id": 30,
    "url": "https://www.tomordonez.com/linkedin-xray-search-googler.html",
    "title": "",
    "body": ""
    }, {
    "id": 31,
    "url": "https://www.tomordonez.com/virtualbox-resize-linux-guest-storage-vdi.html",
    "title": "",
    "body": ""
    }, {
    "id": 32,
    "url": "https://www.tomordonez.com/additional-second-hard-drive-ubuntu.html",
    "title": "",
    "body": ""
    }, {
    "id": 33,
    "url": "https://www.tomordonez.com/open-source-python-crm-tutorial.html",
    "title": "",
    "body": ""
    }, {
    "id": 34,
    "url": "https://www.tomordonez.com/install-tmux-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 35,
    "url": "https://www.tomordonez.com/install-dropbox-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 36,
    "url": "https://www.tomordonez.com/aws-cli-help-page.html",
    "title": "",
    "body": ""
    }, {
    "id": 37,
    "url": "https://www.tomordonez.com/regex-remove-lines-not-containing-character.html",
    "title": "",
    "body": ""
    }, {
    "id": 38,
    "url": "https://www.tomordonez.com/install-react-native-mac.html",
    "title": "",
    "body": ""
    }, {
    "id": 39,
    "url": "https://www.tomordonez.com/file-iterations-python.html",
    "title": "",
    "body": ""
    }, {
    "id": 40,
    "url": "https://www.tomordonez.com/install-chromedriver-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 41,
    "url": "https://www.tomordonez.com/python-socket-save-data-to-file.html",
    "title": "",
    "body": ""
    }, {
    "id": 42,
    "url": "https://www.tomordonez.com/url-encoding-python.html",
    "title": "",
    "body": ""
    }, {
    "id": 43,
    "url": "https://www.tomordonez.com/python-self.html",
    "title": "",
    "body": ""
    }, {
    "id": 44,
    "url": "https://www.tomordonez.com/python-lambda-beautifulsoup.html",
    "title": "",
    "body": ""
    }, {
    "id": 45,
    "url": "https://www.tomordonez.com/best-time-to-run-according-to-science.html",
    "title": "",
    "body": ""
    }, {
    "id": 46,
    "url": "https://www.tomordonez.com/sqlite3-cheatsheet.html",
    "title": "",
    "body": ""
    }, {
    "id": 47,
    "url": "https://www.tomordonez.com/get-schema-sqlite-python.html",
    "title": "",
    "body": ""
    }, {
    "id": 48,
    "url": "https://www.tomordonez.com/python-socket-syntax.html",
    "title": "",
    "body": ""
    }, {
    "id": 49,
    "url": "https://www.tomordonez.com/dev-mapper-fedora-root-full.html",
    "title": "",
    "body": ""
    }, {
    "id": 50,
    "url": "https://www.tomordonez.com/cpp-indent-vim.html",
    "title": "",
    "body": ""
    }, {
    "id": 51,
    "url": "https://www.tomordonez.com/conference-mobile-apps.html",
    "title": "",
    "body": ""
    }, {
    "id": 52,
    "url": "https://www.tomordonez.com/linkedin-recruiter-bookmarklet.html",
    "title": "",
    "body": ""
    }, {
    "id": 53,
    "url": "https://www.tomordonez.com/install-plugin-vim.html",
    "title": "",
    "body": ""
    }, {
    "id": 54,
    "url": "https://www.tomordonez.com/r-tutorial-rstudio-data-analysis.html",
    "title": "",
    "body": ""
    }, {
    "id": 55,
    "url": "https://www.tomordonez.com/wget-download-files.html",
    "title": "",
    "body": ""
    }, {
    "id": 56,
    "url": "https://www.tomordonez.com/taskwarrior-task-management.html",
    "title": "",
    "body": ""
    }, {
    "id": 57,
    "url": "https://www.tomordonez.com/from-markdown-to-sphinx-restructuredtext.html",
    "title": "",
    "body": ""
    }, {
    "id": 58,
    "url": "https://www.tomordonez.com/install-sphinx-and-readthedocs.html",
    "title": "",
    "body": ""
    }, {
    "id": 59,
    "url": "https://www.tomordonez.com/reload-tmuxinator.html",
    "title": "",
    "body": ""
    }, {
    "id": 60,
    "url": "https://www.tomordonez.com/python-logging-tutorial.html",
    "title": "",
    "body": ""
    }, {
    "id": 61,
    "url": "https://www.tomordonez.com/screen-recording-linux-obs-studio.html",
    "title": "",
    "body": ""
    }, {
    "id": 62,
    "url": "https://www.tomordonez.com/python-context-manager.html",
    "title": "",
    "body": ""
    }, {
    "id": 63,
    "url": "https://www.tomordonez.com/pip-install-ssl-module-python-is-not-available.html",
    "title": "",
    "body": ""
    }, {
    "id": 64,
    "url": "https://www.tomordonez.com/installing-minecraft-server-mac.html",
    "title": "",
    "body": ""
    }, {
    "id": 65,
    "url": "https://www.tomordonez.com/bash-commands-for-productivity.html",
    "title": "",
    "body": ""
    }, {
    "id": 66,
    "url": "https://www.tomordonez.com/python-unit-testing-tutorial.html",
    "title": "",
    "body": ""
    }, {
    "id": 67,
    "url": "https://www.tomordonez.com/vim-stuck-insert-mode.html",
    "title": "",
    "body": ""
    }, {
    "id": 68,
    "url": "https://www.tomordonez.com/usb-stuck-read-only-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 69,
    "url": "https://www.tomordonez.com/web-scraping-with-python.html",
    "title": "",
    "body": ""
    }, {
    "id": 70,
    "url": "https://www.tomordonez.com/tableau-tutorial-visualization-co2-world-dataset.html",
    "title": "",
    "body": ""
    }, {
    "id": 71,
    "url": "https://www.tomordonez.com/sublime-text-typewriter-auto-scroll.html",
    "title": "",
    "body": ""
    }, {
    "id": 72,
    "url": "https://www.tomordonez.com/make-static-website-python-github-pages.html",
    "title": "",
    "body": ""
    }, {
    "id": 73,
    "url": "https://www.tomordonez.com/remote-tmux-inside-local-tmux.html",
    "title": "",
    "body": ""
    }, {
    "id": 74,
    "url": "https://www.tomordonez.com/my-vimrc-config.html",
    "title": "",
    "body": ""
    }, {
    "id": 75,
    "url": "https://www.tomordonez.com/learning-github.html",
    "title": "",
    "body": ""
    }, {
    "id": 76,
    "url": "https://www.tomordonez.com/remote-access-linux-fedora.html",
    "title": "",
    "body": ""
    }, {
    "id": 77,
    "url": "https://www.tomordonez.com/install-miniconda-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 78,
    "url": "https://www.tomordonez.com/build-android-app-linux-flutter.html",
    "title": "",
    "body": ""
    }, {
    "id": 79,
    "url": "https://www.tomordonez.com/install-java-fedora-openjdk-android.html",
    "title": "",
    "body": ""
    }, {
    "id": 80,
    "url": "https://www.tomordonez.com/troubleshooting-building-android-app.html",
    "title": "",
    "body": ""
    }, {
    "id": 81,
    "url": "https://www.tomordonez.com/increase-boot-size-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 82,
    "url": "https://www.tomordonez.com/python-jupyter-notebook-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 83,
    "url": "https://www.tomordonez.com/data-wrangling-openrefine-linux.html",
    "title": "",
    "body": ""
    }, {
    "id": 84,
    "url": "https://www.tomordonez.com/d3-tutorial-data-visualization.html",
    "title": "",
    "body": ""
    }, {
    "id": 85,
    "url": "https://www.tomordonez.com/d3-troubleshooting.html",
    "title": "",
    "body": ""
    }, {
    "id": 86,
    "url": "https://www.tomordonez.com/index2.html",
    "title": "",
    "body": ""
    }, {
    "id": 87,
    "url": "https://www.tomordonez.com/index3.html",
    "title": "",
    "body": ""
    }, {
    "id": 88,
    "url": "https://www.tomordonez.com/tag/",
    "title": "",
    "body": ""
    }, {
    "id": 89,
    "url": "https://www.tomordonez.com/category/",
    "title": "",
    "body": ""
    }, {
    "id": 90,
    "url": "https://www.tomordonez.com/pages/",
    "title": "",
    "body": ""
    }, {
    "id": 91,
    "url": "https://www.tomordonez.com/archives.html",
    "title": "",
    "body": ""
    }, {
    "id": 92,
    "url": "https://www.tomordonez.com/authors.html",
    "title": "",
    "body": ""
    }, {
    "id": 93,
    "url": "https://www.tomordonez.com/cf7_style/",
    "title": "",
    "body": ""
    }, {
    "id": 94,
    "url": "https://www.tomordonez.com/page2/",
    "title": "",
    "body": ""
    }, {
    "id": 95,
    "url": "https://www.tomordonez.com/page3/",
    "title": "",
    "body": ""
    }, {
    "id": 96,
    "url": "https://www.tomordonez.com/page4/",
    "title": "",
    "body": ""
    }, {
    "id": 97,
    "url": "https://www.tomordonez.com/page5/",
    "title": "",
    "body": ""
    }, {
    "id": 98,
    "url": "https://www.tomordonez.com/page6/",
    "title": "",
    "body": ""
    }, {
    "id": 99,
    "url": "https://www.tomordonez.com/page7/",
    "title": "",
    "body": ""
    }, {
    "id": 100,
    "url": "https://www.tomordonez.com/page8/",
    "title": "",
    "body": ""
    }, {
    "id": 101,
    "url": "https://www.tomordonez.com/page9/",
    "title": "",
    "body": ""
    }, {
    "id": 102,
    "url": "https://www.tomordonez.com/page10/",
    "title": "",
    "body": ""
    }, {
    "id": 103,
    "url": "https://www.tomordonez.com/page11/",
    "title": "",
    "body": ""
    }, {
    "id": 104,
    "url": "https://www.tomordonez.com/page12/",
    "title": "",
    "body": ""
    }, {
    "id": 105,
    "url": "https://www.tomordonez.com/page13/",
    "title": "",
    "body": ""
    }, {
    "id": 106,
    "url": "https://www.tomordonez.com/page14/",
    "title": "",
    "body": ""
    }, {
    "id": 107,
    "url": "https://www.tomordonez.com/page15/",
    "title": "",
    "body": ""
    }, {
    "id": 108,
    "url": "https://www.tomordonez.com/page16/",
    "title": "",
    "body": ""
    }, {
    "id": 109,
    "url": "https://www.tomordonez.com/page17/",
    "title": "",
    "body": ""
    }, {
    "id": 110,
    "url": "https://www.tomordonez.com/page18/",
    "title": "",
    "body": ""
    }, {
    "id": 111,
    "url": "https://www.tomordonez.com/page19/",
    "title": "",
    "body": ""
    }, {
    "id": 112,
    "url": "https://www.tomordonez.com/page20/",
    "title": "",
    "body": ""
    }, {
    "id": 113,
    "url": "https://www.tomordonez.com/page21/",
    "title": "",
    "body": ""
    }, {
    "id": 114,
    "url": "https://www.tomordonez.com/page22/",
    "title": "",
    "body": ""
    }, {
    "id": 115,
    "url": "https://www.tomordonez.com/page23/",
    "title": "",
    "body": ""
    }, {
    "id": 116,
    "url": "https://www.tomordonez.com/page24/",
    "title": "",
    "body": ""
    }, {
    "id": 117,
    "url": "https://www.tomordonez.com/page25/",
    "title": "",
    "body": ""
    }, {
    "id": 118,
    "url": "https://www.tomordonez.com/install-google-cloud-sdk/",
    "title": "Install Google Cloud SDK",
    "body": "2020/11/10 - Step by step installing Google Cloud SDK in Linux: $ curl -O https://dl. google. com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-315. 0. 0-linux-x86_64. tar. gzTar: $ tar -xvf google-cloud-sdk-315. 0. 0-linux-x86_64. tar. gzInstall the script: $ . /google-cloud-sdk/install. sh	Click to see output		Welcome to the Google Cloud SDK!	To help improve the quality of this product, we collect anonymized usage data and anonymized stacktraces when crashes are encountered; additional information is available at &lt;https://cloud. google. com/sdk/usage-statistics&gt;. This data is 	handled in accordance with our privacy policy	&lt;https://policies. google. com/privacy&gt;. You may choose to opt in this collection now (by choosing 'Y' at the below prompt), or at any time in the future by running the following command: 	gcloud config set disable_usage_reporting false	Do you want to help improve the Google Cloud SDK (y/N)? n	Your current Cloud SDK version is: 315. 0. 0	The latest available version is: 315. 0. 0	├───────────────┬──────────────────────────────────────────────────────┬──────────────────────────┬──────────┤	│   Status  │             Name             │      ID      │  Size  │	├───────────────┼──────────────────────────────────────────────────────┼──────────────────────────┼──────────┤	│ Not Installed │ App Engine Go Extensions               │ app-engine-go      │ 4. 9 MiB │	│ Not Installed │ Appctl                        │ appctl          │ 21. 0 MiB │	│ Not Installed │ Cloud Bigtable Command Line Tool           │ cbt           │ 7. 7 MiB │	│ Not Installed │ Cloud Bigtable Emulator               │ bigtable         │ 6. 6 MiB │	│ Not Installed │ Cloud Datalab Command Line Tool           │ datalab         │ &lt; 1 MiB │	│ Not Installed │ Cloud Datastore Emulator               │ cloud-datastore-emulator │ 18. 4 MiB │	│ Not Installed │ Cloud Firestore Emulator               │ cloud-firestore-emulator │ 42. 1 MiB │	│ Not Installed │ Cloud Pub/Sub Emulator                │ pubsub-emulator     │ 56. 3 MiB │	│ Not Installed │ Cloud SQL Proxy                   │ cloud_sql_proxy     │ 7. 5 MiB │	│ Not Installed │ Cloud Spanner Emulator                │ cloud-spanner-emulator  │ 21. 5 MiB │	│ Not Installed │ Emulator Reverse Proxy                │ emulator-reverse-proxy  │ 14. 5 MiB │	│ Not Installed │ Google Cloud Build Local Builder           │ cloud-build-local    │ 6. 3 MiB │	│ Not Installed │ Google Container Registry's Docker credential helper │ docker-credential-gcr  │ 1. 8 MiB │	│ Not Installed │ Kind                         │ kind           │ 4. 5 MiB │	│ Not Installed │ Kustomize                      │ kustomize        │ 25. 9 MiB │	│ Not Installed │ Minikube                       │ minikube         │ 24. 1 MiB │	│ Not Installed │ Nomos CLI                      │ nomos          │ 17. 8 MiB │	│ Not Installed │ Skaffold                       │ skaffold         │ 14. 5 MiB │	│ Not Installed │ anthos-auth                     │ anthos-auth       │ 16. 3 MiB │	│ Not Installed │ gcloud Alpha Commands                │ alpha          │ &lt; 1 MiB │	│ Not Installed │ gcloud Beta Commands                 │ beta           │ &lt; 1 MiB │	│ Not Installed │ gcloud app Java Extensions              │ app-engine-java     │ 59. 5 MiB │	│ Not Installed │ gcloud app Python Extensions             │ app-engine-python    │ 6. 1 MiB │	│ Not Installed │ gcloud app Python Extensions (Extra Libraries)    │ app-engine-python-extras │ 27. 1 MiB │	│ Not Installed │ kpt                         │ kpt           │ 11. 1 MiB │	│ Not Installed │ kubectl                       │ kubectl         │ &lt; 1 MiB │	│ Not Installed │ pkg                         │ pkg           │     │	│ Installed   │ BigQuery Command Line Tool              │ bq            │ &lt; 1 MiB │	│ Installed   │ Cloud SDK Core Libraries               │ core           │ 15. 4 MiB │	│ Installed   │ Cloud Storage Command Line Tool           │ gsutil          │ 3. 5 MiB │	To install or remove components at your current SDK version [315. 0. 0], run: $ gcloud components install COMPONENT_ID$ gcloud components remove COMPONENT_IDTo update your SDK installation to the latest version [315. 0. 0], run: $ gcloud components updateModify profile to update your $PATH and enable shell command completion? Do you want to continue (Y/n)? yThe Google Cloud SDK installer will now prompt you to update an rc file to bring the Google Cloud CLIs into your environment. Enter a path to an rc file to update, or leave blank to use: [/home/tom/. bashrc]:Backing up [/home/tom/. bashrc] to [/home/tom/. bashrc. backup]. [/home/tom/. bashrc] has been updated. ==&gt; Start a new shell for the changes to take effect. For more information on how to get started, please visit: https://cloud. google. com/sdk/docs/quickstarts Initialize $ . /google-cloud-sdk/bin/gcloud initWelcome! This command will take you through the configuration of gcloud. Your current configuration has been set to: [default] You can skip diagnostics next time by using the following flag: gcloud init --skip-diagnosticsNetwork diagnostic detects and fixes local network connection issues. Checking network connection. . . done. Reachability Check passed. Network diagnostic passed (1/1 checks passed). You must log in to continue. Would you like to log in (Y/n)? Your browser has been opened to visit: https://accounts. google. com/o/oauth2/auth. . . Login or choose an account. Then this page shows: https://cloud. google. com/sdk/auth_success Back on the terminal… You are logged in as: [your username shows here] Pick cloud project to use: [1] some-awesome-project [2] Create a new projectPlease enter numeric choice or text value (must exactly match list item): 1Your current project has been set to: [some-awesome-project]. 	More output		Not setting default zone/region (this feature makes it easier to use	[gcloud compute] by setting an appropriate default value for the	--zone and --region flag). 	See https://cloud. google. com/compute/docs/gcloud-compute section on how to set	default compute region and zone manually. If you would like [gcloud init] to be	able to do this for you the next time you run it, make sure the	Compute Engine API is enabled for your project on the	https://console. developers. google. com/apis page. 	Created a default . boto configuration file at [/home/tom/. boto]. See this file and	[https://cloud. google. com/storage/docs/gsutil/commands/config] for more	information about configuring Google Cloud Storage. 	Your Google Cloud SDK is configured and ready to use!	* Commands that require authentication will use `your-user-name` by default	* Commands will reference project `some-awesome-project` by default	Run `gcloud help config` to learn how to change individual settings	This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects. 	Run `gcloud topic configurations` to learn more. 	Some things to try next:  Run gcloud --help to see the Cloud Platform services you can interact with. And run gcloud help COMMAND to get help on any gcloud command.  Run gcloud topic --help to learn about advanced features of the SDK like arg files and output formatting. "
    }, {
    "id": 119,
    "url": "https://www.tomordonez.com/python-adding-days-date-python-pandas/",
    "title": "Python Adding Days to Date and Python Pandas",
    "body": "2020/11/09 - Adding days to a date in Python using datetime and timedelta. Use Python Pandas to concatenate dates with strings. I wanted to create 20 files with this syntax date-blog-post-name. I wrote a very extensive D3 Tutorial and wanted to break it down into smaller blog posts, published sequentially 5 days apart. The blog posts needed to have this naming syntax: YYYY-MM-DD-name-of-blog-post. mdI extracted the headlines H2 of the original blog post and made an array like this: &gt;&gt;&gt; posts = ['-setup-d3-step-by-step. md', '-d3-and-asynchronous. md', '-d3-and-incompatible-versions. md', '-d3-load-a-csv-file-with-promises. md', '-d3-convert-string-to-date. md', '-d3-bind-data-to-dom. md', '-d3-drawing-svg. md', '-d3-creating-a-bar-chart. md', '-d3-using-scales. md', '-d3-linear-scale. md', '-d3-band-scale. md', '-d3-scales-in-a-bar-chart. md', '-d3-responsive-visualization. md', '-d3-arrow-functions. md', '-d3-adding-axes-to-bar-chart. md', '-d3-bar-chart-title-and-labels. md', '-d3-visualization-margins. md', '-d3-mouse-event-handler. md', '-embedding-d3-in-a-website. md']How do I add the prefix dates? 2020-05-122020-05-17. . . To have files like these? 2020-05-12-setup-d3-step-by-step. md2020-05-17-d3-and-asynchronous. md. . . Adding days to a date in Python: Let’s get to it. &gt;&gt;&gt; import datetime&gt;&gt;&gt; datetime. date. today() + 5Error: TypeError: unsupported operand type(s) for +: 'datetime. date' and 'int'Should I then add the days to the day in the date? &gt;&gt;&gt; datetime. date. today(). day + 514This can’t be right. Adding days to date in Python with timedelta: Let’s look at the Python docs: datetime. Looking at timedelta A timedelta object represents a duration, the difference between two dates or times. You can do this: new_date = old_date + datetime. timedelta(days=N)With an example: &gt;&gt;&gt; import datetime&gt;&gt;&gt; today = datetime. date. today()datetime. date(2020, 11, 10)&gt;&gt;&gt; tomorrow = today + datetime. timedelta(days=1)&gt;&gt;&gt; tomorrowdatetime. date(2020, 11, 11)Let’s test this. What happens if I add 60 days: &gt;&gt;&gt; sixty_days = today + datetime. timedelta(days=60)&gt;&gt;&gt; sixty_daysdatetime. date(2021, 1, 9)Using isoformat: The syntax to create a date is: &gt;&gt;&gt; datetime. date(YYYY, M, D)Such as: &gt;&gt;&gt; post_date = datetime. date(2020, 5, 12)&gt;&gt;&gt; post_datedatetime. date(2020, 5, 12)However, I want the date string, not the weird date object. &gt;&gt;&gt; post_date. isoformat()'2020-05-12'Creating an array of dates, adding days to a date: Easy for loop: &gt;&gt;&gt; post_datedatetime. date(2020, 5, 12)&gt;&gt;&gt; date_list = []&gt;&gt;&gt; for post in range(1, 20):    post_date += datetime. timedelta(days=5)    date_list. append(post_date. isoformat())Then let’s see the output &gt;&gt;&gt; date_list['2020-05-12', '2020-05-17', '2020-05-22', '2020-05-27', '2020-06-01', '2020-06-06', '2020-06-11', '2020-06-16', '2020-06-21', '2020-06-26', '2020-07-01', '2020-07-06', '2020-07-11', '2020-07-16', '2020-07-21', '2020-07-26', '2020-07-31', '2020-08-05', '2020-08-10']Now I have a date_list array with dates and a posts array with blog post names. How do I join them? Python Pandas join strings or concatenate strings: Create two dataframes: &gt;&gt;&gt; import pandas&gt;&gt;&gt; date_list_df = pandas. DataFrame(date_list)&gt;&gt;&gt; posts_df = pandas. DataFrame(posts)The dataframes show the data under column 0: &gt;&gt;&gt; date_list_df	00  2020-05-121  2020-05-172  2020-05-223  2020-05-274  2020-06-015  2020-06-066  2020-06-11. . . &gt;&gt;&gt; posts_df                                                                                 	00	-setup-d3-step-by-step. md1	-d3-and-asynchronous. md2	-d3-and-incompatible-versions. md3	-d3-load-a-csv-file-with-promises. md4	-d3-convert-string-to-date. md5	-d3-bind-data-to-dom. md6	-d3-drawing-svg. md. . . I tried join but this didn’t work: &gt;&gt;&gt; date_list_df. join(posts_df)ValueError: columns overlap but no suffix specified: RangeIndex(start=0, stop=1, step=1)Add names to the columns: &gt;&gt;&gt; posts_df. columns = ['post_name']&gt;&gt;&gt; date_list_df. columns = ['date']Tried join again but this isn’t what I wanted to do: &gt;&gt;&gt; date_list_df. join(posts_df)     date         post_name0  2020-05-12 -setup-d3-step-by-step. md1  2020-05-17 -d3-and-asynchronous. md2  2020-05-22 -d3-and-incompatible-versions. md3  2020-05-27 -d3-load-a-csv-file-with-promises. md4  2020-06-01 -d3-convert-string-to-date. md5  2020-06-06 -d3-bind-data-to-dom. md6  2020-06-11 -d3-drawing-svg. mdI want to concatenate the date with the post name: &gt;&gt;&gt; posts_names = date_list_df['date']. str. cat(posts_df['post_name'])0  2020-05-12-setup-d3-step-by-step. md1  2020-05-17-d3-and-asynchronous. md2  2020-05-22-d3-and-incompatible-versions. md3  2020-05-27-d3-load-a-csv-file-with-promises. md4  2020-06-01-d3-convert-string-to-date. md5  2020-06-06-d3-bind-data-to-dom. md6  2020-06-11-d3-drawing-svg. mdThen create an array from the dataframe: &gt;&gt;&gt; posts_names_array = posts_names. arrayCopying and creating many files in Python: I wanted to do the same you do on the shell: $ cp source_file new_fileAnd repeat for all the posts names in posts_names_array. Use the Python modules os and shutil. &gt;&gt;&gt; import os&gt;&gt;&gt; import shutil&gt;&gt;&gt; dest_dir = '_posts/'&gt;&gt;&gt; source_post = '_posts/2020-02-02-d3-tutorial-data-visualization. md'Then go through the array: &gt;&gt;&gt; for post in posts_names_array:		shutil. copy2(source_post, dest_dir+post)Output: '_posts/2020-05-12-setup-d3-step-by-step. md''_posts/2020-05-17-d3-and-asynchronous. md''_posts/2020-05-22-d3-and-incompatible-versions. md''_posts/2020-05-27-d3-load-a-csv-file-with-promises. md''_posts/2020-06-01-d3-convert-string-to-date. md''_posts/2020-06-06-d3-bind-data-to-dom. md''_posts/2020-06-11-d3-drawing-svg. md'. . . "
    }, {
    "id": 120,
    "url": "https://www.tomordonez.com/responsive-testing-with-localhost-iphone/",
    "title": "Responsive Testing with Localhost and iPhone",
    "body": "2020/08/29 - I learned the hard way that the responsive design features from Chrome and Firefox developer tools are not an exact representation of the mobile interface. My development laptop runs on Linux Fedora, 8GB RAM, and just 64GB of disk. No way to run an iPhone simulator. I had Android installed but it was taking way too much space. I am on a quest to learn React Native to develop a set of ideas and get more hands-on experience developing apps, while also studying GCP, and following a product owner mindset. I recently completed a web design course with HTML, CSS, and JavaScript. I’ve been using Chrome and Firefox developer tools heavily to keep in mind the responsive design experience. For my last exercise in JavaScript I made a grid that changes color when you mouse over a cell. When you double click on a specific cell it changes to a different color. It works well on the browser: With Developer tools set to responsive design iPhone 6/7/8. Moving the mouse over the grid doesn’t do anything. However, you can click on a cell and the color changes. The behavior remains when you double click on a cell and it shows the correct programmed color.  Then I deployed the grid to my website. Tested this on Safari and the behavior was different. Touching a cell didn’t do anything. Double tapping the cell changed the color. Triple tapping the cell didn’t do anything. The behavior changed completely from web to mobile. Why isn’t there a mobile simulator in Linux? Android is way too heavy and can’t install Xcode. Access Localhost from iPhone: My next quest was a quick google away. I am sure you can access localhost from a phone. This took a lot of troubleshooting. The quick answer is found on this Stackoverflow post.  Find the local IP address of the computer From your phone browse to IP:PortIf your localhost runs like this: 127. 0. 0. 1:1234First you need to find the local IP number given to the laptop. Let’s say it is 192. 168. 1. 35. Then from mobile you can browse to: 192. 168. 1. 35:1234Port Number and Firewall: If this doesn’t work you can try enabling incoming traffic to the port number on your firewall. This is for GNOME/Linux: $ sudo firewall-cmd --staterunning$ firewall-cmd --helpUsage: firewall-cmd [OPTIONS. . . ]$ firewall-cmd --list-ports1025-65535/udp 1025-65535/tcp$ sudo firewall-cmd --add-port=1234/tcp$ firewall-cmd --list-ports1025-65535/udp 1025-65535/tcp 1234/tcp$ sudo firewall-cmd --remove-port=1234/tcp$ firewall-cmd --list-ports1025-65535/udp 1025-65535/tcpMore about firewalld here. Run localhost on 0. 0. 0. 0: This is what worked for me. Running the server on 0. 0. 0. 0. More about the difference between 0. 0. 0. 0 and 127. 0. 0. 1 here You can run your server like this: $ python -m http. server 1234 --bind 0. 0. 0. 0Find the IP number of the laptop. Let’s say it is 192. 168. 1. 35. Then from your mobile (iPhone or Android or any). Browse to: 192. 168. 1. 35:1234Now you can test your responsive design on your mobile.  "
    }, {
    "id": 121,
    "url": "https://www.tomordonez.com/setup-docker-google-cloud-platform-gcp/",
    "title": "Setup Docker in Google Cloud Platform GCP",
    "body": "2020/08/27 - I am studying the Google Cloud Platform. I’d like to take the Associate Cloud Engineer certificate in a few months. I heard about Docker for a while but never used it. Long story short. Goodbye Virtualbox. Create a VM instance:  Open a GCP account. Free trial has $300 credit to be used in 90 days Create a project awesome-proj (top left, next to GCP logo) Switch to this project (if your default project is something else) Add a VM instance (Search type VM. Click Add a VM instance) Instance Name: awesome-vm Right sidebar says “You have $300 free trial credits. $24. 67 monthly estimate” Labels: (nothing) Region: leave default Machine configuration Series: N1 (default) Machine configuration Machine type: Change to n1-standard-2: 2vCPU, 7. 5GB memory if you wish to run big data tutorials (the recommended/default option was n1-standard-1: 1vCPU, 3. 75GB memory) Sidebar price changed from $24. 67 monthly estimate to $48. 95 Container (checkbox): (nothing) Boot disk: Debian GNU/Linux 9 (default) Leave everything else default Click Create Check that the instance is created (green check mark)Connect to the instance: (using ssh on browser)  Go to Google Cloud Platform logo. This shows the Project dashboard Go to Resources. Click on Compute Engine &gt; 1 instance The VM instance is listed. Click on SSH A popup says Connecting. Transferring SSH keys to the VM.  It opens SSH on browser Use the top right icons to customize the shell, upload/download files, customize copy/pasteUpdate the instance $ sudo apt-get updateInstall Docker: Docker install details here $ sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common$ curl -fsSL https://download. docker. com/linux/debian/gpg | sudo apt-key add -$ sudo apt-key fingerprint 0EBFCD88$ sudo add-apt-repository  deb [arch=amd64] https://download. docker. com/linux/debian $(lsb_release -cs) stable $ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd. ioCheck the install(Need to use sudo to run docker commands) $ sudo service docker status$ sudo docker run hello-world$ sudo docker imagesSunlab Docker: This is an example of a container setup for Big Data. $ sudo docker run -it --privileged=true --cap-add=SYS_ADMIN -m 6144m -h bootcamp. local --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888 -v /:/mnt/host sunlab/bigbox:latest /bin/bashPlaying with Docker: The terminal prompt now is root: [root@bootcamp /]# whoamiroot[root@bootcamp /]# lsanaconda-post. log bigdata-bootcamp bin data dev etc home lib lib64 media mnt opt proc root run sbin scripts srv sys tini tmp usr varDetach the instance (keyboard shortcut one after the other fast) ctrl + p, ctrl + qReattach the instance… Why can’t I find the shell? mmm right. It’s SSH on the browser. $ sudo docker ps -aAttach the instance: $ sudo docker attach bigbox[root@bootcamp /]# lsanaconda-post. log bigdata-bootcamp bin data dev etc home lib lib64 media mnt opt proc root run sbin scripts srv sys tini tmp usr varStart Services [root@bootcamp /]# /scripts/start-services. shSome of the output: Starting zookeeper . . . STARTEDStarted Hadoop proxyserverStarted Hadoop namenodeStarted Hadoop datanode (hadoop-hdfs-datanode)Started Hadoop resourcemanagerStarted Hadoop historyserverStarted Hadoop nodemanagerStarting Spark worker (spark-worker)Starting Spark master (spark-master)Started HBase master daemon (hbase-master)Started HBase thrift daemon (hbase-thrift)Editing files: I read that you don’t have to install an editor in the Docker container. Here are two options:  Edit the file locally, copy to VM instance, copy from host to Docker container Edit the file in the VM instance with Vim, copy from host to Docker container. Edit file locally  Edit your file: awesome-file Detach the instance (if attached): ctrl+p, ctrl+q Copy to VM instance using SSH on browser, top right wheel icon, Upload file Copy file from host to container: sudo docker cp awesome-file bigbox:/bigdata-bootcamp/sample/hadoop/Edit file in the VM instance  Edit your file with Vim (if you dare to) Follow same process to copy from host to docker. "
    }, {
    "id": 122,
    "url": "https://www.tomordonez.com/embedding-d3-in-a-website/",
    "title": "Embedding D3 in a Website",
    "body": "2020/08/10 - How to embed a D3 visualization in a website or a blog post. My blog follows this directory structure: assets/ csv/  file. csv js/  d3. min. js  d3-barchart-vis. js_posts/ this-blog-post. mdAdd a div with a descriptive id to the section of your blog post where you want to add your visualization: &lt;div id= d3-barchart-vis &gt;&lt;/div&gt;In the d3-barchart-vis. js, the svg selects this &lt;div id: var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);The csv file is called like this so it points at the correct directory: d3. csv( . . /assets/csv/file. csv , rowConverter). then(function(dataset) {At the end of the blog post, after all content, add a reference to the JavaScript files: &lt;script type= text/javascript  src= . . /assets/js/d3. min. js &gt;&lt;/script&gt;&lt;script type= text/javascript  src= . . /assets/js/d3-barchart-vis. js &gt;&lt;/script&gt;D3 visualization in this blog post: "
    }, {
    "id": 123,
    "url": "https://www.tomordonez.com/d3-mouse-event-handler/",
    "title": "D3 Mouse Event Handler",
    "body": "2020/08/05 - Using mouse event handlers in D3. More about D3 Bar Charts:  D3 Creating a Bar Chart D3 Scales in a Bar Chart D3 Adding Axes to Bar Chart D3 Bar Chart Title and LabelsI want the barchart to change colors when you mouse over a bin to highlight what you are pointing at. Add two functions here: svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , d =&gt; xScale(d. year. getFullYear()))  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal )  . on( mouseover , handleMouseOver)  . on( mouseout , handleMouseOut);Then add the functions to process each interaction at the end, before the closing of d3. csv: function handleMouseOver(d, i) {  d3. select(this)   . attr( fill ,  red );}function handleMouseOut(d, i) {  d3. select(this)   . attr( fill ,  teal );}The barchart with mouse event handler looks like this:   Click to see updated code    var margin = {top: 20, right: 20, bottom: 20, left: 20},      w = 500 - margin. left - margin. right,      h = 300 - margin. top - margin. bottom;  var barPadding = 3;  var padding = 40;  var svg = d3. select( #d3-barchart-vis )        . append( svg )        . attr( preserveAspectRatio ,  xMinYMin meet )        . attr( viewBox ,  0 0   + w +     + h);  var parseTime = d3. timeParse( %Y );  var rowConverter = function(d) {    return {      year: parseTime(d. year),      population: +d. population    }  }  d3. csv( file. csv , rowConverter). then(function(dataset) {    xScale = d3. scaleBand()          . domain(dataset. map(d =&gt; d. year. getFullYear()))          . rangeRound([padding, w])          . paddingInner(0. 1)          . paddingOuter(0. 1);    yScale = d3. scaleLinear()          . domain([0, d3. max(dataset, d =&gt; d. population)])          . range([h - padding, padding]);    var xAxis = d3. axisBottom(xScale);    var yAxis = d3. axisLeft(yScale);    svg. selectAll( rect )      . data(dataset)      . enter()      . append( rect )      . attr( x , d =&gt; xScale(d. year. getFullYear()))      . attr( y , d =&gt; yScale(d. population))      . attr( width , xScale. bandwidth())      . attr( height , d =&gt; h - padding - yScale(d. population))      . attr( fill ,  teal )      . on( mouseover , handleMouseOver)      . on( mouseout , handleMouseOut);    // Add the x Axis    svg. append( g )      . attr( class ,  x axis )      . attr( transform ,  translate(0,  + (h - padding) +  ) )      . call(xAxis);    // Label for x Axis    svg. append( text )      . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )      . style( text-anchor ,  middle )      . style( font-size ,  12px )      . text( Year );    // Add the y Axis    svg. append( g )      . attr( class ,  y axis )      . attr( transform ,  translate(  + padding +  ,0) )      . call(yAxis);    // Label for y Axis    svg. append( text )      . attr( transform ,  rotate(-90) )      . attr( x , -(h/2))      . attr( y , 10)      . style( text-anchor ,  middle )      . style( font-size ,  12px )      . text( Population );    svg. append( text )      . attr( x , w/2)      . attr( y , padding)      . attr( text-anchor ,  middle )      . style( font-size ,  16px )      . text( Awesome Barchart );    function handleMouseOver(d, i) {      d3. select(this)       . attr( fill ,  red );    }    function handleMouseOut(d, i) {      d3. select(this)       . attr( fill ,  teal );    }  });  "
    }, {
    "id": 124,
    "url": "https://www.tomordonez.com/d3-visualization-margins/",
    "title": "D3 Visualization Margins",
    "body": "2020/07/31 - Add margins to a D3 visualization. Follow D3 Bar Chart Title and Labels to set the examples below. Adding Margins: The label for the y Axis is too close to the left. I had to plug a few different numbers in . attr( y , 15) to make it fit. // Label for y Axissvg. append( text )  . attr( transform ,  rotate(-90) )  . attr( x , -(h/2))  . attr( y , 15)  . style( text-anchor ,  middle )  . text( Population );We can add the margins like this: var margin = {top: 20, right: 20, bottom: 20, left: 20},  w = 500 - margin. left - margin. right,  h = 300 - margin. top - margin. bottom;Perhaps the labels are too big. They are the same size as the title. I added a font size for the labels, changed the position of the y label, and added the margins. The barchart looks like this:   Updated code with margins    var margin = {top: 20, right: 20, bottom: 20, left: 20},      w = 500 - margin. left - margin. right,      h = 300 - margin. top - margin. bottom;  var barPadding = 3;  var padding = 40;  var svg = d3. select( #d3-barchart-vis )        . append( svg )        . attr( preserveAspectRatio ,  xMinYMin meet )        . attr( viewBox ,  0 0   + w +     + h);  var parseTime = d3. timeParse( %Y );  var rowConverter = function(d) {    return {      year: parseTime(d. year),      population: +d. population    }  }  d3. csv( file. csv , rowConverter). then(function(dataset) {    xScale = d3. scaleBand()          . domain(d3. range(dataset. length))          . rangeRound([padding, w])          . paddingInner(0. 1)          . paddingOuter(0. 1);    yScale = d3. scaleLinear()          . domain([0, d3. max(dataset, function(d) { return d. population; })])          . range([h - padding, padding]);    var xAxis = d3. axisBottom(xScale);    var yAxis = d3. axisLeft(yScale);    svg. selectAll( rect )      . data(dataset)      . enter()      . append( rect )      . attr( x , function(d, i) {        return xScale(i);      })      . attr( y , d =&gt; yScale(d. population))      . attr( width , xScale. bandwidth())      . attr( height , d =&gt; h - padding - yScale(d. population))      . attr( fill ,  teal );    // Add the x Axis    svg. append( g )      . attr( class ,  x axis )      . attr( transform ,  translate(0,  + (h - padding) +  ) )      . call(xAxis);    // Label for x Axis    svg. append( text )      . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )      . style( text-anchor ,  middle )      . style( font-size ,  12px )      . text( Year );    // Add the y Axis    svg. append( g )      . attr( class ,  y axis )      . attr( transform ,  translate(  + padding +  ,0) )      . call(yAxis);    // Label for y Axis    svg. append( text )      . attr( transform ,  rotate(-90) )      . attr( x , -(h/2))      . attr( y , 10)      . style( text-anchor ,  middle )      . style( font-size ,  12px )      . text( Population );    svg. append( text )      . attr( x , w/2)      . attr( y , padding)      . attr( text-anchor ,  middle )      . style( font-size ,  16px )      . text( Awesome Barchart );  });  More about D3 Bar Charts:  D3 Creating a Bar Chart D3 Scales in a Bar Chart D3 Adding Axes to Bar Chart D3 Bar Chart Title and Labels"
    }, {
    "id": 125,
    "url": "https://www.tomordonez.com/d3-bar-chart-title-and-labels/",
    "title": "D3 Bar Chart Title and Labels",
    "body": "2020/07/26 - Add a title and labels to a bar chart in D3. D3 Bar Chart Title: Use append( text ) to append a title to the graph: svg. append( text )  . attr( x , w/2)  . attr( y , padding)  . attr( text-anchor ,  middle )  . style( font-size ,  16px )  . text( Awesome Barchart );Follow:  D3 Creating a Bar Chart D3 Scales in a Bar ChartAdd a label for the x Axis: A label can be added to the x Axis by appending a text and using the transform and translate to position the text. The function translate uses a string concatenation to get to translate(w/2, h-10) which is calculated to translate(500/2, 300-10) or translate(250, 290). Where x is in the middle of the SVG and y is 10px from the bottom (or 290px from the top). svg. append( text )  . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )  . style( text-anchor ,  middle )  . text( Year );Add a label for the y Axis: The label for the y Axis is a bit different. First we need to rotate the label vertically with a negative -90 degrees. Then the point of reference for (0,0) changes. If I am not mistaken it’s now on the top right relative to the rotated text. To center the text vertically. Move it half way to the left at an x distance of -(h/2). The y is set relative to the rotated position. If you move it down (to the right) it will be a positive number (towards the y Axis). If you move it up (to the left) it will be a negative number (away from the y Axis). svg. append( text )  . attr( transform ,  rotate(-90) )  . attr( x , -(h/2))  . attr( y , 15)  . style( text-anchor ,  middle )  . text( Population );The barchart looks like this:   Click to see updated code        var w = 500;  var h = 300;  var barPadding = 3;  var padding = 40;  var svg = d3. select( #d3-barchart-vis )        . append( svg )        . attr( preserveAspectRatio ,  xMinYMin meet )        . attr( viewBox ,  0 0   + w +     + h);  var parseTime = d3. timeParse( %Y );  var rowConverter = function(d) {    return {      year: parseTime(d. year),      population: +d. population    }  }  d3. csv( file. csv , rowConverter). then(function(dataset) {    xScale = d3. scaleBand()          . domain(d3. range(dataset. length))          . rangeRound([padding, w])          . paddingInner(0. 1)          . paddingOuter(0. 1);    yScale = d3. scaleLinear()          . domain([0, d3. max(dataset, function(d) { return d. population; })])          . range([h - padding, padding]);    var xAxis = d3. axisBottom(xScale);    var yAxis = d3. axisLeft(yScale);    svg. selectAll( rect )      . data(dataset)      . enter()      . append( rect )      . attr( x , function(d, i) {        return xScale(i);      })      . attr( y , d =&gt; yScale(d. population))      . attr( width , xScale. bandwidth())      . attr( height , d =&gt; h - padding - yScale(d. population))      . attr( fill ,  teal );    // Add the x Axis    svg. append( g )      . attr( class ,  x axis )      . attr( transform ,  translate(0,  + (h - padding) +  ) )      . call(xAxis);    // Label for x Axis    svg. append( text )      . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )      . style( text-anchor ,  middle )      . text( Year );    // Add the y Axis    svg. append( g )      . attr( class ,  y axis )      . attr( transform ,  translate(  + padding +  ,0) )      . call(yAxis);    // Label for y Axis    svg. append( text )      . attr( transform ,  rotate(-90) )      . attr( x , -(h/2))      . attr( y , 15)      . style( text-anchor ,  middle )      . text( Population );    svg. append( text )      . attr( x , w/2)      . attr( y , padding)      . attr( text-anchor ,  middle )      . style( font-size ,  16px )      . text( Awesome Barchart );  });  "
    }, {
    "id": 126,
    "url": "https://www.tomordonez.com/d3-adding-axes-to-bar-chart/",
    "title": "D3 Adding Axes to Bar Chart",
    "body": "2020/07/21 - Adding axes to a bar chart in D3. Follow:  D3 Creating a Bar Chart D3 Scales in a Bar ChartD3 axes in bar chart: Use any of these: d3. axisTop, d3. axisBottom, d3. axisLeft, d3. axisRight. Then append a g (group) element to the end of the SVG. var xAxis = d3. axisBottom(xScale);var yAxis = d3. axisLeft(yScale);Put this at the end of the script, since the graphics lay on top of each other, making the axes the last visible graph at the top of the SVG. svg. append( g )  . attr( class ,  axis )  . attr( transform ,  translate(0,  + (h - padding) +  ) )  . call(xAxis);svg. append( g )  . attr( class ,  axis )  . attr( transform ,  translate(  + padding +  ,0) )  . call(yAxis);This line is a string concatenation: . attr( transform ,  translate(0,  + (h - padding) +  ) )It computes to the following: . attr( transform ,  translate(0 , + (300 - 40 ) +  ) )Then the result is: svg. append( g )  . attr( class ,  axis )  . attr( transform ,  translate(0,260) )  . call(xAxis);The translate function moves the object by x and y. More details in the MDN Web Docs. The bar chart should look like this:  Updated Code     var w = 500; var h = 300; var barPadding = 3; var padding = 40; var svg = d3. select( #d3-barchart-vis )       . append( svg )       . attr( preserveAspectRatio ,  xMinYMin meet )       . attr( viewBox ,  0 0   + w +     + h); var parseTime = d3. timeParse( %Y ); var rowConverter = function(d) {   return {     year: parseTime(d. year),     population: +d. population   } } d3. csv( file. csv , rowConverter). then(function(dataset) {   xScale = d3. scaleBand()         . domain(d3. range(dataset. length))         . rangeRound([padding, w])         . paddingInner(0. 1)         . paddingOuter(0. 1);   yScale = d3. scaleLinear()         . domain([0, d3. max(dataset, function(d) { return d. population; })])         . range([h - padding, padding]);   var xAxis = d3. axisBottom(xScale);   var yAxis = d3. axisLeft(yScale);   svg. selectAll( rect )     . data(dataset)     . enter()     . append( rect )     . attr( x , function(d, i) {       return xScale(i);     })     . attr( y , d =&gt; yScale(d. population))     . attr( width , xScale. bandwidth())     . attr( height , d =&gt; h - padding - yScale(d. population))     . attr( fill ,  teal );   svg. append( g )     . attr( class ,  x axis )     . attr( transform ,  translate(0,  + (h - padding) +  ) )     . call(xAxis);   svg. append( g )     . attr( class ,  y axis )     . attr( transform ,  translate(  + padding +  ,0) )     . call(yAxis); }); Adding ticks on the Axes: Use . ticks(). However, D3 will override this if it wants to divide the input domain evenly. Use . tickValues([an array of values]) to set them manually. Use . tickFormat to format the axis labels. var xAxis = d3. axisBottom(xScale)       . ticks(someParameterHere);Troubleshooting the Year on the x Axis: The barchart still doesn’t look correct. The ticks on the x Axis are supposed to be years and not integers starting at 1.  Modify the scaleBand() so the domain maps to the years on the converted Date objects from the CSV file. It currently looks like this: xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . rangeRound([padding, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);Changed to this: xScale = d3. scaleBand()      . domain(dataset. map(d =&gt; d. year. getFullYear()))      . rangeRound([padding, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);You can test what this does on the console dataset. map(d =&gt; d. year. getFullYear()) inside this section: d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset. map(d =&gt; d. year. getFullYear()))The output should be: Array(4) [ 1950, 1951, 1955, 1959 ]Then modify implementing the xScale in this section. From this return xScale(i): svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    return xScale(i);  })  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal );Modified to this return xScale(d. year. getFullYear()): svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    return xScale(d. year. getFullYear());  })  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal );The corrected barchart looks like this:  Click to view updated code    var margin = {top: 20, right: 20, bottom: 20, left: 20},    w = 500 - margin. left - margin. right,    h = 300 - margin. top - margin. bottom; var barPadding = 3; var padding = 40; var svg = d3. select( #d3-barchart-vis )       . append( svg )       . attr( preserveAspectRatio ,  xMinYMin meet )       . attr( viewBox ,  0 0   + w +     + h); var parseTime = d3. timeParse( %Y ); var rowConverter = function(d) {   return {     year: parseTime(d. year),     population: +d. population   } } d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(dataset. map(d =&gt; d. year. getFullYear()))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, d =&gt; d. population)])        . range([h - padding, padding]);  var xAxis = d3. axisBottom(xScale);  var yAxis = d3. axisLeft(yScale);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , d =&gt; xScale(d. year. getFullYear()))    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal );  // Add the x Axis  svg. append( g )    . attr( class ,  x axis )    . attr( transform ,  translate(0,  + (h - padding) +  ) )    . call(xAxis);  // Label for x Axis  svg. append( text )    . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Year );  // Add the y Axis  svg. append( g )    . attr( class ,  y axis )    . attr( transform ,  translate(  + padding +  ,0) )    . call(yAxis);  // Label for y Axis  svg. append( text )    . attr( transform ,  rotate(-90) )    . attr( x , -(h/2))    . attr( y , 10)    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Population );  svg. append( text )    . attr( x , w/2)    . attr( y , padding)    . attr( text-anchor ,  middle )    . style( font-size ,  16px )    . text( Awesome Barchart ); }); "
    }, {
    "id": 127,
    "url": "https://www.tomordonez.com/d3-arrow-functions/",
    "title": "D3 Arrow Functions",
    "body": "2020/07/16 - Using arrow functions in D3 to simplify the syntax. More about arrow functions on Mozilla. Given this example: elements. map(function(element) {  return element. length;});This can be simplified to: elements. map((element) =&gt; {  return element. length;});If there is only one parameter and the only statement is return then it can be simplified to this: elements. map(element =&gt; element. length);Follow D3 Scales in a Bar Chart to review this code in detail. This code was used to create the rectangles of the bar chart: svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    return xScale(i);  })  . attr( y , function(d) {    return yScale(d. population);  })  . attr( width , xScale. bandwidth())  . attr( height , function(d) {    return h - padding - yScale(d. population);  })  . attr( fill ,  teal );This can be refactored using arrow functions: svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    return xScale(i);  })  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal );"
    }, {
    "id": 128,
    "url": "https://www.tomordonez.com/jekyll-build-deploy-github-pages/",
    "title": "Jekyll Build and Deploy to Github Pages",
    "body": "2020/07/11 - Cheat sheet Jekyll build and deploy to Github Pages. Build in development and test in localhost: $ bundle exec jekyll serveBackup development files to Github: $ git add . $ git commit -m  Awesome commit message here $ git push -u origin masterDeploy to production: $ JEKYLL_ENV=production jgdFor more details see a step by step setup tutorial of Jekyll with Github Pages "
    }, {
    "id": 129,
    "url": "https://www.tomordonez.com/d3-responsive-visualization/",
    "title": "D3 Responsive Visualization",
    "body": "2020/07/11 - How to make a D3 visualization responsive. Follow this Setup D3 Step by Step and D3 Scales in a Bar Chart to set the examples below. Add&lt;div id= d3-barchart-vis &gt;&lt;/div&gt; to index. html: &lt;!DOCTYPE html&gt;&lt;html lang= en &gt;  &lt;head&gt;    &lt;meta charset= utf-8 &gt;    &lt;title&gt;D3 Canvas&lt;/title&gt;    &lt;link rel= icon  type= image/png  href= icon. png &gt;    &lt;script type= text/javascript  src= assets/js/d3. min. js &gt;&lt;/script&gt;    &lt;link rel= stylesheet  type= text/css  href= main. css &gt;  &lt;/head&gt;  &lt;body&gt;    &lt;div id= d3-barchart-vis &gt;&lt;/div&gt;    &lt;script type= text/javascript  src= project. js &gt;&lt;/script&gt;  &lt;/body&gt;&lt;/html&gt;The project. js needs to be modified when creating the svg. It has to select div id= d3-barchart-vis . It needs to use preserveAspectRatio and viewBox for responsive. The viewBox follows this syntax  min-x min-y width height . I concatenated the numbers and the spaces: var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);The code for project. js looks like this: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , function(d) {      return yScale(d. population);    })    . attr( width , xScale. bandwidth())    . attr( height , function(d) {      return h - padding - yScale(d. population);    })    . attr( fill ,  teal );});The result is this bar chart: "
    }, {
    "id": 130,
    "url": "https://www.tomordonez.com/d3-scales-in-a-bar-chart/",
    "title": "D3 Scales in a Bar Chart",
    "body": "2020/07/06 - Applying D3 Scales scaleBand and scaleLinear in a bar chart. Follow these to understand the code below:  Setup D3 Step by Step D3 Drawing SVG D3 Creating a Bar Chart D3 Using Scales D3 Linear Scale D3 Band ScaleHere is a summary:  Set the width and height of the SVG canvas Set padding for the canvas Create a function to parse CSV dates from string to type date Use promises to load the CSV Scale the width and height of each rectangle of the bar chart Create rectangle elements and append the data to the DOMD3 Scales in a Bar Chart: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , function(d) {      return yScale(d. population);    })    . attr( width , xScale. bandwidth())    . attr( height , function(d) {      return h - padding - yScale(d. population);    })    . attr( fill ,  teal );});The result is this bar chart: The HTML is this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 52  y= 205  width= 101  height= 55  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 164  y= 150  width= 101  height= 110  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 276  y= 95  width= 101  height= 165  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 388  y= 40  width= 101  height= 220  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;You can use console. log() to test the output such as in here: svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    console. log( x );    console. log(d);    console. log(xScale. domain());    console. log(xScale. range());    console. log(xScale. paddingInner());    console. log(xScale. paddingOuter());    console. log(xScale. bandwidth());    console. log(xScale(i));    return xScale(i);  })"
    }, {
    "id": 131,
    "url": "https://www.tomordonez.com/github-pages-custom-domain/",
    "title": "Github Pages Custom Domain",
    "body": "2020/07/04 - How to setup Github Pages with a custom domain. For more details see a step by step setup tutorial of Jekyll with Github Pages Setup Github: Go to Github:  Create a new repo with the format username. github. ioSetup the repo: $ git init$ git remote add origin link-to-repoCNAME, robots. txt: If you have a custom domain, create a CNAME file, add a line with your website, and save it to your local blog root directory: www. yoursite. comCreate a robots. txt and add this line to the file: User-agent: *You can also use Disallow for bad URLs. User-agent: *Disallow: /bad. htmlAllow: /Setup: Jekyll and Cloudflare "
    }, {
    "id": 132,
    "url": "https://www.tomordonez.com/jekyll-dependency-errors/",
    "title": "Jekyll and Dependency Errors",
    "body": "2020/07/02 - Troubleshooting Jekyll and dependency errors When testing the site. For jekyll-3. 8. 5 it says warning: Using the last argument as keyword parameters is deprecated.  Also for pathutil-0. 16. 2 it says the same.  More here And hereEdit the Gemfile and comment this line again gem  github-pages , group: :jekyll_plugins. Then add this one: gem 'jekyll', github: 'jekyll/jekyll'If you have plugins update them to this: group :jekyll_plugins do  gem 'jekyll-feed', github: 'jekyll/jekyll-feed'  gem 'jekyll-sitemap', github: 'jekyll/jekyll-sitemap'  gem 'jekyll-paginate', github: 'jekyll/jekyll-paginate'  gem 'jekyll-seo-tag', github: 'jekyll/jekyll-seo-tag'  gem 'jekyll-redirect-from', github: 'jekyll/jekyll-redirect-from'endMy Gemfile currently looks like this: source  https://rubygems. org gem 'jekyll', github: 'jekyll/jekyll'gem  minima ,  ~&gt; 2. 5 group :jekyll_plugins do  gem 'jekyll-feed', github: 'jekyll/jekyll-feed'  gem 'jekyll-sitemap', github: 'jekyll/jekyll-sitemap'  gem 'jekyll-paginate', github: 'jekyll/jekyll-paginate'  gem 'jekyll-seo-tag', github: 'jekyll/jekyll-seo-tag'  gem 'jekyll-redirect-from', github: 'jekyll/jekyll-redirect-from'endThen run bundle install See a step by step setup tutorial of Jekyll with Github Pages "
    }, {
    "id": 133,
    "url": "https://www.tomordonez.com/d3-band-scale/",
    "title": "D3 Band Scale",
    "body": "2020/07/01 - Using a band scale in D3 with scaleBand(). More about D3 Using Scales. The D3 Docs have this great image to help visualize D3 scaleBand() methods: These are the methods available:  . domain() . range() . rangeRound() . round() . paddingInner() . paddingOuter() . padding() . align() . bandwidth() . step()Given this dataset: year   population1950    51951    101955    151959    20Set the examples below with D3 Creating a Bar Chart d3. scaleBand(). domain(): The xScale is used to scale the year column. Instead of using it as a Date, it maps the range of the dataset length to a width range. : xScale = d3. scaleBand()      . domain(d3. range(dataset. length))The domain . domain(d3. range(dataset. length)) is a range of dataset. length: d3. range(dataset. length)You can use the console. log() inside the CSV function to test: d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset. length);  console. log(d3. range(dataset. length));  . . . The console should output this. See Setup D3 Step by Step: 4Array(4) [ 0, 1, 2, 3 ]The domain for scaleBand() is using this: . domain([0, 1, 2, 3])d3. scaleBand(). range(): Setting the range to fit within the width of the SVG: xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . range([0, w]);The . range([0, w]) creates a range from [0, w], where the values are implicitly calculated with this formula: (w - 0) / . domain(). lengthIf the width was set to 500 then: (w - 0) / . domain(). length(500 - 0) / 4 = 125For a bar chart, this creates rectangles with the same width of 125. So far we have this: xScale = d3. scaleBand()      . domain([0, 1, 2, 3])      . range([0, 125, 250, 375])d3. scaleBand(). paddingInner(): The API docs say that if you don’t specify a paddingInner, the default is zero 0. If you specify one, the number must be &lt;= 1. A value of 0 means there is no blank space between the bars. A value of 1 means a bandwidth of 0. xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . range([0, w])      . paddingInner(0. 1);d3. scaleBand(). paddingOuter(): This is the padding to apply before the first bar and after the last bar. The number is in the range of [0, 1]. If not specified, the padding returns zero. xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . range([0, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);d3. scaleBand(). rangeRound(): The range can calculate long decimal numbers such as this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 12. 195121951219477  y= 205  width= 122  height= 55  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 134. 14634146341461  y= 150  width= 122  height= 110  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 256. 0975609756098  y= 95  width= 122  height= 165  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 378. 04878048780483  y= 40  width= 122  height= 220  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;We can replace range with rangeRound. xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . rangeRound([padding, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);The HTML now shows something like this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 14  y= 205  width= 122  height= 55  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 135  y= 150  width= 122  height= 110  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 256  y= 95  width= 122  height= 165  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 377  y= 40  width= 122  height= 220  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;d3. scaleBand(). bandwidth(): In a D3 bar chart, this used to return the width of each bar. More about D3 Scales and D3 Bar Chart:  D3 Drawing SVG D3 Creating a Bar Chart D3 Using Scales D3 Linear Scale D3 Scales in a Bar Chart"
    }, {
    "id": 134,
    "url": "https://www.tomordonez.com/jekyll-pagination/",
    "title": "Jekyll and Pagination",
    "body": "2020/06/29 - How to setup Jekyll and Pagination. Jekyll comes with a default plugin jekyll-paginate that you can set by adding the line paginate: 5 to _config. yml. This is the Jekyll doc. Go to _config. yml and make sure this line is under plugins: plugins: - jekyll-paginateThen add another line (outside of plugins): paginate: 5If your theme’s index is index. markdown, change it to index. html or you will get this error: Pagination: Pagination is enabled, but I couldn't find an index. htmlpage to use as the pagination template. Skipping pagination. Update your _layouts/home. html. Change this line, from this: for post in site. postsTo this: for post in paginator. postsFor pagination links, add the code as shown on Jekyll’s Render the paginated posts. Before the closing endif of if site. posts. size &gt; 0. You can center the CSS of pagination in assets/main. scss: . pagination {	display: flex;}. previous, a. previous {	flex: 1;}. page_number {	flex: 1;}next, a. next {	flex: 1;}"
    }, {
    "id": 135,
    "url": "https://www.tomordonez.com/d3-linear-scale/",
    "title": "D3 Linear Scale",
    "body": "2020/06/26 - Create a linear scale in D3 with scaleLinear. Given this dataset: year   population1950    51951    101955    151959    20D3 Linear Scale with scaleLinear(): Use this syntax: yScale = d3. scaleLinear()      . domain([0, d3. max(dataset, function(d) { return d. population; })])      . range([h - padding, padding]);Mapping input domain to output range:  Input domain: From 0 to the max value of the population column Output range: Limit the visualization to the padded limits of the SVG height. With SVG the coordinates increase left to right, top to bottom. For this: . range([h - padding, padding]); The height h is the height of the SVG, for example var h = 300;.  padding can be set as var padding = 40; h - padding is 300 - 40 which is 260Then the range is: . range([260, 40])The range is between the y coordinate 260 and the y coordinate 40. Which is the padded region of the height of the SVG. More about D3 and Scales:  D3 Drawing SVG D3 Creating a Bar Chart D3 Using Scales D3 Band Scale D3 Scales in a Bar Chart"
    }, {
    "id": 136,
    "url": "https://www.tomordonez.com/jekyll-related-posts/",
    "title": "Jekyll Related Posts",
    "body": "2020/06/22 - How to setup Jekyll related posts using tags. This is a Jekyll custom plugin. Documentation here. Update your Gemfile: gem 'jekyll-tagging-related_posts'Run bundle $ bundleOutput: Fetching nuggets 1. 6. 0Installing nuggets 1. 6. 0Fetching jekyll-tagging 1. 1. 0Installing jekyll-tagging 1. 1. 0Fetching jekyll-tagging-related_posts 1. 1. 0 Installing jekyll-tagging-related_posts 1. 1. 0Post-install message from nuggets:nuggets-1. 6. 0 [2018-07-12]:* Added &lt;tt&gt;JSON. *_{multi,canonical}&lt;/tt&gt;. Post-install message from jekyll-tagging:jekyll-tagging-1. 1. 0 [2017-03-07]:* Added ability to append extra data to all tag pages. (tfe)* Provides compatibility to the current jekyll (3. 4. 1). * A few fixes. (felipe)* Some documentation improvements. (wsmoak, jonathanpberger)* Prooves who is the worst open source maintainer. (pattex ^__^)Update _config. yml and _config-deploy. yml: plugins: - jekyll/tagging - jekyll-tagging-related_postsCreate a _layouts directory in blog root: $ mkdir _layoutsCopy the post. html layout from the minima theme Gem to this new directory. $ cp /home/tom/. rvm/gems/ruby-2. 7. 1@blog/gems/minima-2. 5. 1/_layouts/post. html _layouts/Add this code to post. html after the blog post content and before disqus code. {% if site. related_posts. size &gt;= 1 %}&lt;div&gt; &lt;h3&gt;Related Posts&lt;/h3&gt; &lt;ul&gt; {% for related_post in site. related_posts limit: 5 %}  &lt;li&gt;&lt;a href= {{ related_post. url }} &gt;{{ related_post. title }}&lt;/a&gt;&lt;/li&gt; {% endfor %} &lt;/ul&gt;&lt;/div&gt;{% endif %}Setup: Jekyll Custom Plugin Deploy "
    }, {
    "id": 137,
    "url": "https://www.tomordonez.com/d3-using-scales/",
    "title": "D3 Using Scales",
    "body": "2020/06/21 - Understanding scales in D3. This is where things get interesting with D3, and you can spend countless hours trying to fix a visualization in the SVG way. More about D3 Drawing SVG Definitions:  Input domain is the range of input data values Output range is the range of output valuesGiven a dataset such as: dataset = [5, 10, 15, 20]Using scales in D3 require to map a range of input values to a range of output values.  Input domain: [5, 20] Output range: [10, 250] in pixels. The minimum input value of 5 is represented as 10px, and the maximum value of 20 is represented as 250px. The input value can be normalized and this result can be scaled to the output. For example, normalizing input values to a range of 0 to 1. 0 can be represented as 0px and 1 as 250px. The rectangles on this bar chart look too short compared to the SVG canvas: The rectangles can be scaled to improve the visualization. Using scales in D3:  D3 Linear Scale D3 Band Scale D3 Scales in a Bar Chart"
    }, {
    "id": 138,
    "url": "https://www.tomordonez.com/jekyll-external-links/",
    "title": "Jekyll and External Links",
    "body": "2020/06/19 - Open Jekyll external links in a new window. By default linking to external sites open in the same window using this syntax: [External Title](link to external site)Jekyll uses kramdown and you can link like this: [External Title](link to external site){:target= _blank }What I find annoying about this, is that it adds a weird highlighted row in SublimeText. I also think it’s a weird syntax to remember. Setup Jekyll and External Links: An alternative option is this JS called new window fix Download the code into _includes/new-window-fix. html and remove the PDF section if you don’t need it. Add this to your _layouts/default. html before the closing body tag. {% include new-window-fix. html %}Add external link to menu: There isn’t a clear way to add an external link to the menu as discussed on navigation external links. This comment shows a quick fix to add the external link to _includes/header. html. I added a link to my Linkedin profile as shown: &lt;div class= trigger &gt; {%- for path in page_paths -%}  {%- assign my_page = site. pages | where:  path , path | first -%}  {%- if my_page. title -%}  &lt;a class= page-link  href= {{ my_page. url | relative_url }} &gt;{{ my_page. title | escape }}&lt;/a&gt;  &lt;a class= page-link  href= https://www. linkedin. com/in/tomordonez/ &gt;Linkedin&lt;/a&gt;  {%- endif -%} {%- endfor -%}&lt;/div&gt;The header_pages has to be enabled in _config. yml for the menu to show. I have an about page here: header_pages: - about. md"
    }, {
    "id": 139,
    "url": "https://www.tomordonez.com/d3-creating-a-bar-chart/",
    "title": "D3 Creating a Bar Chart",
    "body": "2020/06/16 - In D3 creating a bar chart after loading a CSV file. Using the CSV dataset: year,population1950,51951,101955,151959,20Syntax to load a CSV file in D3: Use this syntax to load the CSV file. See D3 Load a CSV file with Promises. Passing the function rowConverter to convert strings to dates as seen in D3 Convert String to Date. d3. csv( file. csv , rowConverter). then(function(dataset) {  // add code here});SVG coordinates in D3: The rectangles of the bar chart are created by adding attributes for (x,y). SVG coordinates are measured left to right and top to bottom. The coordinate (0,0) is the top left corner. The coordinates increase to the right for x and down for y.  x located at the bottom left of the rectangle.  y at the top left of the rectangle. You are drawing kind of upside down. That’s how weird SVG is. Drawing each rectangle for the bar chart: In this example, width draws 20px to the right, and height draws 100px downwards from the y coordinate. All rectangles of the bar chart are drawn downwards, following the characteristics of the SVG having its (0,0) on the top left corner.  Add this to a project. js file. More details in Setup D3 Step by Step: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , 0)    . attr( y , 0)    . attr( width , 20)    . attr( height , 100);});Run the Python server to see the result and the HTML: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;&lt;/svg&gt;However, by setting . attr( x , 0), it makes the bar chart rectangles to overlap on top of each other, as they are all drawn at the same (x, y) position. Scaling the coordinates of the rectangles: The coordinates of the rectangles need to scale dynamically. For example for x values, use an anonymous function to pass each value d of the dataset, and the index i for each value. Then compute i times the width w over the length of the dataset. Do the calculation by hand and you will understand how it works. Update the code to this and reload the browser: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return i * (w/dataset. length);      })    . attr( y , function(d) {      return h - d. population;      })    . attr( width , w/dataset. length - barPadding)    . attr( height , function(d) {      return d. population;      })    . attr( fill ,  teal );});The result is this: The HTML shows this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 0  y= 295  width= 122  height= 5  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 125  y= 290  width= 122  height= 10  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 250  y= 285  width= 122  height= 15  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 375  y= 280  width= 122  height= 20  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;About this: attr( x , function(d, i): We need to set the x position of each rectangle. This points to the bottom left of each rectangle. attr( x , function(d, i) {  return i * (w/dataset. length);})For each value of the dataset: year   population1950    51951    101955    151959    20The corresponding x values are as follows. Keep in mind the d values are not used for the calculation, only the index (i) of each value. First, let’s look at the indices:  d      i (index)[1950, 5]    0[1951, 10]   1[1955, 15]   2[1950, 20]   3The calculation (w/dataset. length) is done to evenly position each rectangle across the width w of the SVG.  w was set before as var w = 500 length of the dataset is 4 i is used to find a starting position for each rectangle. The calculations of i * (w/dataset. length) are. 0 * (500/4)1 * (500/4)2 * (500/4)3 * (500/4)Then they are returned to the call back function as a parameter of each x attribute. . attr( x , 0). attr( x , 125). attr( x , 250). attr( x , 375)This means the first rectangle is positioned at x = 0, then second rectangle at x = 125 and so on. About this: . attr( y , function(d): We need to set the y position of each rectangle. This points to the top left of each rectangle. . attr( y , function(d) {  return h - d. population;})In this code h was previously set as var h = 300;. The call back function is passing every row of the dataset as d and we need the second value population of each row to calculate the y coordinate of each rectangle. return h - d. population;Given these values, where a value of d corresponds to [d. year, d. population].   d[1950, 5][1951, 10][1955, 15][1950, 20]The subtractions are: 300 - 5 = 295300 - 10 = 290300 - 15 = 285300 - 20 = 280For the SVG the (0,0) is at the top left. Then y value of 295 means measure 295px from top moving downwards. These are returned to the call back function to form these: . attr( y , 295). attr( y , 290). attr( y , 285). attr( y , 280)Putting these two attributes together, we can get the coordinates for each rectangle . attr( x , 0)    . attr( y , 295). attr( x , 125)    . attr( y , 290). attr( x , 250)    . attr( y , 285). attr( x , 375)    . attr( y , 280)The rectangles are at these coordinates:  Rectangle 1: (0, 295) Rectangle 2: (125, 290) Rectangle 3: (250, 285) Rectangle 4: (375, 280)About this: . attr( width , w/dataset. length - barPadding): Now that we got the coordinates for each rectangle, then we need to draw them. The . attr width, draws each rectangle from left to right. We know that (w/dataset. length) = 500 / 4. We are using a variable barPadding that we should have defined previously to make some separation between the rectangles. var barPadding = 3;The width is the same for all rectangles: w/dataset. length - barPadding = 500/4 - 3 = 122This value is then used as a parameter: . attr( width , w/dataset. length - barPadding). attr( width , 122)About this: . attr( height , function(d): As seen here: . attr( height , function(d) {  return d. population;})Set the height starting from the position y by returning the value d. population on the dataset. In other words, build the rectangle downwards from top to bottom starting at position y. Putting it all together using this format (x, width, y, height):  Rectangle 1: (0,  122, 295, 5) Rectangle 2: (125, 122, 290, 10) Rectangle 3: (250, 122, 285, 15) Rectangle 4: (375, 122, 280, 20)Here is a description of the first two rectangles:  Rect1 starts at x=0, draw width=122 towards the right, at position y=295, draw height=5 towards the bottom.  There is a barPadding = 3 towards the right, separating Rect1 from Rect2.  Rect2 starts x=125, draw width=122 towards the right, at position y=290, draw height=10 towards the bottom. Keep in mind again the weird SVG way of positioning/drawing elements left to right, top to bottom. Next: D3 Using Scales "
    }, {
    "id": 140,
    "url": "https://www.tomordonez.com/jekyll-cloudflare/",
    "title": "Jekyll and CloudFlare",
    "body": "2020/06/15 - How to setup Jekyll with CloudFlare. Setup an account with Cloudflare if you don’t have one. Find the DNS name server In your DNS provider point it to Cloudflare. Cloudflare settings: Create these records in Cloudflare: Type  Name        ContentALIAS  yoursite. com    youruser. github. ioCNAME  www. yoursite. com  youruser. github. ioTXT   yoursite. com    youruser. github. ioAdd A records as seen on Setting up an Apex domain Add TXT record to verify Google webmaster tools:  Add property Add TXT google verification codeSetup these Page rules. As seen here https://www. yoursite. com/*Cache Level: Cache Everythinghttps://yoursite. com/*Forwarding URL: (Status Code: 301 - Permanent Redirect, URl: https://www. yoursite. com$1)http://www. yoursite. com/*Always Use HTTPSIn your Overview dashboard set these (if you are on the free plan)  Security level: medium SSL: Full Caching level: StandardSee a step by step setup tutorial of Jekyll with Github Pages "
    }, {
    "id": 141,
    "url": "https://www.tomordonez.com/jekyll-text-expand-collapsible-markdown/",
    "title": "Jekyll Text Expand or Collapsible Markdown",
    "body": "2020/06/12 - Hide or show text in Jekyll with text expand or collapsible markdown. I often add whole output to every command I use. This can take a large space in a blog post and might disrupt reading focus. There are three options for hiding/display text that can be expanded, also known by these keywords: text expand, expand/collapse, collapsible markdown, details element. Use whatever works best. Text Expand: This is a JS that might need some tweaking. When you click on read more it expands the section but it scrolls back to the top. This is the doc for Jekyll Text Expand. Download the file text-expand. html into the _includes directory. Then edit the _layouts/default. html and add this before the closing body tag: {% include text-expand. html %}Then you can use the expand tag in a blog post by adding only one line for each of the open/closing tag such as: [expand]Long content hereand here. . . [/expand]Collapbsible Markdown with Details element: This uses the details disclosure element: details. More details in the Mozilla details element doc. You have to wrap your content like this: &lt;details&gt;	&lt;summary&gt;Click to expand&lt;/summary&gt;	Long content here	and here&lt;/details&gt;To add a code block, you need to have a previous empty line, enclose the code block with three tildes ~~~, optionally you can add the language at the end of the first enclosing tildes: &lt;details&gt;	&lt;summary&gt;Click to expand&lt;/summary&gt;		~~~ python	Code here	~~~&lt;/details&gt;However, you can encounter this issue Details is not formatted correctly in Jekyll/Github pages. You can enclose the content with the &lt;pre&gt; tag. I am using this for my long content output: &lt;details&gt;	&lt;summary&gt;Click to expand&lt;/summary&gt;	&lt;pre&gt;			Long content here	&lt;/pre&gt;&lt;/details&gt;Here is an example: 	Click to expand		Long content here	I also customized the CSS: details {	padding-bottom: 20px;	color: grey;}The text in details not processed correctly: I haven’t tested this. This blog post shows adding support for HTML5 details element to Jekyll. It uses a custom plugin. Add the ruby code into _plugins/details_tag. rb. Then use like this: {% details Click to expand %}	~~~ python	Code here	~~~{% enddetails %}More troubleshooting in using details in Github and collapsible markdown. See Jekyll without Plugins for other customizations. "
    }, {
    "id": 142,
    "url": "https://www.tomordonez.com/d3-drawing-svg/",
    "title": "D3 Drawing SVG",
    "body": "2020/06/11 - Drawing an SVG in D3 to add HTML elements to a D3 visualization. SVG stands for Scalable Vector Graphics. More info on Wikipedia. This is used to draw a canvas for adding HTML elements to a D3 visualization. Create the SVG element with width w and height h: var w = 500;var h = 300;var svg = d3. select( body )      . append( svg );      . attr( width , w)      . attr( height , h);See Setup D3 Step by Step to add this code to a project. js and run a Python server to see the output on the browser. The HTML code shows that an svg tag was created: &lt;svg width= 500  height= 300 &gt;&lt;/svg&gt;The SVG is visualized as a rectangle: Next: D3 Creating a Bar Chart "
    }, {
    "id": 143,
    "url": "https://www.tomordonez.com/jekyll-redirect-link/",
    "title": "Jekyll Redirect Link",
    "body": "2020/06/08 - Create Jekyll redirect links when migrating your website, for links that changed or that were removed. Add this to the Gemfile in the plugins block: gem 'jekyll-redirect-from', github: 'jekyll/jekyll-redirect-from'Then run bundle install. As seen in the docs here Add it to the _config. yml under plugins - jekyll-redirect-fromMy problem was that for my previous website, the blog posts had this format: blog_post_name. htmlThis new website removes the . html and a lot of blog posts crawled by google are being sent to a 404 File not found page. I want to redirect blog_post_name. html to blog_post_name/ In the blog post the header should show something like this: title:  Blog Post Name redirect_from: - /blog_post_name. html404 page: More about 404 pages here You can create a 404. md file if you add this: ---layout: pagetitle:  Not Found permalink:  /404. html comments: falseredirect_from: - /index2. html---Use the redirect_from: to redirect bad URLs from Google search results. Or use the same approach for blog posts. You can also try to fix them in your Google Search Console. "
    }, {
    "id": 144,
    "url": "https://www.tomordonez.com/d3-bind-data-to-dom/",
    "title": "D3 Bind data to DOM",
    "body": "2020/06/06 - Load a CSV in D3 and bind the data to elements in the DOM. Steps to bind data to DOM in D3::  Select HTML elements with . select() Add the data with . data() Bind the data to elements with . enter() Append elements to the DOM with . append()More about rowConverter in D3 Convert String to Date d3. csv( file. csv , rowConverter). then(function(dataset) {  d3. select( body ). selectAll( p )  . data(dataset)  . enter()  . append( p );});Loading the CSV uses then syntax. More on D3 Load a CSV file with Promises This is how binding data to DOM works:  It selects the element body.  Then it selects all p elements. However, no p elements exist yet.  Use the attribute . data to read dataset.  Use . enter() to bind the dataset values with p elements Use . append( p ) to append the p elements to the DOM. Run the D3 server in Python as seen in Setup D3 Step by Step Go to the Elements tab to see the HTML code. The body section has this now: &lt;body&gt;  &lt;script type= text/javascript  src= project. js &gt;&lt;/script&gt;  &lt;p&gt;&lt;/p&gt;  &lt;p&gt;&lt;/p&gt;  &lt;p&gt;&lt;/p&gt;  &lt;p&gt;&lt;/p&gt;&lt;/body&gt;It created four p tags. In the console type d3. selectAll( p );: Pt {_groups: Array(1), _parents: Array(1)}_groups: [NodeList(4)]_parents: [html]__proto__: ObjectExpand groups _groups: Array(1)0: NodeList(4) [p, p, p, p]length: 1Expand NodeList 0: NodeList(4)  0: p  1: p  2: p  3: pExpand the first 0: p. It’s pretty long, scroll down to the bottom: __data__: {year: Sun Jan 01 1950 00:00:00 GMT-0500 (Eastern Standard Time), population: 5}__proto__: HTMLParagraphElementExpand data: __data__:population: 5year: Sun Jan 01 1950 00:00:00 GMT-0500 (Eastern Standard Time) {}Next: D3 Drawing SVG "
    }, {
    "id": 145,
    "url": "https://www.tomordonez.com/jekyll-reading-time-blog-posts/",
    "title": "Jekyll and Reading Time Blog Posts",
    "body": "2020/06/05 - Add reading time for blog posts in Jekyll without a custom plugin. Similar to Medium you can show the time it will take to read the blog post. Here is the documentation to reading time Create the file reading-time. html in your _includes and add the code shown on the doc. Go to your _layouts/post. html and include the reading-time. html line within the header tag, before the closing paragraph. You can test to see if it shows correctly on your blog post such as Aug 29, 2020 • tom • 3 min read. Deploy the source to master: $ jekyll build$ git add . $ git commit -m  Added reading time $ git push -u origin masterSee Jekyll without Plugins for other customizations. "
    }, {
    "id": 146,
    "url": "https://www.tomordonez.com/jekyll-custom-plugin-deploy/",
    "title": "Jekyll Custom Plugin Deploy",
    "body": "2020/06/04 - How to deploy a Jekyll custom plugin. As shown on the Github Pages docs. Github Pages cannot build sites using unsupported plugins. Here is the list of approved plugins. Go to dependency versions. As shown in this blog post Deploy Jekyll to Github Pages. You can use a gem to setup deployment of Jekyll when you are using custom plugins. Update your Gemfile: gem 'jgd'Run bundle $ bundleOutput: Fetching trollop 2. 9. 9Installing trollop 2. 9. 9Fetching jgd 1. 12Installing jgd 1. 12Post-install message from trollop:!  The 'trollop' gem has been deprecated and has been replaced by 'optimist'. !  See: https://rubygems. org/gems/optimist!  And: https://github. com/ManageIQ/optimistMore about trollop in the official doc and this tutorial: writing a Ruby CLI using Trollop. To deploy use this: $ JEKYLL_ENV=production jgdChange the source of your Github Pages:  Go to your Github repo Settings Options Scroll down to Github Pages Source Change your branch from master to gh-pages.  SaveThen deploy again: $ JEKYLL_ENV=production jgd	Some of the output:		[DEPRECATION] This gem has been renamed to optimist 	and will no longer be supported. Please switch to optimist 	as soon as possible. 	+ set -e	+ set -o pipefail	+ URL=https://github. com/. . . git	+ BRANCH=gh-pages	+ BRANCH_FROM=master	+ DEPLOY_CONFIG=_config-deploy. yml	+ BUNDLE=	+ DRAFTS=	++ pwd	+ SRC=/home/. . . /blog	++ mktemp -d -t jgd-XXX	+ TEMP=/tmp/jgd-NAn	+ trap 'rm -rf /tmp/jgd-NAn' EXIT	+ CLONE=/tmp/jgd-NAn/clone	+ COPY=/tmp/jgd-NAn/copy	+ echo -e 'Cloning Github repository:'		Cloning Github repository:	+ git clone -b master https://github. com/. . . /tmp/jgd-NAn/clone		Cloning into '/tmp/jgd-NAn/clone'. . . 	remote: Enumerating objects: 3025, done.  	remote: Counting objects: 100% (3025/3025), done.                   	remote: Compressing objects: 100% (1920/1920), done.                  	remote: Total 3025 (delta 1272), 	 reused 2692 (delta 954), pack-reused 0        	Receiving objects: 100% (3025/3025), 31. 22 MiB 	 | 13. 20 MiB/s, done. 	Resolving deltas: 100% (1272/1272), done. 	+ cp -R /tmp/jgd-NAn/clone /tmp/jgd-NAn/copy	+ cd /tmp/jgd-NAn/clone	+ echo -e '\nBuilding Jekyll site:'		Building Jekyll site:	+ rm -rf _site	+ '[' -r _config-deploy. yml ']'	+ jekyll build	Configuration file: /tmp/jgd-NAn/clone/_config. yml	      Source: /tmp/jgd-NAn/clone	    Destination: /tmp/jgd-NAn/clone/_site	 Incremental build: disabled. Enable with --incremental	   Generating. . . 	    Jekyll Feed: Generating feed for posts	          done in 1. 67 seconds. 	 Auto-regeneration: disabled. Use --watch to enable. 	+ '[' '!' -e _site ']'	+ cp -R _site /tmp/jgd-NAn	+ cd /tmp/jgd-NAn	+ rm -rf /tmp/jgd-NAn/clone	+ mv /tmp/jgd-NAn/copy /tmp/jgd-NAn/clone	+ cd /tmp/jgd-NAn/clone	+ echo -e '\nPreparing gh-pages branch:'	Preparing gh-pages branch:	++ git branch -a	++ grep origin/gh-pages	+ '[' -z '' ']'	+ git checkout --orphan gh-pages		Switched to a new branch 'gh-pages'	+ echo -e '\nDeploying into gh-pages branch:'	Deploying into gh-pages branch:	+ rm -rf 404. md about. md assets CNAME _config. yml Gemfile 	 Gemfile. lock _includes index. markdown _layouts _posts	+ cp -R /tmp/jgd-NAn/_site/404. html /tmp/jgd-NAn/_site/about. html	+ rm -f README. md	+ git add . 	++ date	+ git commit -am 'new version Thu 08 Oct 2020 	 12:10:05 AM EDT' --allow-empty	[gh-pages (root-commit) a36ecd2] new version 	 Thu 08 Oct 2020 12:10:05 AM EDT	 819 files changed, 61268 insertions(+)	 create mode 100644 . github/FUNDING. yml	 create mode 100644 . gitignore	 create mode 100644 . jekyll-cache/Jekyll/Cache/Jekyll--Cache/b7	 . . .   + git push origin gh-pages	+ sed 's|https://github. com/. . . |[skipped]|g'	remote: 	remote: Create a pull request for 'gh-pages' on GitHub by visiting:    	remote:   https://github. com/. . . /pull/new/gh-pages     	remote: 	To [skipped]	 * [new branch]   gh-pages -&gt; gh-pages	+ echo -e '\nCleaning up:'	Cleaning up:	+ rm -rf /tmp/jgd-NAn/clone	+ rm -rf ''	+ rm -rf /tmp/jgd-NAn	See Jekyll without Plugins for other customizations. "
    }, {
    "id": 147,
    "url": "https://www.tomordonez.com/jekyll-without-plugins/",
    "title": "Jekyll without Plugins",
    "body": "2020/06/03 - Use Jekyll without plugins instead of using restricted custom plugins since these custom plugins might not work on Github Pages. Here is a list of features to use Jekyll without plugins. Reading Time: Similar to Medium you can show the time it will take to read the blog post. Setup: Jekyll Reading Time for Blog Posts Search Box: You can also install a search bar. Setup: Install a Jekyll Search Bar Text Expand/Collapse or Collapbsible Markdown: I often add whole output to every command I use. This can take a large space in a blog post and might disrupt reading focus. Setup: Jekyll Text Expand or Collapsible Markdown "
    }, {
    "id": 148,
    "url": "https://www.tomordonez.com/jekyll-theme-customization/",
    "title": "Jekyll Theme Customization",
    "body": "2020/06/01 - Customize your Jekyll theme. The default Jekyll theme is minima. The docs have good details on how to set it up. It has instructions on how to set your _config. yml 	Here is a template:		title: Name and Title of My Blog	email: 	description: &gt;-	 Some awesome description here	baseurl:   	url:  https://www. mywebsite. com 	twitter_username: mytwitter	github_username: mygithub	permalink: /:title/	# Build settings	theme: minima	minima:	 skin: solarized	header_pages:	 - about. md	disqus:	  shortname: mydisqus_shortname	author:	 name: My Name	show_excerpts: true	minima:	 social_links:	  twitter: mytwitter	  github: mygithub	  linkedin: mylinkedin_shortname	google_analytics: myGAcode	plugins:	 - jekyll-feed	 - jekyll-feed	 - jekyll-sitemap	 - jekyll-paginate	 - jekyll-seo-tag	 - jekyll-redirect-from	exclude:	 - . sass-cache/	 - . jekyll-cache/	 - gemfiles/	 - Gemfile	Jekyll Theme Customization: The default Jekyll theme is installed as a gem and you won’t see the source files in your blog directory. To find the source files run this: $ bundle info minimaMy output was this: * minima (2. 5. 1)Summary: A beautiful, minimal theme for Jekyll. Homepage: https://github. com/jekyll/minimaPath: /home/tom/. rvm/gems/ruby-2. 7. 1@blog/gems/minima-2. 5. 1If you open this path, the README file shows where files are located:  The _layouts directory define the markup for your theme.  The _includes directory has snippets of code that can be inserted in layouts.  The _sass directory define the theme’s styles.  The assets directory contains the main. scss. The main. scss imports sass files from the _sass directory. It gets processed into the theme’s main stylesheet main. css called by _layouts/default. html via _includes/head. html. To override the default structure and style, create the specific directory at the root of the blog, copy the file to that directory, and then edit the file. For example:  To override the _includes/head. html.  Create an _includes directory in the root of your blog.  Copy _includes/head. html from minima gem folder to this directory.  Edit that file. Updating the default CSS:  Go to the gem path.  Copy the assets/ folder to your blog root.  Edit the /assets/main. scss file. Using a different Jekyll theme: I tried a Jekyll theme that looked like Medium Instead of installing Jekyll, do the following: $ git clone https://github. com/wowthemesnet/mundana-theme-jekyll. git blog$ cd blog$ bundle$ bundle exec jekyll serveI liked it for a while but it had some weird bugs. I spent many hours trying to fix them but then I gave up and switched back to the default minima Jekyll theme. See a step by step setup tutorial of Jekyll with Github Pages "
    }, {
    "id": 149,
    "url": "https://www.tomordonez.com/d3-convert-string-to-date/",
    "title": "D3 Convert String to Date",
    "body": "2020/06/01 - In D3 convert a string to date format after loading a CSV file. When loading a CSV file in D3, the data is parsed as strings. D3 Load a CSV file with Promises If this is the data contained in the CSV: year,population1950,51951,101955,151959,20It will parse the years as strings and the population as strings. You need to convert these strings to the correct type in D3.  year: From string to date format.  population: From string to integer format. Convert string to date in D3: You can use this syntax and see the resources for more details: var parseTime = d3. timeParse( %Y );var formatTime = d3. timeFormat(specifier);More details here:  %Y: Parse year in decimal number such as 2020.  specifier: More details here d3. timeParse d3. timeFormatAdd this code to a project. js file. See Setup D3 Step by Step: var parseTime = d3. timeParse( %Y );Pass a function as a parameter when loading the CSV. First we need to create the function. A function to parse every row of the CSV file: This function is passed as a parameter of the d3. csv method. It takes every row as d. It parses the year from parseTime(d. year) and for population, it uses the + operator to force the string to numbers on +d. population. var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  };}Clear the console and reload the browser. See if there are any errors in the console or in the Python server. Loading a CSV file with rowConverter: Load the csv file and use rowConverter: d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset);});Add this to project. js and reload the browser: var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  };}d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset);});This is what the code does:  It opens file. csv and passes the function rowConverter as a parameter.  It takes every row using the variable d of file. csv.  Parses the year from a string to a parseTime(d. year) format.  Parses the population from a string to an integer using the + operator +d. population.  Then print the values in the console. The console shows: (4) [{…}, {…}, {…}, {…}, columns: Array(2)]  0: {year: Sun Jan 01 1950 00:00:00 GMT-0500 (Eastern Standard Time), population: 5}  1: {year: Mon Jan 01 1951 00:00:00 GMT-0500 (Eastern Standard Time), population: 10}  2: {year: Sat Jan 01 1955 00:00:00 GMT-0500 (Eastern Standard Time), population: 15}  3: {year: Thu Jan 01 1959 00:00:00 GMT-0500 (Eastern Standard Time), population: 20}  columns: (2) [ year ,  population ]  length: 4  __proto__: Array(0)Next: D3 Bind data to DOM "
    }, {
    "id": 150,
    "url": "https://www.tomordonez.com/google-maps-api-python/",
    "title": "Google Maps API with Python",
    "body": "2020/05/30 - This is a tutorial to extract data from the Google Maps API using Python. Get a Google Maps API key: As shown in get API key. Go to the GCP dashboard:  APIs &amp; Services Then Credentials Click on Create Credentials Then API keyEnable a Google Maps API service:  Go to the Menu Scroll down to Google Maps Then APIs Click Places API Click EnableThese are all available API services: Places APIMaps SDK for AndroidDirections APIDistance Matrix APIMaps Elevation APIMaps Embed APIGeocoding APIGeolocation APIMaps JavaScript APIRoads APIMaps SDK for iOSTime Zone APIMaps Static APIStreet View Static APIAPI Key &gt; Settings &gt; Application Restrictions: Without restrictions the API key created has a warning icon. Edit the Settings for the API key. Edit the name. For example: Google Maps Places API Key Then Application Restrictions controls which websites or IP addresses or applications can use the API key. You can only choose one of:  None: for testing purposes only.  HTTP websites:for API clients that run on a web browser. These types of applications expose the API publicly. Use a service account as shown here.  IP addresses: web servers, cron jobs, etc.  Android apps iOS appsGoogle Maps API key best practices: More about this in API key best practices, more best practices, and API key restrictions One of the best practices says: You may use an unrestricted API with the Google Maps API. However, it is recommended to restrict the API keys in the following scenarios:  The test environment is public The applications that uses the API key is ready to be used in production. Not sure if this is recommended. I am only going to pull data from a local test environment. I could set it to None. For now I set it to IP addresses and entered my public IP address. API Key &gt; Settings &gt; API restrictions: As shown in API restrictions, it says that all API keys used in production should use API restrictions. The API restrictions specify the enabled APIs that can be called:  Don’t restrict the key Restrict the keyFirst I am going to test finding a place. Select Restrict the key and from the drop down choose Places API Then click Save. The API key now has a green check mark. These are all the API restrictions options: BigQuery APIBigQuery Storage APICloud Datastore APICloud Debugger APICloud Logging APICloud Monitoring APICloud SQLCloud StorageCloud Trace APIGoogle Cloud APIsGoogle Cloud Storage JSON APIPlaces APIService Management APIService Usage APIGoogle Maps Places API: More about the Places API here Each service uses an HTTP request and returns JSON. The services use a place_id to uniquely identify a place. This is important to review duplicates later. Find the Place ID of a place here Place Search: Resources:  Google Maps DocsAs shown on the docs, Places API lets you search for place information using categories, establishments, points of interest, and geographic locations. Search by proximity or by string. It returns a list of places along with summary information. Types of Requests  Find Place Nearby Search Text SearchFind Place requests: Input is a text. Output is a place. Input can be a name, address, or phone number. It cannot be a lat/long number. The request must be a string. The URL has this form: https://maps. googleapis. com/maps/api/place/findplacefromtext/output?parametersRequired parameters  key: Your API key input: A string that can be a name, address, or phone number.  inputtype: Use textquery or phonenumber. Example, using Google Maps Python client library. I will explain the setup later: &gt;&gt;&gt; place = gmaps. find_place(input= 1600 Amphitheatre Parkway, Mountain View, CA , input_type= textquery )&gt;&gt;&gt; place{'candidates': [{'place_id': 'ChIJtYuu0V25j4ARwu5e4wwRYgE'}], 'status': 'OK'}Other parameters  language: For example English language= en .  locationbias: Find a specific area by using a radius and lat/long. If this is not used, the API uses IP address.  fields: Types of data to return. Language: Language codes. locationbias: Use like this: circle:radius@lat,lngFor example. Search for steakhouse within 2000 meters of Google’s HQ office. https://maps. googleapis. com/maps/api/place/findplacefromtext/json?input=steakhouse&amp;inputtype=textquery&amp;fields=photos,formatted_address,name,opening_hours,rating&amp;locationbias=circle:2000@37. 4222339,-122. 0854804&amp;key=YOUR_API_KEYFields: More in the official docs This is where it gets interesting…Billing. Types of Fields:  Basic: business_status, formatted_address, geometry, icon,name, permanently_closed, photos, place_id, plus_code, types Contact: opening_hours Atmosphere: price_level, rating, user_ratings_totalSome services don’t allow to specify a type of field and different billing charges apply if one of them is triggered. A way to test this is looking at the response and see if it contains a field for one of these types. Example using Google Maps API Python client library: &gt;&gt;&gt; place = gmaps. find_place(input= stakehouse , input_type= textquery , location_bias= circle:2000@37. 4222339,-122. 0854804 , language= en )&gt;&gt;&gt; place{'candidates': [{'place_id': 'ChIJsUEYn56wj4ARMg1qK1EVHyw'}], 'status': 'OK'}It doesn’t return much. Let’s use Basic fields &gt;&gt;&gt; place = gmaps. find_place(input= stakehouse , input_type= textquery , fields=[ name ,  formatted_address ,  business_status ,  geometry ,  photos ,  types ], location_bias= circle:2000@37. 4222339,-122. 0854804 , language= en )&gt;&gt;&gt; place{'candidates': [{'business_status': 'CLOSED_TEMPORARILY', 'formatted_address': '545 San Antonio Rd Suite 31, Mountain View, CA 94040, United States', 'geometry': {'location': {'lat': 37. 4032079, 'lng': -122. 1118804}, 'viewport': {'northeast': {'lat': 37. 40471237989271, 'lng': -122. 1105114701073}, 'southwest': {'lat': 37. 40201272010727, 'lng': -12. 1132111298927}}}, 'name':  Paul Martin's America Mountain View , 'photos': [{'height': 4048, 'html_attributions': ['&lt;a href= https://maps. google. com/maps/contrib/11203600442766896285 &gt;Casey DuBose&lt;/a&gt;'], 'photo_reference': 'CmRaAAAAit6MjPA4tMxwkAx61ZquIzBYndTl5zAcCV-bjUPhl0dm0S3giXjEANdqAvxvxsJvCehIChMcOCPVJwxIzHAQWW9Igv01P_R-gilhmU52I0MSRgBgWXh4g5N7wRQDPQKEhC2y0uoOD03_XFjS6o7xi0UGhTiLFIHq8rbIF68PRZaCoEjumKy_Q', 'width': 3036}], 'types': ['restaurant', 'food', 'point_of_interest', 'establishment']}], 'status': 'OK'}Let’s confirm by searching for Paul Martin's America Mountain View:  Restaurant Temporarily closed 545 San Antonio Rd Suite 31, Mountain View, CA 94040 10 min drive, 3. 1 miles from Google’s office.  4. 2 stars (628 reviews) $$$Let’s use the Contact (opening_hours) and Atmosphere (price_level, rating, user_ratings_total) fields to see if we get the same data: &gt;&gt;&gt; place = gmaps. find_place(input= stakehouse , input_type= textquery , fields=[ name ,  formatted_address ,  business_status ,  geometry ,  photos ,  types ,  opening_hours ,  price_level ,  rating ,  user_ratings_total ], location_bias= circle:2000@37. 4222339,-122. 0854804 , language= en )&gt;&gt;&gt; place{'candidates': [{'business_status': 'CLOSED_TEMPORARILY', 'formatted_address': '545 San Antonio Rd Suite 31, Mountain View, CA 94040, United States', 'geometry': {'location' {'lat': 37. 4032079, 'lng': -122. 1118804}, 'viewport': {'northeast': {'lat': 37. 40471237989271, 'lng': -122. 1105114701073}, 'southwest': {'lat': 37. 40201272010727, 'lng': -12. 1132111298927}}}, 'name':  Paul Martin's America Mountain View , 'photos': [{'height': 4048, 'html_attributions': ['&lt;a href= https://maps. google. com/maps/contrib/11203600442766896285 &gt;Casey DuBose&lt;/a&gt;'], 'photo_reference': 'CmRaAAAAAUrcGJCnapLA_JhEgpZqkZy4f_B7a_jCCGESU-WWMBi50Q8ILEV818OWisSjzvBzfHA_WtdDn_45BLmrtZV5KSsrPNpxpH6mb47NSp56qzeazy9TSUje4I6ZMT4A7BIEhCJ2IM68_GY0kwK4w1IttDpGhRU3BWNHXrsBJnv2oPseqsUAyElUA', 'width': 3036}], 'price_level': 3, 'rating': 4. 2, 'types': ['restaurant', 'food', 'point_of_interest' 'establishment'], 'user_ratings_total': 628}], 'status': 'OK'}Nearby Search Requests: Search within a specific area. Details in Google Maps Docs. The warning on the Doc says  It returns all available data fields You will be billed if it triggers types of fields (basic, contact, atmosphere) You can’t constraint to return only specific fields To keep from paying data you don’t need. Then use Find Place RequestsI used this service and it triggers all types of fields. You will be billed for every SKU. More about billing later. The URL has this form: https://maps. googleapis. com/maps/api/place/nearbysearch/output?parametersRequired Parameters  key location: Specified as latitude,longitude radius: Defined in meters with a max of 50,000 (31 miles)Optional Parameters  keyword: Matching all content that Google indexed for this place (name, address, type, customer reviews) language: Same as before. See Language codes.  rankyby: Use distance and one or more of keyword or language, to rank in ascending order from specified location type: See location type pagetoken: Returns up to 20 results from a previous searchExample, using Google Maps Python client library: &gt;&gt;&gt; place = gmaps. places_nearby(location=(37. 4222339,-122. 0854804), radius=5000, keyword= startup , language= en )&gt;&gt;&gt; place{'html_attributions': [], 'results': [], 'status': 'ZERO_RESULTS'}&gt;&gt;&gt; place = gmaps. places_nearby(location=(37. 4222339,-122. 0854804), radius=10000, keyword= startup , language= en )&gt;&gt;&gt; place{'html_attributions': [], 'next_page_token': 'CqQCGQEAALespF2ND-o3SI0I5SZR267A4XxtZ4LDBCBCG8SJ_IRp9ttbncIziuYJtpUNuP3pPan80VgualTqejpOwMQyHVIIKNv7MKqdJTBwVpbb6SOoSjskx6yDgaBd5WGiKBfi7TkrCtxe5yOvJYbU6t5KH2jLrqHMLg_4woyq0_TuZTICEpU1veSLv49pLfltDXPg0bKjZSwXejRSUrwFmSfEfRlopfUhBPZNFgGhl1_Qg2XG8R9aHhrPDNhYUoZWQ3HejWw3xVeMpS5_4w_43Qy0wRemnVCPSw7FHz3oAoCfFDFtGp66i3LlQlwQ2Vdraen-NyxoMOLKonL4YfSYst-6kyFzzCIy_Cr-w6VKCCoGJmPxLR07fjZoiE-c6pCfqepxIQC67lM0MvZV-sSAKkgp8eQRoUnTTz_OZwMEsZqsG7PzWFuQffv7Y', 'results': [{'business_sttus': 'OPERATIONAL', 'geometry': {'location': {'lat': 37. 4522222, 'lng': -122. 1661111}, 'viewport': {'northeast': {'lat': 37. 45368197989271, 'lng': -122. 1650791201073}, 'souhwest': {'lat': 37. 45098232010727, 'lng': -122. 1677787798927}}}, 'icon': 'https://maps. gstatic. com/mapfiles/place_api/icons/generic_business-71. png', 'id': '83aa3dba2f9f4aebb49ed92f74abe010398f16a', 'name': 'BootUp Ventures: Startup Ecosystem Co-Working, Office Suites &amp; Event Space', 'opening_hours': {'open_now': False}, 'photos': [{'height': 124, 'html_attributions': ['&lt;a href= https://maps. google. com/maps/contrib/109071310568073474532 &gt;Boost Ventures&lt;/a&gt;'], 'photo_reference': 'CmRaAAAAVHi9Mc5N4rwtg7oSunQr545R18PsgHgh92ODUQ4pL_Sc_s6nhBkk4_xK027AdctQEofpwVFlpTTkM2Bs-URTUqISKYOZqWBFgupe7NDTUSb68oC32Y8AV5x5iUbh65OEhAO71DTWNwWYDK2X04iPiaMGhTPZrlZvJcTN38b7lcMAXhOMevL5g', 'width': 2152}],'place_id': 'ChIJS7Ac0Qu7j4ARkgnXFqygEM8', 'plus_code': {'compound_code': 'FR2M+VH Menlo Park, California', 'global_code': '849VFR2M+VH'}, 'rating': 4. 8, 'reference': 'ChIJSAc0Qu7j4ARkgnXFqygEM8', 'scope': 'GOOGLE', 'types': ['point_of_interest', 'establishment'], 'user_ratings_total': 63, 'vicinity': '68 Willow Rd, Menlo Park'}, {'business_staus': 'OPERATIONAL', 'geometry': {'location': {'lat': 37. 458447, 'lng': -122. 1729745}, 'viewport': {'northeast': {'lat': 37. 45983937989272, 'lng': -122. 1715244701073}, 'southest': {'lat': 37. 45713972010728, 'lng': -122. 1742241298927}}}, 'icon': 'https://maps. gstatic. com/mapfiles/place_api/icons/generic_business-71. png', 'id': '1a4337e0ce2b385bb905778fbc45fb6a0daba2c', 'name': 'Startup Capital Ventures', '&gt;&gt;&gt; [item['name'] for item in place['results']]['BootUp Ventures: Startup Ecosystem Co-Working, Office Suites &amp; Event Space', 'Startup Capital Ventures', 'Startup Rabbit', 'Cab Startup', 'Startup Launchpad, Inc', 'BootUpWorld', 'Startup Realty', 'Plug and Play Tech Center', 'Mercury', 'Palo-Alto Startup House', 'Bay Area Startups Services, Inc', 'Nuro', 'The Hive', 'Starship Technologies', EquityBee', 'Fyde', 'HelloStartups', 'sFoundation Inc. ', 'Y Combinator', 'Osaka Innovation Hub Silicon Valley Office']Text Search Requests: Returns information for a set of places based on a string. Details in Google Maps Docs. For example startups in Mountain View. The service is used for ambiguous address queries. More in Geocoding addresses best practices. Other examples:  Incomplete addresses Poorly formatted addresses Non-address components like business namesThe warning on the Doc says  It returns all available data fields You will be billed if it triggers types of fields (basic, contact, atmosphere) You can’t constraint to return only specific fields To keep from paying data you don’t need. Then use Find Place RequestsThe URL has this form: https://maps. googleapis. com/maps/api/place/textsearch/output?parametersRequired Parameters  key query: Search string. This parameter becomes optional when you use typeOptional Parameters  region: Country code top level domain. Wikipedia location: A latitude,longitude. You must use radius radius: Distance in meters, 50,000 max (31 miles) language For example language= en  pagetoken: Used to fetch the next page.  type: Restrict to a type of place as seen in supported typesGeocoding Request: You can use these two services:  Geocoding to convert an address to latitude,longitude.  Reverse Geocoding to find the address from lat/long or from place_idMore details in the Geocoding API Here is the URL example: https://maps. googleapis. com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&amp;key=YOUR_API_KEYUsing the Google Maps API Python client library: &gt;&gt;&gt; place = gmaps. geocode(address= 1600 Amphitheatre Parkway, Mountain View, CA )googlemaps. exceptions. ApiError: REQUEST_DENIED (This API project is not authorized to use this API. )Mmm. What happened? I didn’t enable the Geocode API or create a Key for this service. Enable Geocoding API:  Go to GCP Menu &gt; Google Maps &gt; APIs Click Geocoding API (Billing: $5/1K requests 0-100K req/mo) Click EnableCreate a Geocoding API Key: Create a different API key for this service as seen on API key best practices  Go to GCP Menu &gt; APIs &amp; Services &gt; Credentials Create Credentials &gt; API key A message says API key created Click on Restrict Key Change name to Google Maps Geocoding API key Choose an Application Restriction For API restrictions change to Restrict key and select Geocoding API Click SaveTry again. I will explain the setup later: &gt;&gt;&gt; place = gmaps_geo. geocode(address= 1600 Amphitheatre Parkway, Mountain View, CA )&gt;&gt;&gt; place[{'access_points': [{'access_point_type': 'TYPE_SEGMENT', 'location': {'latitude': 37. 4213102, 'longitude': -122. 0852443}, 'location_on_segment': {'latitude': 37. 4212816, 'lngitude': -122. 0852472}, 'place_id': 'ChIJpdYZQgK6j4ARnmfrthhmnZ8', 'segment_position': 0. 5404474139213562, 'unsuitable_travel_modes': []}], 'address_components': [{'long_nae': '1600', 'short_name': '1600', 'types': ['street_number']}, {'long_name': 'Amphitheatre Parkway', 'short_name': 'Amphitheatre Pkwy', 'types': ['route']}, {'long_name': 'Muntain View', 'short_name': 'Mountain View', 'types': ['locality', 'political']}, {'long_name': 'Santa Clara County', 'short_name': 'Santa Clara County', 'types': ['administative_area_level_2', 'political']}, {'long_name': 'California', 'short_name': 'CA', 'types': ['administrative_area_level_1', 'political']}, {'long_name': 'United States', 'sort_name': 'US', 'types': ['country', 'political']}, {'long_name': '94043', 'short_name': '94043', 'types': ['postal_code']}], 'formatted_address': '1600 Amphitheatre Pkwy, ountain View, CA 94043, USA', 'geometry': {'location': {'lat': 37. 4223105, 'lng': -122. 0846329}, 'location_type': 'ROOFTOP', 'viewport': {'northeast': {'lat': 37. 423659480295, 'lng': -122. 0832839197085}, 'southwest': {'lat': 37. 42096151970851, 'lng': -122. 0859818802915}}}, 'place_id': 'ChIJtYuu0V25j4ARwu5e4wwRYgE', 'plus_code': {'compound_code' 'CWC8+W4 Mountain View, CA, United States', 'global_code': '849VCWC8+W4'}, 'types': ['street_address']}]Python client library or http. client or requests: You can use any of these options:  Google Maps has a Python client library. Github http. client is a Python native library, part of the urllib. request module. Python docs requests is a Python external library. ReadthedocsA quick search of requests vs http. client shows Why is Python 3 http. client so much faster than python-requests?. An answer says that http. client is at a lower level on the stack, while requests can be slower because it adds other features. Stackoverflow. Another more popular query was requests vs urllib, which shows this question: What are the differences between the urllib, urllib2, urllib3 and requests module?. The answer with the most votes recommends to use requests for its simplicity and added features. The Google Maps Python client library requires requests upon installation. I think that this library might be just a wrapper for requests. Google Maps API with Google Maps Python client library: Resources:  Github Tests DocsInstall: $ pip install -U googlemaps --userOutput of installation was: Downloading https://files. pythonhosted. org/packages/cb/87/5cbe65cd19defe67472db7afd84963a77fbbbe4a764320a67d4a64282b61/googlemaps-4. 4. 1. tar. gzRequirement already satisfied, skipping upgrade: requests&lt;3. 0,&gt;=2. 20. 0 in /usr/lib/python3. 7/site-packages (from googlemaps) (2. 22. 0)Requirement already satisfied, skipping upgrade: chardet&lt;3. 1. 0,&gt;=3. 0. 2 in /usr/lib/python3. 7/site-packages (from requests&lt;3. 0,&gt;=2. 20. 0-&gt;googlemaps) (3. 0. 4)Requirement already satisfied, skipping upgrade: idna&lt;2. 9,&gt;=2. 5 in /usr/lib/python3. 7/site-packages (from requests&lt;3. 0,&gt;=2. 20. 0-&gt;googlemaps) (2. 8)Requirement already satisfied, skipping upgrade: urllib3!=1. 25. 0,!=1. 25. 1,&lt;1. 26,&gt;=1. 21. 1 in /usr/lib/python3. 7/site-packages (from requests&lt;3. 0,&gt;=2. 20. 0-&gt;googlemaps) (1. 25. 7)Installing collected packages: googlemaps Running setup. py install for googlemaps . . . doneSuccessfully installed googlemaps-4. 4. 1Docs on the Python shell: &gt;&gt;&gt; help(googlemaps)Setup code: &gt;&gt;&gt; import googlemaps&gt;&gt;&gt; gmaps = googlemaps. Client(key='YOUR KEY')Search for a stakehouse near Google’s office: &gt;&gt;&gt; place = gmaps. find_place(input= stakehouse , input_type= textquery , location_bias= circle:2000@37. 4222339,-122. 0854804 , language= en )&gt;&gt;&gt; place{'candidates': [{'place_id': 'ChIJsUEYn56wj4ARMg1qK1EVHyw'}], 'status': 'OK'}It doesn’t return much. Let’s use Basic fields &gt;&gt;&gt; place = gmaps. find_place(input= stakehouse , input_type= textquery , fields=[ name ,  formatted_address ,  business_status ,  geometry ,  photos ,  types ], location_bias= circle:2000@37. 4222339,-122. 0854804 , language= en )&gt;&gt;&gt; place{'candidates': [{'business_status': 'CLOSED_TEMPORARILY', 'formatted_address': '545 San Antonio Rd Suite 31, Mountain View, CA 94040, United States', 'geometry': {'location': {'lat': 37. 4032079, 'lng': -122. 1118804}, 'viewport': {'northeast': {'lat': 37. 40471237989271, 'lng': -122. 1105114701073}, 'southwest': {'lat': 37. 40201272010727, 'lng': -12. 1132111298927}}}, 'name':  Paul Martin's America Mountain View , 'photos': [{'height': 4048, 'html_attributions': ['&lt;a href= https://maps. google. com/maps/contrib/11203600442766896285 &gt;Casey DuBose&lt;/a&gt;'], 'photo_reference': 'CmRaAAAAit6MjPA4tMxwkAx61ZquIzBYndTl5zAcCV-bjUPhl0dm0S3giXjEANdqAvxvxsJvCehIChMcOCPVJwxIzHAQWW9Igv01P_R-gilhmU52I0MSRgBgWXh4g5N7wRQDPQKEhC2y0uoOD03_XFjS6o7xi0UGhTiLFIHq8rbIF68PRZaCoEjumKy_Q', 'width': 3036}], 'types': ['restaurant', 'food', 'point_of_interest', 'establishment']}], 'status': 'OK'}Search for anything that contains startup near Google’s office within 5,000 meters: &gt;&gt;&gt; place = gmaps. places_nearby(location=(37. 4222339,-122. 0854804), radius=5000, keyword= startup , language= en )&gt;&gt;&gt; place{'html_attributions': [], 'results': [], 'status': 'ZERO_RESULTS'}No results. Let’s try again within 10,000 meters (Max is 50,000) &gt;&gt;&gt; place = gmaps. places_nearby(location=(37. 4222339,-122. 0854804), radius=10000, keyword= startup , language= en )&gt;&gt;&gt; place{'html_attributions': [], 'next_page_token': 'CqQCGQEAALespF2ND-o3SI0I5SZR267A4XxtZ4LDBCBCG8SJ_IRp9ttbncIziuYJtpUNuP3pPan80VgualTqejpOwMQyHVIIKNv7MKqdJTBwVpbb6SOoSjskx6yDgaBd5WGiKBfi7TkrCtxe5yOvJYbU6t5KH2jLrqHMLg_4woyq0_TuZTICEpU1veSLv49pLfltDXPg0bKjZSwXejRSUrwFmSfEfRlopfUhBPZNFgGhl1_Qg2XG8R9aHhrPDNhYUoZWQ3HejWw3xVeMpS5_4w_43Qy0wRemnVCPSw7FHz3oAoCfFDFtGp66i3LlQlwQ2Vdraen-NyxoMOLKonL4YfSYst-6kyFzzCIy_Cr-w6VKCCoGJmPxLR07fjZoiE-c6pCfqepxIQC67lM0MvZV-sSAKkgp8eQRoUnTTz_OZwMEsZqsG7PzWFuQffv7Y', 'results': [{'business_sttus': 'OPERATIONAL', 'geometry': {'location': {'lat': 37. 4522222, 'lng': -122. 1661111}, 'viewport': {'northeast': {'lat': 37. 45368197989271, 'lng': -122. 1650791201073}, 'souhwest': {'lat': 37. 45098232010727, 'lng': -122. 1677787798927}}}, 'icon': 'https://maps. gstatic. com/mapfiles/place_api/icons/generic_business-71. png', 'id': '83aa3dba2f9f4aebb49ed92f74abe010398f16a', 'name': 'BootUp Ventures: Startup Ecosystem Co-Working, Office Suites &amp; Event Space', 'opening_hours': {'open_now': False}, 'photos': [{'height': 124, 'html_attributions': ['&lt;a href= https://maps. google. com/maps/contrib/109071310568073474532 &gt;Boost Ventures&lt;/a&gt;'], 'photo_reference': 'CmRaAAAAVHi9Mc5N4rwtg7oSunQr545R18PsgHgh92ODUQ4pL_Sc_s6nhBkk4_xK027AdctQEofpwVFlpTTkM2Bs-URTUqISKYOZqWBFgupe7NDTUSb68oC32Y8AV5x5iUbh65OEhAO71DTWNwWYDK2X04iPiaMGhTPZrlZvJcTN38b7lcMAXhOMevL5g', 'width': 2152}],'place_id': 'ChIJS7Ac0Qu7j4ARkgnXFqygEM8', 'plus_code': {'compound_code': 'FR2M+VH Menlo Park, California', 'global_code': '849VFR2M+VH'}, 'rating': 4. 8, 'reference': 'ChIJSAc0Qu7j4ARkgnXFqygEM8', 'scope': 'GOOGLE', 'types': ['point_of_interest', 'establishment'], 'user_ratings_total': 63, 'vicinity': '68 Willow Rd, Menlo Park'}, {'business_staus': 'OPERATIONAL', 'geometry': {'location': {'lat': 37. 458447, 'lng': -122. 1729745}, 'viewport': {'northeast': {'lat': 37. 45983937989272, 'lng': -122. 1715244701073}, 'southest': {'lat': 37. 45713972010728, 'lng': -122. 1742241298927}}}, 'icon': 'https://maps. gstatic. com/mapfiles/place_api/icons/generic_business-71. png', 'id': '1a4337e0ce2b385bb905778fbc45fb6a0daba2c', 'name': 'Startup Capital Ventures', '&gt;&gt;&gt; [item['name'] for item in place['results']]['BootUp Ventures: Startup Ecosystem Co-Working, Office Suites &amp; Event Space', 'Startup Capital Ventures', 'Startup Rabbit', 'Cab Startup', 'Startup Launchpad, Inc', 'BootUpWorld', 'Startup Realty', 'Plug and Play Tech Center', 'Mercury', 'Palo-Alto Startup House', 'Bay Area Startups Services, Inc', 'Nuro', 'The Hive', 'Starship Technologies', EquityBee', 'Fyde', 'HelloStartups', 'sFoundation Inc. ', 'Y Combinator', 'Osaka Innovation Hub Silicon Valley Office']Find the lat,long of Google’s office: &gt;&gt;&gt; gmaps_geo = googlemaps. Client(key='YOUR GEOCODING KEY')&gt;&gt;&gt; place = gmaps_geo. geocode(address= 1600 Amphitheatre Parkway, Mountain View, CA )&gt;&gt;&gt; place[{'access_points': [{'access_point_type': 'TYPE_SEGMENT', 'location': {'latitude': 37. 4213102, 'longitude': -122. 0852443}, 'location_on_segment': {'latitude': 37. 4212816, 'lngitude': -122. 0852472}, 'place_id': 'ChIJpdYZQgK6j4ARnmfrthhmnZ8', 'segment_position': 0. 5404474139213562, 'unsuitable_travel_modes': []}], 'address_components': [{'long_nae': '1600', 'short_name': '1600', 'types': ['street_number']}, {'long_name': 'Amphitheatre Parkway', 'short_name': 'Amphitheatre Pkwy', 'types': ['route']}, {'long_name': 'Muntain View', 'short_name': 'Mountain View', 'types': ['locality', 'political']}, {'long_name': 'Santa Clara County', 'short_name': 'Santa Clara County', 'types': ['administative_area_level_2', 'political']}, {'long_name': 'California', 'short_name': 'CA', 'types': ['administrative_area_level_1', 'political']}, {'long_name': 'United States', 'sort_name': 'US', 'types': ['country', 'political']}, {'long_name': '94043', 'short_name': '94043', 'types': ['postal_code']}], 'formatted_address': '1600 Amphitheatre Pkwy, ountain View, CA 94043, USA', 'geometry': {'location': {'lat': 37. 4223105, 'lng': -122. 0846329}, 'location_type': 'ROOFTOP', 'viewport': {'northeast': {'lat': 37. 423659480295, 'lng': -122. 0832839197085}, 'southwest': {'lat': 37. 42096151970851, 'lng': -122. 0859818802915}}}, 'place_id': 'ChIJtYuu0V25j4ARwu5e4wwRYgE', 'plus_code': {'compound_code' 'CWC8+W4 Mountain View, CA, United States', 'global_code': '849VCWC8+W4'}, 'types': ['street_address']}] "
    }, {
    "id": 151,
    "url": "https://www.tomordonez.com/jekyll-search-bar/",
    "title": "Jekyll Search Bar",
    "body": "2020/05/27 - Add a search bar to Jekyll. Setup Jekyll Search: Here is the documentation to Search with Lunr. js Save the file search-lunr. html in _includes. In this file, you can exclude the types of documents to search. For example: if page. url contains '. xml' or page. url contains 'assets' or page. url contains '. json' or page. url contains 'about. html'Download the file lunr. js into your js folder, then make sure that search-lunr. html indicates the correct location of the file. For example: src= /assets/js/lunr. js I copied the default. html layout file from my Gem location to the _layouts directory: cp /home/tom/. rvm/gems/ruby-2. 7. 1@blog/gems/minima-2. 5. 1/_layouts/default. html _layouts/Inside the default. html layout page, include the search-lunr. html as indicated in the docs inside curly percentage brackets. Add this in the main class, before the content tag. include search-lunr. htmlCSS for Jekyll Search: Customize the CSS for the search box. At the bottom of search-lunr. html there is code with the form. You can wrap this in a class: &lt;div class= search &gt;  &lt;form onSubmit= return lunr_search(document. getElementById('lunrsearch'). value); &gt;    &lt;p&gt;&lt;input type= text  class= form-control  id= lunrsearch  name= q  maxlength= 255  value=   placeholder= Search  /&gt;&lt;/p&gt;  &lt;/form&gt;&lt;/div&gt;Then in assets/main. scss you can try something like this: . search input {  height: 30px;  width: 60%;  padding-left: 10px;  border: 1px solid #D9D9D9;  border-radius: 10px;  font-size: 16px;}"
    }, {
    "id": 152,
    "url": "https://www.tomordonez.com/d3-load-a-csv-file-with-promises/",
    "title": "D3 Load a CSV file with Promises",
    "body": "2020/05/27 - Loading a CSV file in D3 with Promises. Also load CSV, JSON, DSV, and TSV files in D3. More about D3 and Incompatible Versions. D3 version 5 uses a feature called Promises. Use this syntax: d3. csv( file. csv ). then(function(dataset) {  console. log(dataset);});The console shows this: Promise {&lt;pending&gt;}(4) [{…}, {…}, {…}, {…}, columns: Array(2)]  0: {year:  1950 , population:  5 }  1: {year:  1951 , population:  10 }  2: {year:  1955 , population:  15 }  3: {year:  1959 , population:  20 }  columns: (2) [ year ,  population ]  length: 4  __proto__: Array(0)Or use this syntax, it shows the same console output: var mydata = d3. csv( file. csv )mydata. then(function(dataset) {  console. log(dataset);});Or you can load multiple datasets in D3 and Promises like this: var data1 = d3. csv( file1. csv )var data2 = d3. csv( file2. csv )Promise. all([data1, data2]). then(someFunction)function someFunction(values) {  somethingFunHere(values);}More about D3 Promises in the Observable blog and this blog post. Loading data with CSV, DSV, TSV or JSON: There are a few methods to load data in D3:  d3. dsv this means delimiter separated values d3. csv d3. tsv d3. jsonYou can use d3. dsv like this and explicitly define the separator: d3. dsv( , ,  file. csv ). then(function(dataset) {  console. log(dataset);});Console shows: Promise {&lt;pending&gt;}(4) [{…}, {…}, {…}, {…}, columns: Array(2)]  0: {year:  1950 , population:  5 }  1: {year:  1951 , population:  10 }  2: {year:  1955 , population:  15 }  3: {year:  1959 , population:  20 }  columns: (2) [ year ,  population ]  length: 4  __proto__: Array(0)Or use d3. csv as I have shown before: d3. csv( file. csv ). then(function(dataset) {  console. log(dataset);});Or use d3. json like this. Let’s create a data. json and add this: [ {   year : 1950,   population : 5 }, {   year : 1951,   population : 10 }, {   year : 1955,   population : 15 }, {   year : 1959,   population : 20 }]Then load the data: d3. json( data. json ). then(function(dataset) {  console. log(dataset);});Console shows the same output: Promise {&lt;pending&gt;}(4) [{…}, {…}, {…}, {…}]  0: {year: 1950, population: 5}  1: {year: 1951, population: 10}  2: {year: 1955, population: 15}  3: {year: 1959, population: 20}  length: 4  __proto__: Array(0)Next: D3 Creating a Bar Chart "
    }, {
    "id": 153,
    "url": "https://www.tomordonez.com/static-website-jekyll-github-pages/",
    "title": "Static Website with Jekyll and Github Pages",
    "body": "2020/05/25 - This is a step by step tutorial to build a Github Pages site with Jekyll. Github Pages are freely hosted on Github and you can use Jekyll, a static website generator to customize your Github Pages site. Summary:  Install Ruby Jekyll and Github Pages Customizing the theme Using SEO Creating blog posts Github Pages custom domain Change your DNS name server Deploying custom plugins Google Analytics Pagination Jekyll without Plugins Reading Time Search Box Text Expand/Collapse Open external site in new windowInstall Ruby: Install RVM as seen here $ gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB$ \curl -sSL https://get. rvm. io | bash -s stableIf you are on Linux/Gnome. As seen here. Go to the Terminal preferences and find a checkbox that says Run command as a login shell. This must be checked. Close the terminal and open it again. $ source ~/. rvm/scripts/rvm$ type rvm | head -n 1This should say rvm is a function Restart the shell and Install Ruby. Stable version on 5/24/20 is 2. 7. 1 $ rvm install 2. 7. 1$ rvm use 2. 7. 1 --defaultCreate a gemset for the blog $ rvm use 2. 7. 1@blog --createList gemsets with: $ rvm gemset listSwitch gemsets with: $ rvm gemset use name-of-gemsetInstall Jekyll: Setup Jekyll: $ gem install bundler jekyllGo to the root directory where you want to install. Then create a new jekyll blog. $ jekyll new blog$ cd blogThis creates the following: 404. htmlabout. markdown_config. ymlGemfileGemfile. lockindex. markdown_posts/Run the blog: $ bundle exec jekyll serveOpen http://localhost:4000 and Ctrl+C to stop Jekyll and Markdown: Jekyll uses Kramdown. Github Pages uses a customized CommonMark markdown.  Jekyll Kramdown markdown Kramdown syntax CommonMark doc CommonMark Github Pages versionJekyll and Github Pages: Look at Github pages dependency versions As of 5/24/20:  jekyll: 3. 8. 5 (even though latest stable was 4. 0. 1) github pages: 204Modify the Gemfile to use Github pages as shown.  Comment this line: gem  jekyll ,  ~&gt; 4. 0. 1  Uncomment this line: gem  github-pages , group: :jekyll_pluginsUpgrade: $ bundle updateOutput was: Note: jekyll version regressed from 4. 0. 1 to 3. 8. 5Run bundle: $ bundle installOutput: Bundle complete! 6 Gemfile dependencies, 85 gems now installed. Use `bundle info [gemname]` to see where a bundled gem is installed. Test again: $ bundle exec jekyll serveMore about Github Pages: Official docs here. A few important details:  There are three types of sites: project, user, and organization.  The default publishing source for project sites is the root of the gh-pages branch. For user/org sites the default source might be master. Review below a section on setting up a user site with custom plugins with the gh-pages branch.  The site cannot be larger than 1GB.  Bandwidth limit of 100GB per month. Troubleshooting dependency errors: When testing the site. For jekyll-3. 8. 5 it says warning: Using the last argument as keyword parameters is deprecated.  Also for pathutil-0. 16. 2 it says the same.  More here And hereEdit the Gemfile and comment this line again gem  github-pages , group: :jekyll_plugins. Then add this one: gem 'jekyll', github: 'jekyll/jekyll'If you have plugins update them to this: group :jekyll_plugins do  gem 'jekyll-feed', github: 'jekyll/jekyll-feed'  gem 'jekyll-sitemap', github: 'jekyll/jekyll-sitemap'  gem 'jekyll-paginate', github: 'jekyll/jekyll-paginate'  gem 'jekyll-seo-tag', github: 'jekyll/jekyll-seo-tag'  gem 'jekyll-redirect-from', github: 'jekyll/jekyll-redirect-from'endMy Gemfile currently looks like this: source  https://rubygems. org gem 'jekyll', github: 'jekyll/jekyll'gem  minima ,  ~&gt; 2. 5 group :jekyll_plugins do  gem 'jekyll-feed', github: 'jekyll/jekyll-feed'  gem 'jekyll-sitemap', github: 'jekyll/jekyll-sitemap'  gem 'jekyll-paginate', github: 'jekyll/jekyll-paginate'  gem 'jekyll-seo-tag', github: 'jekyll/jekyll-seo-tag'  gem 'jekyll-redirect-from', github: 'jekyll/jekyll-redirect-from'endThen run bundle install Using a theme and edit _config. yml: The default theme is minima. The docs have good details on how to set it up. It has instructions on how to set your _config. yml 	Here is a template:		title: Name and Title of My Blog	email: 	description: &gt;-	 Some awesome description here	baseurl:   	url:  https://www. mywebsite. com 	twitter_username: mytwitter	github_username: mygithub	permalink: /:title/	# Build settings	theme: minima	minima:	 skin: solarized	header_pages:	 - about. md	disqus:	  shortname: mydisqus_shortname	author:	 name: My Name	show_excerpts: true	minima:	 social_links:	  twitter: mytwitter	  github: mygithub	  linkedin: mylinkedin_shortname	google_analytics: myGAcode	plugins:	 - jekyll-feed	 - jekyll-feed	 - jekyll-sitemap	 - jekyll-paginate	 - jekyll-seo-tag	 - jekyll-redirect-from	exclude:	 - . sass-cache/	 - . jekyll-cache/	 - gemfiles/	 - Gemfile	Customizing the theme ‘minima’: The default theme is installed as a gem and you won’t see the source files in your blog directory. To find the source files run this: $ bundle info minimaMy output was this: * minima (2. 5. 1)Summary: A beautiful, minimal theme for Jekyll. Homepage: https://github. com/jekyll/minimaPath: /home/tom/. rvm/gems/ruby-2. 7. 1@blog/gems/minima-2. 5. 1If you open this path, the README file shows where files are located:  The _layouts directory define the markup for your theme.  The _includes directory has snippets of code that can be inserted in layouts.  The _sass directory define the theme’s styles.  The assets directory contains the main. scss. The main. scss imports sass files from the _sass directory. It gets processed into the theme’s main stylesheet main. css called by _layouts/default. html via _includes/head. html. To override the default structure and style, create the specific directory at the root of the blog, copy the file to that directory, and then edit the file. For example:  To override the _includes/head. html.  Create an _includes directory in the root of your blog.  Copy _includes/head. html from minima gem folder to this directory.  Edit that file. Updating the default CSS:  Go to the gem path.  Copy the assets/ folder to your blog root.  Edit the /assets/main. scss file. Using SEO: The minima theme comes with the plugin jekyll-seo-tag which is approved by Github Pages. Follow the usage docs and advanced usage. You can use the following in your post/page YAML header:  title descriptionUsing a different theme: I tried a theme that looked like Medium Instead of installing Jekyll as shown above, do the following: $ git clone https://github. com/wowthemesnet/mundana-theme-jekyll. git blog$ cd blog$ bundle$ bundle exec jekyll serveI liked it for a while but it had some weird bugs. I spent many hours trying to fix them but then I gave up and switched back to the default minima theme. Creating an about page: In your root blog create edit the default about and modify it as about. md: ---layout: pagetitle:  About permalink:  /about. html comments: false---Creating blog posts: These go in the _posts folder. I got a template that I follow for each new blog post: ---layout: posttitle:  Title in Double Quotes author: tomcategories: [A category]tags: [some tags here]---file: YYYY-MM-DD-title. md (or it won't show up)title: Don't forget to update thisAdd images with: ![Image Name]({{ site. baseurl }}/assets/images/add_image. jpg)Add local URL with: [Local URL](. . /local-url)Creating redirects: Add this to the Gemfile in the plugins block: gem 'jekyll-redirect-from', github: 'jekyll/jekyll-redirect-from'Then run bundle install. As seen in the docs here Add it to the _config. yml under plugins - jekyll-redirect-fromMy problem was that for my previous website, the blog posts had this format: blog_post_name. htmlThis new website removes the . html and a lot of blog posts crawled by google are being sent to a 404 File not found page. I want to redirect blog_post_name. html to blog_post_name/ In the blog post the header should show something like this: title:  Blog Post Name redirect_from: - /blog_post_name. html404 page: More about 404 pages here You can create a 404. md file if you add this: ---layout: pagetitle:  Not Found permalink:  /404. html comments: falseredirect_from: - /index2. html---Use the redirect_from: to redirect bad URLs from Google search results. Or use the same approach for blog posts. You can also try to fix them in your Google Search Console. Add a Favicon: Based on your theme, it should allow you to add a Favicon on the _config. yml. Otherwise you would need to add it directly to the head. html. For the minima theme, the docs say that you can add an _includes/custom-head. html to your root folder and add your code for the favicon files. However, this didn’t work for me as shown on 2. 5. 1 can’t include custom-head. html. I added the favicon code directly to head. html. Setup Github: Go to Github:  Create a new repo with the format username. github. ioSetup the repo: $ git init$ git remote add origin link-to-repoCNAME, robots. txt: If you have a custom domain, create a CNAME file, add a line with your website, and save it to your local blog root directory: www. yoursite. comCreate a robots. txt and add this line to the file: User-agent: *You can also use Disallow for bad URLs. User-agent: *Disallow: /bad. htmlAllow: /Deploy to Github: If you want to cache your credentials $ git config --global credential. helper 'cache --timeout=3600'There are two way to build your blog, development and production. Build in development and test in localhost: $ bundle exec jekyll serveBuild in development uses _config. yml: $ jekyll buildBuild to production using an environment variable: $ JEKYLL_ENV=production jekyll buildIf you prefer to use a separate deploy configuration you can add a _config-deploy. yml to your blog root. $ cp _config. yml _config-deploy. ymlAdd this line to _config-deploy. yml: environment: productionBuild the blog like this: $ jekyll build --config _config-deploy. ymlThen deploy: $ git add . $ git commit -m  Awesome commit message here $ git push -u origin masterYou can also build to production using a gem called jgd as explained below in Deploying custom plugins. Change your DNS name server: Setup an account with Cloudflare if you don’t have one. Find the DNS name server In your DNS provider point it to Cloudflare. Cloudflare settings: Create these records in Cloudflare: Type  Name        ContentALIAS  yoursite. com    youruser. github. ioCNAME  www. yoursite. com  youruser. github. ioTXT   yoursite. com    youruser. github. ioAdd A records as seen on Setting up an Apex domain Add TXT record to verify Google webmaster tools:  Add property Add TXT google verification codeSetup these Page rules. As seen here https://www. yoursite. com/*Cache Level: Cache Everythinghttps://yoursite. com/*Forwarding URL: (Status Code: 301 - Permanent Redirect, URl: https://www. yoursite. com$1)http://www. yoursite. com/*Always Use HTTPSIn your Overview dashboard set these (if you are on the free plan)  Security level: medium SSL: Full Caching level: StandardMigrating content: Here is where I was challenged. With the Pelican Python static website generator the files were named as title. md. With Jekyll they need to be in the format YYYY-MM-DD-title. md. For Pelican the header looks like this: Title: Powerful things you can do with the Markdown editor Date: 2020-02-09 20:00Category: Jekyll, tutorialTags: featuredSlug: powerful-thingsAuthor: Tom OrdonezStatus: publishedSummary: A blog post about Markdown editor. For Jekyll it needs to look like this: ---layout: posttitle:  Powerful things you can do with the Markdown editor author: tomcategories: [ Jekyll, tutorial ]image: assets/images/11. jpgtags: [featured]---How to make this change to about 100 blog posts? I needed to extract the Date from the header and use it to rename the file. Then extract other content like the Title, Category, and Tags. Then replace this header with the new header. Also the blog posts used this syntax to insert images in the content {static}/images/ while Jekyll uses: { {site. baseurl} }/assets/images/Read more in Python, Files, and OS Module Deploying custom plugins: As shown on the Github Pages docs. Github Pages cannot build sites using unsupported plugins. Here is the list of approved plugins. Go to dependency versions. As shown in this blog post Deploy Jekyll to Github Pages. You can use a gem to setup deployment of Jekyll when you are using custom plugins. Update your Gemfile: gem 'jgd'Run bundle $ bundleOutput: Fetching trollop 2. 9. 9Installing trollop 2. 9. 9Fetching jgd 1. 12Installing jgd 1. 12Post-install message from trollop:!  The 'trollop' gem has been deprecated and has been replaced by 'optimist'. !  See: https://rubygems. org/gems/optimist!  And: https://github. com/ManageIQ/optimistMore about trollop in the official doc and this tutorial: writing a Ruby CLI using Trollop. To deploy use this: $ JEKYLL_ENV=production jgdChange the source of your Github Pages:  Go to your Github repo Settings Options Scroll down to Github Pages Source Change your branch from master to gh-pages.  SaveThen deploy again: $ JEKYLL_ENV=production jgd	Some of the output:		[DEPRECATION] This gem has been renamed to optimist 	and will no longer be supported. Please switch to optimist 	as soon as possible. 	+ set -e	+ set -o pipefail	+ URL=https://github. com/. . . git	+ BRANCH=gh-pages	+ BRANCH_FROM=master	+ DEPLOY_CONFIG=_config-deploy. yml	+ BUNDLE=	+ DRAFTS=	++ pwd	+ SRC=/home/. . . /blog	++ mktemp -d -t jgd-XXX	+ TEMP=/tmp/jgd-NAn	+ trap 'rm -rf /tmp/jgd-NAn' EXIT	+ CLONE=/tmp/jgd-NAn/clone	+ COPY=/tmp/jgd-NAn/copy	+ echo -e 'Cloning Github repository:'		Cloning Github repository:	+ git clone -b master https://github. com/. . . /tmp/jgd-NAn/clone		Cloning into '/tmp/jgd-NAn/clone'. . . 	remote: Enumerating objects: 3025, done.  	remote: Counting objects: 100% (3025/3025), done.                   	remote: Compressing objects: 100% (1920/1920), done.                  	remote: Total 3025 (delta 1272), 	 reused 2692 (delta 954), pack-reused 0        	Receiving objects: 100% (3025/3025), 31. 22 MiB 	 | 13. 20 MiB/s, done. 	Resolving deltas: 100% (1272/1272), done. 	+ cp -R /tmp/jgd-NAn/clone /tmp/jgd-NAn/copy	+ cd /tmp/jgd-NAn/clone	+ echo -e '\nBuilding Jekyll site:'		Building Jekyll site:	+ rm -rf _site	+ '[' -r _config-deploy. yml ']'	+ jekyll build	Configuration file: /tmp/jgd-NAn/clone/_config. yml	      Source: /tmp/jgd-NAn/clone	    Destination: /tmp/jgd-NAn/clone/_site	 Incremental build: disabled. Enable with --incremental	   Generating. . . 	    Jekyll Feed: Generating feed for posts	          done in 1. 67 seconds. 	 Auto-regeneration: disabled. Use --watch to enable. 	+ '[' '!' -e _site ']'	+ cp -R _site /tmp/jgd-NAn	+ cd /tmp/jgd-NAn	+ rm -rf /tmp/jgd-NAn/clone	+ mv /tmp/jgd-NAn/copy /tmp/jgd-NAn/clone	+ cd /tmp/jgd-NAn/clone	+ echo -e '\nPreparing gh-pages branch:'	Preparing gh-pages branch:	++ git branch -a	++ grep origin/gh-pages	+ '[' -z '' ']'	+ git checkout --orphan gh-pages		Switched to a new branch 'gh-pages'	+ echo -e '\nDeploying into gh-pages branch:'	Deploying into gh-pages branch:	+ rm -rf 404. md about. md assets CNAME _config. yml Gemfile 	 Gemfile. lock _includes index. markdown _layouts _posts	+ cp -R /tmp/jgd-NAn/_site/404. html /tmp/jgd-NAn/_site/about. html	+ rm -f README. md	+ git add . 	++ date	+ git commit -am 'new version Thu 08 Oct 2020 	 12:10:05 AM EDT' --allow-empty	[gh-pages (root-commit) a36ecd2] new version 	 Thu 08 Oct 2020 12:10:05 AM EDT	 819 files changed, 61268 insertions(+)	 create mode 100644 . github/FUNDING. yml	 create mode 100644 . gitignore	 create mode 100644 . jekyll-cache/Jekyll/Cache/Jekyll--Cache/b7	 . . .   + git push origin gh-pages	+ sed 's|https://github. com/. . . |[skipped]|g'	remote: 	remote: Create a pull request for 'gh-pages' on GitHub by visiting:    	remote:   https://github. com/. . . /pull/new/gh-pages     	remote: 	To [skipped]	 * [new branch]   gh-pages -&gt; gh-pages	+ echo -e '\nCleaning up:'	Cleaning up:	+ rm -rf /tmp/jgd-NAn/clone	+ rm -rf ''	+ rm -rf /tmp/jgd-NAn	Related Posts - Jekyll Plugin: This is a custom Jekyll plugin. Documentation here. Update your Gemfile: gem 'jekyll-tagging-related_posts'Run bundle $ bundleOutput: Fetching nuggets 1. 6. 0Installing nuggets 1. 6. 0Fetching jekyll-tagging 1. 1. 0Installing jekyll-tagging 1. 1. 0Fetching jekyll-tagging-related_posts 1. 1. 0 Installing jekyll-tagging-related_posts 1. 1. 0Post-install message from nuggets:nuggets-1. 6. 0 [2018-07-12]:* Added &lt;tt&gt;JSON. *_{multi,canonical}&lt;/tt&gt;. Post-install message from jekyll-tagging:jekyll-tagging-1. 1. 0 [2017-03-07]:* Added ability to append extra data to all tag pages. (tfe)* Provides compatibility to the current jekyll (3. 4. 1). * A few fixes. (felipe)* Some documentation improvements. (wsmoak, jonathanpberger)* Prooves who is the worst open source maintainer. (pattex ^__^)Update _config. yml and _config-deploy. yml: plugins: - jekyll/tagging - jekyll-tagging-related_postsCreate a _layouts directory in blog root: $ mkdir _layoutsCopy the post. html layout from the minima theme Gem to this new directory. $ cp /home/tom/. rvm/gems/ruby-2. 7. 1@blog/gems/minima-2. 5. 1/_layouts/post. html _layouts/Add this code to post. html after the blog post content and before disqus code. {% if site. related_posts. size &gt;= 1 %}&lt;div&gt; &lt;h3&gt;Related Posts&lt;/h3&gt; &lt;ul&gt; {% for related_post in site. related_posts limit: 5 %}  &lt;li&gt;&lt;a href= {{ related_post. url }} &gt;{{ related_post. title }}&lt;/a&gt;&lt;/li&gt; {% endfor %} &lt;/ul&gt;&lt;/div&gt;{% endif %}Google Analytics: The default theme minima comes with this line in the _config. yml file: google_analytics: UA-XXXXXXEnter your Google Analytics code there. For reference, this is inserted into _includes/head. html. {%- if jekyll. environment == 'production' and site. google_analytics -%}	{%- include google-analytics. html -%}{%- endif -%}Therefore the blog needs to be deployed either using the production environment variable or a different deploy config file as previously shown. Using the environment variable: $ JEKYLL_ENV=production jekyll buildIf you are using the gem jgd then deploy like this $ JEKYLL_ENV=production jgdPagination: Jekyll comes with a default plugin jekyll-paginate that you can set by adding the line paginate: 5 to _config. yml. This is the Jekyll doc. Go to _config. yml and make sure this line is under plugins: plugins: - jekyll-paginateThen add another line (outside of plugins): paginate: 5If your theme’s index is index. markdown, change it to index. html or you will get this error: Pagination: Pagination is enabled, but I couldn't find an index. htmlpage to use as the pagination template. Skipping pagination. Update your _layouts/home. html. Change this line, from this: for post in site. postsTo this: for post in paginator. postsFor pagination links, add the code as shown on Jekyll’s Render the paginated posts. Before the closing endif of if site. posts. size &gt; 0. You can center the CSS of pagination in assets/main. scss: . pagination {	display: flex;}. previous, a. previous {	flex: 1;}. page_number {	flex: 1;}next, a. next {	flex: 1;}Jekyll without Plugins: An alternative option to using custom plugins restricted by Jekyll is by not using plugins at all. Here is a list of features to use Jekyll without plugins. Reading Time: Similar to Medium you can show the time it will take to read the blog post. Here is the documentation to reading time Create the file reading-time. html in your _includes and add the code shown on the doc. Go to your _layouts/post. html and include the reading-time. html line within the header tag, before the closing paragraph. You can test to see if it shows correctly on your blog post such as Aug 29, 2020 • tom • 3 min read. Deploy the source to master: $ jekyll build$ git add . $ git commit -m  Added reading time $ git push -u origin masterIf you installed the jgd gem, then deploy static pages to the gh-pages branch: $ JEKYLL_ENV=production jgdSearch Box: Here is the documentation to Search with Lunr. js Save the file search-lunr. html in _includes. In this file, you can exclude the types of documents to search. For example: if page. url contains '. xml' or page. url contains 'assets' or page. url contains '. json' or page. url contains 'about. html'Download the file lunr. js into your js folder, then make sure that search-lunr. html indicates the correct location of the file. For example: src= /assets/js/lunr. js I copied the default. html layout file from my Gem location to the _layouts directory: cp /home/tom/. rvm/gems/ruby-2. 7. 1@blog/gems/minima-2. 5. 1/_layouts/default. html _layouts/Inside the default. html layout page, include the search-lunr. html as indicated in the docs inside curly percentage brackets. Add this in the main class, before the content tag. include search-lunr. htmlCustomize the CSS for the search box. At the bottom of search-lunr. html there is code with the form. You can wrap this in a class: &lt;div class= search &gt;  &lt;form onSubmit= return lunr_search(document. getElementById('lunrsearch'). value); &gt;    &lt;p&gt;&lt;input type= text  class= form-control  id= lunrsearch  name= q  maxlength= 255  value=   placeholder= Search  /&gt;&lt;/p&gt;  &lt;/form&gt;&lt;/div&gt;Then in assets/main. scss you can try something like this: . search input {  height: 30px;  width: 60%;  padding-left: 10px;  border: 1px solid #D9D9D9;  border-radius: 10px;  font-size: 16px;}Text Expand/Collapse or Collapbsible Markdown: I often add whole output to every command I use. This can take a large space in a blog post and might disrupt reading focus. There are three options for hiding/display text that can be expanded, also known by these keywords: text expand, expand/collapse, collapsible markdown, details element. Use whatever works best. Text Expand: This is a JS that might need some tweaking. When you click on read more it expands the section but it scrolls back to the top. This is the doc for Jekyll Text Expand. Download the file text-expand. html into the _includes directory. Then edit the _layouts/default. html and add this before the closing body tag: {% include text-expand. html %}Then you can use the expand tag in a blog post by adding only one line for each of the open/closing tag such as: [expand]Long content hereand here. . . [/expand]Collapbsible Markdown with Details element: This uses the details disclosure element: details. More details in the Mozilla details element doc. You have to wrap your content like this: &lt;details&gt;	&lt;summary&gt;Click to expand&lt;/summary&gt;	Long content here	and here&lt;/details&gt;To add a code block, you need to have a previous empty line, enclose the code block with three tildes ~~~, optionally you can add the language at the end of the first enclosing tildes: &lt;details&gt;	&lt;summary&gt;Click to expand&lt;/summary&gt;		~~~ python	Code here	~~~&lt;/details&gt;However, you can encounter this issue Details is not formatted correctly in Jekyll/Github pages. You can enclose the content with the &lt;pre&gt; tag. I am using this for my long content output: &lt;details&gt;	&lt;summary&gt;Click to expand&lt;/summary&gt;	&lt;pre&gt;			Long content here	&lt;/pre&gt;&lt;/details&gt;Here is an example: 	Click to expand		Long content here	I also customized the CSS: details {	padding-bottom: 20px;	color: grey;}The text in details not processed correctly: I haven’t tested this. This blog post shows adding support for HTML5 details element to Jekyll. It uses a custom plugin. Add the ruby code into _plugins/details_tag. rb. Then use like this: {% details Click to expand %}	~~~ python	Code here	~~~{% enddetails %}More troubleshooting in using details in Github and collapsible markdown. Open external site in new window: By default linking to external sites open in the same window using this syntax: [External Title](link to external site)Jekyll uses kramdown and you can link like this: [External Title](link to external site){:target= _blank }What I find annoying about this, is that it adds a weird highlighted row in SublimeText. I also think it’s a weird syntax to remember. An alternative option is this JS called new window fix Download the code into _includes/new-window-fix. html and remove the PDF section if you don’t need it. Add this to your _layouts/default. html before the closing body tag. {% include new-window-fix. html %}Add external site to menu: There isn’t a clear way to add an external link to the menu as discussed on navigation external links. This comment shows a quick fix to add the external link to _includes/header. html. I added a link to my Linkedin profile as shown: &lt;div class= trigger &gt; {%- for path in page_paths -%}  {%- assign my_page = site. pages | where:  path , path | first -%}  {%- if my_page. title -%}  &lt;a class= page-link  href= {{ my_page. url | relative_url }} &gt;{{ my_page. title | escape }}&lt;/a&gt;  &lt;a class= page-link  href= https://www. linkedin. com/in/tomordonez/ &gt;Linkedin&lt;/a&gt;  {%- endif -%} {%- endfor -%}&lt;/div&gt;The header_pages has to be enabled in _config. yml for the menu to show. I have an about page here: header_pages: - about. md"
    }, {
    "id": 154,
    "url": "https://www.tomordonez.com/python-files-os-module/",
    "title": "Python, Files, and OS Module",
    "body": "2020/05/24 - Migrating from one static website to another, I had to rename multiple files, extract some content, remove it, and do other operations. For about 100 blog posts I had to rename the files from title. md to the format YYYY-MM-DD-title. md. The header of each post looked like this: Title: Powerful things you can do with the Markdown editor Date: 2020-02-09 20:00Category: Jekyll, tutorialTags: featuredSlug: powerful-thingsAuthor: Tom OrdonezStatus: publishedSummary: A blog post about Markdown editor. And I had to change it to this ---layout: posttitle:  Powerful things you can do with the Markdown editor author: tomcategories: [ Jekyll, tutorial ]image: assets/images/11. jpgtags: [featured]---Some blog posts also contained images which used this syntax {static}/images/, and I had to change it to this: { {site. baseurl} }/assets/images/This is what the Python script looks like, it’s not the best but it worked. This can give you an idea on how to work with files and Python OS module. import osimport shutilimport randomlines = ['---', 'layout: post', 'title: ', 'author: tom', 'categories: ', 'image: ', 'tags: ', '---']matches = ['Title: ', 'Date: ', 'Category: ', 'Tags: ', 'Slug: ', 'Author: ', 'Status: ', 'Summary: ']dest_dir = 'path to . . /newblog/_posts/'# Extract data from old formatfor blogpost in os. listdir():	# Only for markdown files	if  . md  in blogpost:		print(blogpost)		# Extract the metadata data from blogpost, then remove the header		with open(blogpost, 'r+') as fh:			content = fh. readlines()			if content[0]. startswith('Title'):				title = content[0]. rstrip('\r\n'). split(': ')[1]			else:				title = ''			date_prefix = content[1]. split()[1]			if content[2]. startswith('Category'):				categories = content[2]. rstrip('\r\n'). split(': ')[1]			else:				categories = ''			# Category in old format: 'Data Analytics', 'Analytics', 'Python', 'Coding', 			# 'Linux', 'Android', 'Mobile Dev', 'Tableau', 'Data Analysis'			# 'Sourcing', 'Cloud', 'Recruiting', 'Data Science'			if any(match in categories for match in ['Data Analytics', 'Analytics', 'Data Analysis', 'Data Science', 'Tableau']):				image = 'assets/images/'+str(random. randint(1, 12))+'. jpg'				categories = 'Data Analytics'			elif any(match in categories for match in ['Python', 'Coding', 'Cloud']):				image = 'assets/images/'+str(random. randint(1, 12))+'. jpg'				categories = 'Code'			elif any(match in categories for match in ['Linux']):				image = 'assets/images/'+str(random. randint(1, 12))+'. jpg'			elif any(match in categories for match in ['Android', 'Mobile Dev']):				image = 'assets/images/'+str(random. randint(1, 12))+'. jpg'				categories = 'Mobile'			elif any(match in categories for match in ['Sourcing', 'Recruiting']):				image = 'assets/images/'+str(random. randint(1, 12))+'. jpg'				categories = 'Recruiting'			if content[3]. startswith('Tags'):				tags = content[3]. rstrip('\r\n'). split(': ')[1]			else:				tags = ''			# Create the jekyll header			header = lines[0] + '\n' + lines[1] + '\n' + lines[2] + f' {title} ' + '\n' + lines[3] + '\n' + lines[4] 				+ '[' + categories + ']' + '\n' + lines[5] + image + '\n' + lines[6] + '[' + tags + ']' + '\n' + lines[7]			# Remove the old header			for index in range(len(content)):				if any(content[index]. startswith(match) for match in matches):					last_index = index			del content[:last_index+1]			# Capture the blogpost content after the header			flat_content = ''. join(content)			# Copy to a new file with format year-month-day-title. md			new_name = date_prefix+'-'+blogpost			dest = dest_dir + new_name			print( Copying {} . format(new_name))			shutil. copy2(blogpost, dest)			# Concat and write the new header and the cleaned content			with open(dest, 'r+') as fh:				fh. seek(0, 0)				fh. write(header + '\n' + flat_content)			# Correct image syntax in content			# From this syntax: ![Scraping]({static}/images/scraping. jpg)			# To this:![Scraping]({{ site. baseurl }}/assets/images/scraping. jpg)			with open(dest, 'r') as fh:				filedata = fh. read()						filedata = filedata. replace( {static}/images/ ,  { {site. baseurl} }/assets/images/ )						with open(dest, 'w') as fh:				fh. write(filedata)"
    }, {
    "id": 155,
    "url": "https://www.tomordonez.com/curly-braces-markdown-jekyll/",
    "title": "Curly Braces in Markdown with Jekyll",
    "body": "2020/05/23 - If you use double curly braces in code blocks in markdown with Jekyll. They just won’t show up. Enclose the code block in raw and endraw tags. Like this: {% raw % } some code here {% endraw % } Writing the previous line and make it appear correctly in this blog post sent me to a 4-hour bottomless pit of googling. I think it’s not possible to show the raw/endraw code in a code block using the raw/endraw tags. Basically I am trying to show: open raw tagsome code hereclose raw tagThis is mostly used to show code that has double curly braces: { {site. baseurl} }/assets/images/I have also seen some strange behavior when adding a JavaScript code block and using Safari on mobile in Reader view mode. The HTML content is not displayed correctly. I thought that maybe my Jekyll template was broken. Here are other resources I read:  Escaping double curly braces HTML character encoding Show raw tags in post generated by jekyll Liquid ‘raw’ Tag and Fenced Code Blocks Include Jekyll/Liquid code without rendering it raw/endraw throwing warning on Jekyll 3. 3. 1 raw endraw tags not being respected Raw Doc on Liquid Jekyll is escaping an anchor tag on my page How to escape curly brace percentage in markdown? Issue with Jekyll liquid templates and raw tags Jekyll Raw Code Block Syntax"
    }, {
    "id": 156,
    "url": "https://www.tomordonez.com/d3-and-incompatible-versions/",
    "title": "D3 and Incompatible Versions",
    "body": "2020/05/22 - Different versions of D3 don’t play along well. If you are following a book or tutorials, make sure to check which version of D3 is used or some things won’t work as expected. Here is an example of loading a CSV file. D3 and Incompatible Versions/Syntax: Loading a CSV file in D3 changes with different versions of D3 and this incompatibility is very confusing. As shown in this blog post about promises syntax. There are differences in D3 loading a CSV in D3. v4 and D3. v5. For D3. v4 you can use this syntax: d3. csv( file. csv , function(data) {  somethingHappens(data);  console. log(data);});Create these files as shown in Setup D3 Step by Step:  index. html main. css project. jsCreate a CSV file called file. csv that has year and population as shown: year,population1950,51951,101955,151959,20The index. html is using this source https://d3js. org/d3. v5. min. js. Add this code to project. js: d3. csv( file. csv , function(data) {  console. log(data);});Reload the browser and look at the console: Navigated to http://127. 0. 0. 1:8888/index. htmlproject. js:2 {year:  1950 , population:  5 }project. js:2 {year:  1951 , population:  10 }project. js:2 {year:  1955 , population:  15 }project. js:2 {year:  1959 , population:  20 }This works so far and we are using the source for D3. v5 instead of v4. Clear the console with Ctrl + L or type clear() (this won’t clear if you set Preserve log). This syntax from D3 v4 won’t work on v5 as shown on D3 API docs. Actually this documentation is pretty confusing as it doesn’t specify which D3 version is used in the examples: d3. csv( file. csv , function(data) {}). then(function(data) { console. log(data);});The data is not loaded and the console shows this: Navigated to http://127. 0. 0. 1:8888/index. html[columns: Array(2)] columns: Array(2)   0:  year    1:  population    length: 2   __proto__: Array(0)   . . .   . . . This doesn’t work either: var mydata = d3. csv( file. csv , function(data) {});First it returns undefined. Then calling the variable: mydataShows this: Promise {&lt;resolved&gt;: Array(0)}__proto__: Promise[[PromiseStatus]]:  resolved [[PromiseValue]]: Array(0)columns: (2) [ year ,  population ]length: 0__proto__: Array(0)Make sure to check which version of D3 is used since errors will drive you crazy. More about promises in D3 Load a CSV file with Promises "
    }, {
    "id": 157,
    "url": "https://www.tomordonez.com/d3-and-asynchronous/",
    "title": "D3 and Asynchronous",
    "body": "2020/05/17 - D3 and JavaScript run asynchronous. You have to grok anonymous callback functions. D3 and Asynchronous: JavaScript runs asynchronous. Here is a good explanation on StackOverflow. The answer provides a great analogy for synchronous and asynchronous. Synchronous is like making a phone call to tech support to fix your Internet. While they figure out the problem, you wait on the call, until the problem is solved then you end the call. Asynchronous is making the same phone call. Instead of waiting on the call you ask them to call you back when they fix the problem. You end the call, go about your day, then wait for them to call back. Asynchronous on an app means if you are loading a page, it will load the html, the css, the js. If there is a feature that is not working, only that feature won’t load. If you change the data on a feature, then you won’t have to reload the page, the feature changes as you interact with the features. D3 and Anonymous functions: If you know Python, this is like a lambda function. This is what loading a CSV in D3 v4 looks like. You can also load a file with promises as shown in D3 Load a CSV file with Promises. : d3. csv( file. csv , function(data) {  somethingHappens(data);});console. log(data);This is an anonymous callback function function(data). It runs the code inside the curly braces and then it is called back to this function. More about D3 and Incompatible Versions The script will continue and execute console. log(data), regardless if the data was not completely loaded. Then the console won’t display any data. You could hack this code and put some sort of timer but how much time do you pass to that function? d3. csv( file. csv , function(data) {  somethingHappens(data);});someSortOfTimer(60);console. log(data);Instead, within the callback function, write all the code that uses the data that is loaded: d3. csv( file. csv , function(data) {  somethingHappens(data);  console. log(data);});Here is how to Setup D3 Step by Step "
    }, {
    "id": 158,
    "url": "https://www.tomordonez.com/setup-d3-step-by-step/",
    "title": "Setup D3 Step by Step",
    "body": "2020/05/12 - Setup D3 step by step. Creating HTML, CSS, JS files. Running a web server. Setup up D3 project files: First create three files:  index. html main. css project. jsDownload the D3. js library from D3 Website or use the external reference. If you downloaded the D3 file into a lib folder, then add this to your index. html: &lt;script type= text/javascript  src= lib/d3. min. js &gt;&lt;/script&gt;If you want to use an external D3 reference use one of these, depending on the D3 version: &lt;script type= text/javascript  src= https://d3js. org/d3. v4. min. js &gt;&lt;/script&gt;&lt;script type= text/javascript  src= https://d3js. org/d3. v5. min. js &gt;&lt;/script&gt;Create the index HTML file: Here is an index. html example including the references to main. css, project. js, and d3. v5. min. js. &lt;!DOCTYPE html&gt;&lt;html lang= en &gt;  &lt;head&gt;    &lt;meta charset= utf-8 &gt;    &lt;title&gt;D3 Canvas&lt;/title&gt;    &lt;link rel= icon  type= image/png  href= icon. png &gt;    &lt;script type= text/javascript  src= https://d3js. org/d3. v5. min. js &gt;&lt;/script&gt;    &lt;link rel= stylesheet  type= text/css  href= main. css &gt;  &lt;/head&gt;  &lt;body&gt;    &lt;script type= text/javascript  src= project. js &gt;&lt;/script&gt;  &lt;/body&gt;&lt;/html&gt;Example of a D3. js file: This is an example of a D3 Javascript file. Save it into project. js. var w = 700;var h = 500;var barPadding = 3;var padding = 40;var parseTime = d3. timeParse( %Y );var formatTime = d3. timeFormat( %Y );var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var rowConverter = function(d) {  return {    year: parseTime(d. year),    total: +d. running_total  };}Run a web server with Python: In your D3 project folder, run the web server with Python. Run it like this if you want the process to run in the background with &amp;. : $ python -m http. server 8888 --bind 127. 0. 0. 1 &amp;. Or like this if you want to see the output. With --bind to explicitly use localhost only: $ python -m http. server 8888 --bind 127. 0. 0. 1Then open the browser on http://127. 0. 0. 1:8888/ and browse to the index. html to see your D3 project: Serving HTTP on 127. 0. 0. 1 port 8888 (http://127. 0. 0. 1:8888/)127. 0. 0. 1 - - [06/Jun/2020 08:24:06]  GET /index. html HTTP/1. 1  200 -127. 0. 0. 1 - - [06/Jun/2020 08:24:06]  GET /main. css HTTP/1. 1  200 -127. 0. 0. 1 - - [06/Jun/2020 08:24:06]  GET /project. js HTTP/1. 1  200 -Or like this if you want to access localhost from your phone: $ python -m http. server 8888 --bind 0. 0. 0. 0Find the IP number of the laptop. Let’s say it is 192. 168. 1. 35. Then from your mobile go to: 192. 168. 1. 35:8888On the browser verify that you see the &lt;title&gt; tag from the index. html: &lt;title&gt;D3 Canvas&lt;/title&gt;Stop the server with Ctrl C. CDD or Console Driven Development: It’s a good idea to follow a Console Driven Development approach as you make changes to your D3 files html, js, and css. When you interact with the visualization, you can see the result in the console and see if it works as expected. Go to the browser, right click Inspect and find the Console. For a detailed D3 tutorial go to D3 Tutorial Data Visualization "
    }, {
    "id": 159,
    "url": "https://www.tomordonez.com/d3-troubleshooting/",
    "title": "D3 Troubleshooting",
    "body": "2020/02/09 - D3 is a great JavaScript library for data visualization. However, like every technology, it can be annoying when you don’t know why things don’t work as expected. A good way for troubleshooting D3 is by using the browser console. Let’s run a small project as shown on the D3 tutorial above. It can be as simple as creating an index. html and linking to the D3. js library. &lt;!DOCTYPE html&gt;&lt;html lang= en &gt; &lt;head&gt;  &lt;meta charset= utf-8 &gt;  &lt;script type= text/javascript  src= https://d3js. org/d3. v5. min. js &gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt;&lt;/html&gt;Then run the webserver like this: $ python -m http. server 8888 --bind 127. 0. 0. 1Open the browser on: http://127. 0. 0. 1:8888/. For this test I used Firefox 72. Open the console on the browser. (You can use clear() to clear the output) D3 Methods: On the console, type d3. Then expand the result, which shows all these methods. Now you can investigate which methods are available: {…}​__esModule: true​active: function active()​arc: function arc()​area: function ry()​areaRadial: function ly()​ascending: function n()​axisBottom: function axisBottom()​axisLeft: function axisLeft()​axisRight: function axisRight()​. . . . . . it has about 500 methods. . . For example, two useful methods are d3. min and d3. max. Let’s create an array: numbers = [3, 9, 1, 0, 2, 8, 2];It shows this output: Array(7) [ 3, 9, 1, 0, 2, 8, 2 ]Then the min and max can be easily calculated: d3. min(numbers)0d3. max(numbers)9This can be helpful when using scales to define the input domain. Array methods: Type the name of the array we created numbersExpand the result to see the values of the array and the array methods (7) […]​0: 3​1: 9​2: 1​3: 0​4: 2​5: 8​6: 2​length: 7​&lt;prototype&gt;: []​​concat: function concat()​​constructor: function Array()​​copyWithin: function copyWithin()​​entries: function entries()​​every: function every()​​fill: function fill()​​filter: function filter()​​find: function find()​​findIndex: function findIndex()​​flat: function flat()​​flatMap: function flatMap()​​forEach: function forEach()​​includes: function includes()​​indexOf: function indexOf()​​join: function join()​​keys: function keys()​​lastIndexOf: function lastIndexOf()​​length: 0​​map: function map()​​pop: function pop()​​push: function push()​​reduce: function reduce()​​reduceRight: function reduceRight()​​reverse: function reverse()​​shift: function shift()​​slice: function slice()​​some: function some()​​sort: function sort()​​splice: function splice()​​toLocaleString: function toLocaleString()​​toSource: function toSource()​​toString: function toString()​​unshift: function unshift()​​values: function values()​​Symbol(Symbol. iterator): function values()​​Symbol(Symbol. unscopables): Object { copyWithin: true, entries: true, fill: true, … }​​&lt;prototype&gt;: Object { … } ​There is a sort function. numbers. sort()Array(7) [ 0, 1, 2, 2, 3, 8, 9 ]There is also length. numbers. length7Object Methods: Let’s create an object. simpsons = { name:  Homer , age: 40, city:  Springfield };Type simpsons Object { name:  Homer , age: 40, city:  Springfield  }Create an array from this object: simpsonsArray = d3. values(simpsons)Array(3) [  Homer , 40,  Springfield  ]To see methods available for Object simpsons, type simpsons and expand the output. {…}​age: 40​city:  Springfield ​name:  Homer ​&lt;prototype&gt;: {…}​​__defineGetter__: function __defineGetter__()​​__defineSetter__: function __defineSetter__()​​__lookupGetter__: function __lookupGetter__()​​__lookupSetter__: function __lookupSetter__()​​__proto__: ​​constructor: function Object()​​hasOwnProperty: function hasOwnProperty()​​isPrototypeOf: function isPrototypeOf()​​propertyIsEnumerable: function propertyIsEnumerable()​​toLocaleString: function toLocaleString()​​toSource: function toSource()​​toString: function toString()​​valueOf: function valueOf()​​&lt;get __proto__()&gt;: function __proto__()​​&lt;set __proto__()&gt;: function __proto__()Console. log on callback function: On your D3 script, use console. log on a callback function to see the data before and after such as: var someFunction (d) {	console. log( Before something happens );	console. log(d);	somethingHappens(d);	console. log( After something happens );	console. log(d);	}d3. nest reformats a Date object to string: As seen here and here. If you are using d3. nest() to group data that has a Date. Nest will force the value into a string. Given a dataset where you parse the year: const parseTime = d3. timeParse( %Y );After loading dataset, then creating an array of objects: const data = dataset. map(d =&gt; {	return {	 year: parseTime(d. year),	 count: +d. sale	};})The parsed d. year stored into year should be like this: Date Sat Jan 01 2011 00:00:00 GMT-0500 (Eastern Standard Time)If you want to summarize the data and calculate amount per year: const salesYear = d3. nest(). key(d =&gt; d. year). rollup(amount =&gt; d3. sum(amount, d =&gt; d. sale)). entries(data);However, . key(d =&gt; d. year) forces the Date object to String like this:  Sat Jan 01 2011 00:00:00 GMT-0500 (Eastern Standard Time) If you were to plot Sales Amount vs Year by using a scale that maps Year to Date objects. You will get a NaN. Because now Year is not a Date object, but a String. const line = d3. line()  . x(d =&gt; xScale(d. year))  . y(d =&gt; yScale(d. amount));You need to convert the Date string back to Date object. const line = d3. line()	. x(d =&gt; xScale(new Date(d. year)))	. y(d =&gt; yScale(d. amount));D3 examples: There are two websites that have a lot of examples. However, keep in mind some use different versions of D3, and the syntax might change in different versions:  Blocks Observable Docs"
    }, {
    "id": 160,
    "url": "https://www.tomordonez.com/d3-tutorial-data-visualization/",
    "title": "D3 Tutorial for Data Visualization",
    "body": "2020/02/02 - This is a detailed D3 tutorial for data visualization. Setup D3, build a barchart, and responsive D3. Updated June 2020. Resources::  D3 Troubleshooting Interactive Data Visualization for the Web, 2nd Ed D3 Official Website D3 Examples at Popular Blocks Blog, from the creator of D3 D3 API Docs D3 Making a Map D3 Data Wanderings Blog D3 Making a Slider SVG Reference Managing colors in D3 Color Brewer for Maps Python HTTP Server D3. js on Jekyll D3. js charts responsive Create a responsive bar chart with D3. js JavaScript Date ObjectD3 Version and Compatibility: Different versions of D3 don’t play along well. If you are following a book or tutorials, make sure to check which version of D3 is used or some things won’t work as expected. Setting up D3: First create three files, name them as you think is best, for example:  index. html main. css project. jsDownload the D3. js library from D3 Website or use the external reference. If you downloaded the file into a lib folder, then add this to your index. html: &lt;script type= text/javascript  src= lib/d3. min. js &gt;&lt;/script&gt;If you want to use an external reference use one of these, depending on the version: &lt;script type= text/javascript  src= https://d3js. org/d3. v4. min. js &gt;&lt;/script&gt;&lt;script type= text/javascript  src= https://d3js. org/d3. v5. min. js &gt;&lt;/script&gt;Create an HTML template: Here is an HTML template example including the references to main. css and project. js. I also recommend putting a favicon so you can quickly find the tab. I downloaded a favicon and saved it as icon. png &lt;!DOCTYPE html&gt;&lt;html lang= en &gt;  &lt;head&gt;    &lt;meta charset= utf-8 &gt;    &lt;title&gt;D3 Canvas&lt;/title&gt;    &lt;link rel= icon  type= image/png  href= icon. png &gt;    &lt;script type= text/javascript  src= https://d3js. org/d3. v5. min. js &gt;&lt;/script&gt;    &lt;link rel= stylesheet  type= text/css  href= main. css &gt;  &lt;/head&gt;  &lt;body&gt;    &lt;script type= text/javascript  src= project. js &gt;&lt;/script&gt;  &lt;/body&gt;&lt;/html&gt;Example of a D3. js file: This is an example of some of a D3 Javascript file just to see what it looks like: var w = 700;var h = 500;var barPadding = 3;var padding = 40;var parseTime = d3. timeParse( %Y );var formatTime = d3. timeFormat( %Y );var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var rowConverter = function(d) {  return {    year: parseTime(d. year),    total: +d. running_total  };}Run a web server with Python: In your project folder, run the web server with Python. Run it like this if you want the process to run in the background with &amp;. : $ python -m http. server 8888 --bind 127. 0. 0. 1 &amp;. Or like this if you want to see the output. With --bind to explicitly use localhost only: $ python -m http. server 8888 --bind 127. 0. 0. 1Then open the browser on http://127. 0. 0. 1:8888/ and browse to the index. html: Serving HTTP on 127. 0. 0. 1 port 8888 (http://127. 0. 0. 1:8888/)127. 0. 0. 1 - - [06/Jun/2020 08:24:06]  GET /index. html HTTP/1. 1  200 -127. 0. 0. 1 - - [06/Jun/2020 08:24:06]  GET /main. css HTTP/1. 1  200 -127. 0. 0. 1 - - [06/Jun/2020 08:24:06]  GET /project. js HTTP/1. 1  200 -On the browser verify that you see the &lt;title&gt; tag from the template: &lt;title&gt;D3 Canvas&lt;/title&gt;Stop the server with Ctrl C. CDD or Console Driven DevelopmentIt’s a good idea to follow a Console Driven Development approach as you make changes to your files html, js, and css. When you interact with the visualization, you can see the result in the console and see if it works as expected. Go to the browser, right click Inspect and find the Console. The dataset: For this tutorial, I am using a CSV file called file. csv that has year and population as shown: year,population1950,51951,101955,151959,20About Asynchronous: JavaScript runs asynchronous. Here is a good explanation about it: How do I return the response from an asynchronous call? The answer provides a great analogy for synchronous and asynchronous. Synchronous is like making a phone call to tech support to fix your Internet. While they figure out the problem, you wait on the call, until the problem is solved then you end the call. Asynchronous is making the same phone call. Instead of waiting on the call you ask them to call you back when they fix the problem. You end the call, go about your day, then wait for them to call back. I assume that asynchronous on an app means if you are loading a page, it will load the html, the css, the js. If there is a feature that is not working, only that feature won’t load. If you change the data on a feature, then you won’t have to reload the page, the feature changes as you interact with the features. About Anonymous functions: If you know Python, this is like a lambda function. This is what loading a CSV in D3 v4 looks like: d3. csv( file. csv , function(data) {  somethingHappens(data);});console. log(data);This is an anonymous callback function function(data). It runs the code inside the curly braces and then it is called back to this function. There is a problem here. The script will continue and execute console. log(data), regardless if the data was not completely loaded. Then the console won’t display any data. You could hack this code and perhaps put some sort of a timer but then how much time are you supposed to pass to that function? d3. csv( file. csv , function(data) {  somethingHappens(data);});someSortOfTimer(60);console. log(data);Instead, within the callback function, write all the code that uses the data that is loaded: d3. csv( file. csv , function(data) {  somethingHappens(data);  console. log(data);});Loading a CSV file - Incompatible D3 versions/syntax: Loading a CSV file changes in different versions of D3 and this incompatibility is very confusing. As shown in this blog post. There are differences loading a CSV in D3. v4 and D3. v5. For D3. v4 you can use this syntax: d3. csv( file. csv , function(data) {  somethingHappens(data);  console. log(data);});Let’s try this using the Console Driven Development approach. Make sure that you created these files:  index. html: Using the template as shown before.  main. css project. js file. csv: Add the data as shown beforeKeep in mind that the index. html is using this source https://d3js. org/d3. v5. min. js. Add this code to project. js: d3. csv( file. csv , function(data) {  console. log(data);});Reload the browser and look at the console: Navigated to http://127. 0. 0. 1:8888/index. htmlproject. js:2 {year:  1950 , population:  5 }project. js:2 {year:  1951 , population:  10 }project. js:2 {year:  1955 , population:  15 }project. js:2 {year:  1959 , population:  20 }This works so far and we are using the source for D3. v5 instead of v4. Clear the console with Ctrl + L or type clear() (this won’t clear if you set Preserve log). This syntax from v4 won’t work on v5 as shown on D3 API docs. Actually this documentation is pretty confusing as it doesn’t specify which D3 version is used on the examples: d3. csv( file. csv , function(data) {}). then(function(data) { console. log(data);});The data is not loaded and the console shows this: Navigated to http://127. 0. 0. 1:8888/index. html[columns: Array(2)] columns: Array(2)   0:  year    1:  population    length: 2   __proto__: Array(0)   . . .   . . . This doesn’t work either: var mydata = d3. csv( file. csv , function(data) {});First it returns undefined. Then calling the variable: mydataShows this: Promise {&lt;resolved&gt;: Array(0)}__proto__: Promise[[PromiseStatus]]:  resolved [[PromiseValue]]: Array(0)columns: (2) [ year ,  population ]length: 0__proto__: Array(0)Loading a CSV file in D3. v5 with Promises: Following the previous example. D3 version 5 uses a feature called Promises. Use this syntax: d3. csv( file. csv ). then(function(dataset) {  console. log(dataset);});The console shows this: Promise {&lt;pending&gt;}(4) [{…}, {…}, {…}, {…}, columns: Array(2)]  0: {year:  1950 , population:  5 }  1: {year:  1951 , population:  10 }  2: {year:  1955 , population:  15 }  3: {year:  1959 , population:  20 }  columns: (2) [ year ,  population ]  length: 4  __proto__: Array(0)Or use this syntax, it shows the same console output: var mydata = d3. csv( file. csv )mydata. then(function(dataset) {  console. log(dataset);});Or you can load multiple datasets like this: var data1 = d3. csv( file1. csv )var data2 = d3. csv( file2. csv )Promise. all([data1, data2]). then(someFunction)function someFunction(values) {  somethingFunHere(values);}More about Promises in the Observable blog. Loading data with CSV, DSV, TSV or JSON: There are a few methods to load data:  d3. dsv this means delimiter separated values d3. csv d3. tsv d3. jsonYou can use d3. dsv like this and explicitly define the separator: d3. dsv( , ,  file. csv ). then(function(dataset) {  console. log(dataset);});Console shows: Promise {&lt;pending&gt;}(4) [{…}, {…}, {…}, {…}, columns: Array(2)]  0: {year:  1950 , population:  5 }  1: {year:  1951 , population:  10 }  2: {year:  1955 , population:  15 }  3: {year:  1959 , population:  20 }  columns: (2) [ year ,  population ]  length: 4  __proto__: Array(0)Or use d3. csv as I have shown before: d3. csv( file. csv ). then(function(dataset) {  console. log(dataset);});Or use d3. json like this. Let’s create a data. json and add this: [ {   year : 1950,   population : 5 }, {   year : 1951,   population : 10 }, {   year : 1955,   population : 15 }, {   year : 1959,   population : 20 }]Then load the data: d3. json( data. json ). then(function(dataset) {  console. log(dataset);});Console shows the same output: Promise {&lt;pending&gt;}(4) [{…}, {…}, {…}, {…}]  0: {year: 1950, population: 5}  1: {year: 1951, population: 10}  2: {year: 1955, population: 15}  3: {year: 1959, population: 20}  length: 4  __proto__: Array(0)Converting String to other Types: When loading a CSV file, the data is parsed as strings. If this is the data contained in the CSV: year,population1950,51951,101955,151959,20It will parse the years as strings and the population as strings. You need to convert these strings to the correct type.  year: From string to date format.  population: From string to integer format. Parsing and formatting the year: You can use this syntax and see the resources for more details var parseTime = d3. timeParse( %Y );var formatTime = d3. timeFormat(specifier);More details here:  %Y: Parse year in decimal number such as 2020.  specifier: More details here d3. timeParse d3. timeFormatFor now let’s add this code to our project. js: var parseTime = d3. timeParse( %Y );Now we need to pass a function as a parameter when loading the CSV. First we need to create the function. A function to parse every row of the CSV file: This function is passed as a parameter of the d3. csv method. It takes every row as d. It parses the year from parseTime(d. year) and for population, it uses the + operator to force the string to numbers on +d. population. Let’s add this to our code so now it shows this: var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  };}Clear the console and reload the browser. See if there are any errors in the console or in the Python server. Loading a CSV file with rowConverter: Load the csv file and use rowConverter. Add this to project. js and reload the browser: var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  };}d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset);});This is what the code does:  It opens file. csv and passes the function rowConverter as a parameter.  It takes every row using the variable d of file. csv.  Parses the year from a string to a parseTime(d. year) format.  Parses the population from a string to an integer using the + operator +d. population.  Then print the values in the console. The console shows: (4) [{…}, {…}, {…}, {…}, columns: Array(2)]  0: {year: Sun Jan 01 1950 00:00:00 GMT-0500 (Eastern Standard Time), population: 5}  1: {year: Mon Jan 01 1951 00:00:00 GMT-0500 (Eastern Standard Time), population: 10}  2: {year: Sat Jan 01 1955 00:00:00 GMT-0500 (Eastern Standard Time), population: 15}  3: {year: Thu Jan 01 1959 00:00:00 GMT-0500 (Eastern Standard Time), population: 20}  columns: (2) [ year ,  population ]  length: 4  __proto__: Array(0)Bind the dataset values to elements in the DOM: Now we need to use the data and bind it to elements in the DOM. This is the cycle:  Select HTML elements with . select() Add the data with . data() Bind the data to elements with . enter() Append elements to the DOM with . append()Let’s add some code inside the d3. csv block: var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  };}d3. csv( file. csv , rowConverter). then(function(dataset) {  d3. select( body ). selectAll( p )  . data(dataset)  . enter()  . append( p );});This is how it works:  It selects the element body.  Then it selects all p elements. However, no p elements exist yet.  Use the attribute . data to read dataset.  Use . enter() to bind the dataset values with p elements Use . append( p ) to append the p elements to the DOM. Stop and reload the server. Also, reload the page. It seems as if nothing happened. Since we removed console. log(dataset), there is also no output on the console. Go to the Elements tab to see the HTML code. The body section has this now: &lt;body&gt;  &lt;script type= text/javascript  src= project. js &gt;&lt;/script&gt;  &lt;p&gt;&lt;/p&gt;  &lt;p&gt;&lt;/p&gt;  &lt;p&gt;&lt;/p&gt;  &lt;p&gt;&lt;/p&gt;&lt;/body&gt;It created four p tags. Let’s go back to the console and type d3. selectAll( p );: Pt {_groups: Array(1), _parents: Array(1)}_groups: [NodeList(4)]_parents: [html]__proto__: ObjectExpand groups _groups: Array(1)0: NodeList(4) [p, p, p, p]length: 1Expand NodeList 0: NodeList(4)  0: p  1: p  2: p  3: pExpand the first 0: p. It’s pretty long, scroll down to the bottom: __data__: {year: Sun Jan 01 1950 00:00:00 GMT-0500 (Eastern Standard Time), population: 5}__proto__: HTMLParagraphElementExpand data: __data__:population: 5year: Sun Jan 01 1950 00:00:00 GMT-0500 (Eastern Standard Time) {}Drawing SVG: SVG stands for Scalable Vector Graphics. More info on Wikipedia. This is used to draw a sort of canvas to add the elements of our visualization. Create the SVG element with width w and height h. Add this code to the top of the example as shown: var w = 500;var h = 300;var barPadding = 3; # This will be used later for bar chartsvar padding = 40; # This toovar svg = d3. select( body )      . append( svg );      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  };}d3. csv( file. csv , rowConverter). then(function(dataset) {  d3. select( body ). selectAll( p )  . data(dataset)  . enter()  . append( p );});Reload the page and look at the HTML code. An svg tag has been created: &lt;svg width= 500  height= 300 &gt;&lt;/svg&gt;You can see the svg on the page shows a small rectangle: (This is an image) Creating a Bar Chart: Using the CSV dataset: year,population1950,51951,101955,151959,20We are loading it as file. csv and we will add the bar chart code inside the callback function. d3. csv( file. csv , rowConverter). then(function(dataset) {  // add code here});Create rectangle shapes by adding attributes for (x,y), where x is measured from left to right of the SVG and y is measured from top to bottom of the SVG. The coordinate (0,0) is the top left corner. The coordinates increase to the right for x and down for y. Also, x points to the bottom left of the rectangle, y points to the top left of the rectangle. You are drawing kind of upside down. That’s how weird SVG is. Drawing each rectangle for the bar chart: In this example, width draws 20px to the right, and height draws 100px downwards from the y coordinate. In other words, all rectangles of the bar chart are drawn downwards, following the characteristics of the SVG having its (0,0) on the top left corner. (This is an image) So far we have this: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , 0)    . attr( y , 0)    . attr( width , 20)    . attr( height , 100);});Reload the browser to see the result and the HTML: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;  &lt;rect x= 0  y= 0  width= 20  height= 100 &gt;&lt;/rect&gt;&lt;/svg&gt;However by setting . attr( x , 0), it makes the bar chart rectangles to overlap on top of each other, as they are all drawn at the same (x, y) position. Scaling the coordinates of the rectangles: The coordinates of the rectangles need to scale dynamically. For example for x values, use an anonymous function to pass each value d of the dataset, and the index i for each value. Then compute i times the width w over the length of the dataset. Do the calculation by hand and you will understand how it works. Update the code to this and reload the browser: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return i * (w/dataset. length);      })    . attr( y , function(d) {      return h - d. population;      })    . attr( width , w/dataset. length - barPadding)    . attr( height , function(d) {      return d. population;      })    . attr( fill ,  teal );});The result is this: (This is an image) The HTML shows this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 0  y= 295  width= 122  height= 5  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 125  y= 290  width= 122  height= 10  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 250  y= 285  width= 122  height= 15  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 375  y= 280  width= 122  height= 20  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;How this works: attr( x , function(d, i): We need to set the x position of each rectangle. This points to the bottom left of each rectangle. attr( x , function(d, i) {  return i * (w/dataset. length);})For each value of the dataset: year   population1950    51951    101955    151959    20The corresponding x values are as follows. Keep in mind the d values are not used for the calculation, only the index (i) of each value. First, let’s look at the indices:  d      i (index)[1950, 5]    0[1951, 10]   1[1955, 15]   2[1950, 20]   3The calculation (w/dataset. length) is done to evenly position each rectangle across the width w of the SVG.  w was set before as var w = 500 length of the dataset is 4 i is used to find a starting position for each rectangle. The calculations of i * (w/dataset. length) are. 0 * (500/4)1 * (500/4)2 * (500/4)3 * (500/4)Then they are returned to the call back function as a parameter of each x attribute. . attr( x , 0). attr( x , 125). attr( x , 250). attr( x , 375)This means the first rectangle is positioned at x = 0, then second rectangle at x = 125 and so on. How this works: . attr( y , function(d): We need to set the y position of each rectangle. This points to the top left of each rectangle. . attr( y , function(d) {  return h - d. population;})In this code h was previously set as var h = 300;. The call back function is passing every row of the dataset as d and we need the second value population of each row to calculate the y coordinate of each rectangle. return h - d. population;Given these values, where a value of d corresponds to [d. year, d. population].   d[1950, 5][1951, 10][1955, 15][1950, 20]The subtractions are: 300 - 5 = 295300 - 10 = 290300 - 15 = 285300 - 20 = 280For the SVG the (0,0) is at the top left. Then y value of 295 means measure 295px from top moving downwards. These are returned to the call back function to form these: . attr( y , 295). attr( y , 290). attr( y , 285). attr( y , 280)Putting these two attributes together, we can get the coordinates for each rectangle . attr( x , 0)    . attr( y , 295). attr( x , 125)    . attr( y , 290). attr( x , 250)    . attr( y , 285). attr( x , 375)    . attr( y , 280)The rectangles are at these coordinates:  Rectangle 1: (0, 295) Rectangle 2: (125, 290) Rectangle 3: (250, 285) Rectangle 4: (375, 280)How this works: . attr( width , w/dataset. length - barPadding): Now that we got the coordinates for each rectangle, then we need to draw them. The . attr width, draws each rectangle from left to right. We know that (w/dataset. length) = 500 / 4. We are using a variable barPadding that we should have defined previously to make some separation between the rectangles. var barPadding = 3;The width is the same for all rectangles: w/dataset. length - barPadding = 500/4 - 3 = 122This value is then used as a parameter: . attr( width , w/dataset. length - barPadding). attr( width , 122)How this works: . attr( height , function(d): As seen here: . attr( height , function(d) {  return d. population;})Set the height starting from the position y by returning the value d. population on the dataset. In other words, build the rectangle downwards from top to bottom starting at position y. Putting it all together using this format (x, width, y, height):  Rectangle 1: (0,  122, 295, 5) Rectangle 2: (125, 122, 290, 10) Rectangle 3: (250, 122, 285, 15) Rectangle 4: (375, 122, 280, 20)Here is a description of the first two rectangles:  Rect1 starts at x=0, draw width=122 towards the right, at position y=295, draw height=5 towards the bottom.  There is a barPadding = 3 towards the right, separating Rect1 from Rect2.  Rect2 starts x=125, draw width=122 towards the right, at position y=290, draw height=10 towards the bottom. Keep in mind again the weird SVG way of positioning/drawing elements left to right, top to bottom. Using Scales: This is where things get interesting and you can spend countless hours trying to fix the drawing in the SVG way. Definitions:  Input domain is the range of input data values Output range is the range of output valuesWith the example having an input domain of [5, 20] (From 5 to 20): dataset = [5, 10, 15, 20]And the output range of [10, 250] in pixels. Where the minimum input value of 5 is represented as 10px, and the maximum value of 20 is represented as 250px. The input value can be normalized and this result can be scaled to the output. For example normalizing input values to a range of 0 to 1. Then maybe the 0 can be represented as 0px and the maximum of 1 can be 100px. As shown below. The bar chart rectangles look too short compared to the SVG canvas. We can scale the rectangles to improve the visualization: (This is an image) Create a linear scale with scaleLinear(): Following the same CSV with this data: year   population1950    51951    101955    151959    20The yScale is used to scale the population  The input domain could start at 0, up to the max value of the population column The output range is to limit the drawing to the padded limits of the SVG height. As seen here: yScale = d3. scaleLinear()      . domain([0, d3. max(dataset, function(d) { return d. population; })])      . range([h - padding, padding]);Remember that the SVG way is that the coordinates increase left to right, top to bottom. For this: . range([h - padding, padding] The height h is the height of the SVG previously defined as var h = 300;.  padding was defined previously as var padding = 40; h - padding is 300 - 40 which is 260Then the range is: . range([260, 40])No, the range is not backwards. For SVG means the range is between the y coordinate 260 and the y coordinate 40. Which is the padded region of the height of the SVG. Create a Band scale with scaleBand(): More about scaleBand() in the D3 Scale Docs. The D3 Docs have this great image to help visualize the different methods: These are the methods available:  . domain() . range() . rangeRound() . round() . paddingInner() . paddingOuter() . padding() . align() . bandwidth() . step()Following the same CSV with this data: year   population1950    51951    101955    151959    20d3. scaleBand(). domain(): The xScale is used to scale the year column. Instead of using it as a Date, it maps the range of the dataset length to a width range. : xScale = d3. scaleBand()      . domain(d3. range(dataset. length))The domain . domain(d3. range(dataset. length)) is a range of dataset. length: d3. range(dataset. length)You can use the console. log() inside the CSV function to test: d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset. length);  console. log(d3. range(dataset. length));  . . . The console should output this: 4Array(4) [ 0, 1, 2, 3 ]The domain for scaleBand() is using this: . domain([0, 1, 2, 3])d3. scaleBand(). range(): Setting the range to fit within the width of the SVG: xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . range([0, w]);The range calculates even bars using this formula (w - 0) / . domain(). length. In our case that is 500/4 = 125. d3. scaleBand(). paddingInner(): As explained in the API docs. It says that if you don’t specify a paddingInner, the default is zero 0. If you specify one, the number must be &lt;= 1. A value of 0 means there is no blank space between the bars. A value of 1 means a bandwidth of 0. xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . range([0, w])      . paddingInner(0. 1);d3. scaleBand(). paddingOuter(): This is the padding to apply before the first bar and after the last bar. The number is in the range of [0, 1]. If not specified, the padding returns zero. xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . range([0, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);d3. scaleBand(). rangeRound(): The range can calculate long decimal numbers such as this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 12. 195121951219477  y= 205  width= 122  height= 55  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 134. 14634146341461  y= 150  width= 122  height= 110  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 256. 0975609756098  y= 95  width= 122  height= 165  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 378. 04878048780483  y= 40  width= 122  height= 220  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;We can replace range with rangeRound. xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . rangeRound([padding, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);The HTML now shows something like this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 14  y= 205  width= 122  height= 55  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 135  y= 150  width= 122  height= 110  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 256  y= 95  width= 122  height= 165  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 377  y= 40  width= 122  height= 220  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;d3. scaleBand(). bandwidth(): This is used later to return the width of each bar. Applying scales scaleBand() and scaleLinear(): We need to update the code and add the scale functions: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( body )      . append( svg )      . attr( width , w)      . attr( height , h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , function(d) {      return yScale(d. population);    })    . attr( width , xScale. bandwidth())    . attr( height , function(d) {      return h - padding - yScale(d. population);    })    . attr( fill ,  teal );});The result is this bar chart: (This is an image) The HTML is this: &lt;svg width= 500  height= 300 &gt;  &lt;rect x= 52  y= 205  width= 101  height= 55  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 164  y= 150  width= 101  height= 110  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 276  y= 95  width= 101  height= 165  fill= teal &gt;&lt;/rect&gt;  &lt;rect x= 388  y= 40  width= 101  height= 220  fill= teal &gt;&lt;/rect&gt;&lt;/svg&gt;Not exactly sure how the width was calculated to 101. I know that the rangeRound([padding, w]) calculates each bar width to (w-40)/domain(). length equivalent to (500 - 40) / 4 = 115. Using the inner and outer padding, somehow this is reduced to 101. You can use console. log() to test the output such as in here: svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    console. log( x );    console. log(d);    console. log(xScale. domain());    console. log(xScale. range());    console. log(xScale. paddingInner());    console. log(xScale. paddingOuter());    console. log(xScale. bandwidth());    console. log(xScale(i));    return xScale(i);  })Barchart Visualization: This how the bar chart looks like: (This is a D3 vis) D3 Responsive Visualization on index. html and project. js: Continuing adding responsive to the project. js, we need to make two changes: The index. html now says this. I added &lt;div id= d3-barchart-vis &gt;&lt;/div&gt;: &lt;!DOCTYPE html&gt;&lt;html lang= en &gt;  &lt;head&gt;    &lt;meta charset= utf-8 &gt;    &lt;title&gt;D3 Canvas&lt;/title&gt;    &lt;link rel= icon  type= image/png  href= icon. png &gt;    &lt;script type= text/javascript  src= assets/js/d3. min. js &gt;&lt;/script&gt;    &lt;link rel= stylesheet  type= text/css  href= main. css &gt;  &lt;/head&gt;  &lt;body&gt;    &lt;div id= d3-barchart-vis &gt;&lt;/div&gt;    &lt;script type= text/javascript  src= project. js &gt;&lt;/script&gt;  &lt;/body&gt;&lt;/html&gt;The project. js needs to be modified when creating the svg. It selects div id= d3-barchart-vis . It uses preserveAspectRatio and viewBox for responsive. The viewBox follows this syntax  min-x min-y width height . I concatenated the numbers and the spaces: var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);The code for project. js looks like this: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , function(d) {      return yScale(d. population);    })    . attr( width , xScale. bandwidth())    . attr( height , function(d) {      return h - padding - yScale(d. population);    })    . attr( fill ,  teal );});Using Arrow Functions: More about arrow functions on Mozilla. Given this example: elements. map(function(element) {  return element. length;});This can be simplified to: elements. map((element) =&gt; {  return element. length;});If there is only one parameter and the only statement is return then it can be simplified to this: elements. map(element =&gt; element. length);Refactoring svg. selectAll( rect ) var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal );});Adding Axes to our Bar Chart: Use any of these: d3. axisTop, d3. axisBottom, d3. axisLeft, d3. axisRight. Then append a g (group) element to the end of the SVG. var xAxis = d3. axisBottom(xScale);var yAxis = d3. axisLeft(yScale);Put this at the end of the script, since the graphics lay on top of each other, making the axes the last visible graph at the top of the SVG. svg. append( g )  . attr( class ,  axis )  . attr( transform ,  translate(0,  + (h - padding) +  ) )  . call(xAxis);svg. append( g )  . attr( class ,  axis )  . attr( transform ,  translate(  + padding +  ,0) )  . call(yAxis);This line is a string concatenation: . attr( transform ,  translate(0,  + (h - padding) +  ) )It computes to the following: . attr( transform ,  translate(0 , + (300 - 40 ) +  ) )Then the result is: svg. append( g )  . attr( class ,  axis )  . attr( transform ,  translate(0,260) )  . call(xAxis);The translate function moves the object by x and y. More details in the MDN Web Docs. The bar chart should now look like this: (This is an image) The complete code with x and y axes: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  var xAxis = d3. axisBottom(xScale);  var yAxis = d3. axisLeft(yScale);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal );  svg. append( g )    . attr( class ,  x axis )    . attr( transform ,  translate(0,  + (h - padding) +  ) )    . call(xAxis);  svg. append( g )    . attr( class ,  y axis )    . attr( transform ,  translate(  + padding +  ,0) )    . call(yAxis);});Adding ticks on the Axes: Use . ticks(). However, D3 will override this if it wants to divide the input domain evenly. Use . tickValues([an array of values]) to set them manually. Use . tickFormat to format the axis labels. var xAxis = d3. axisBottom(xScale)       . ticks(someParameterHere);For now I will not use this. Adding a Title to the Graph: Use append( text ) to append a title to the graph: svg. append( text )  . attr( x , w/2)  . attr( y , padding)  . attr( text-anchor ,  middle )  . style( font-size ,  16px )  . text( Awesome Barchart );Add a label for the x Axis: A label can be added to the x Axis by appending a text and using the transform and translate to position the text. The function translate uses a string concatenation to get to translate(w/2, h-10) which is calculated to translate(500/2, 300-10) or translate(250, 290). Where x is in the middle of the SVG and y is 10px from the bottom (or 290px from the top). svg. append( text )  . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )  . style( text-anchor ,  middle )  . text( Year );Add a label for the y Axis: The label for the y Axis is a bit different. First we need to rotate the label vertically with a negative -90 degrees. Then the point of reference for (0,0) changes. If I am not mistaken it’s now on the top right relative to the rotated text. To center the text vertically. Move it half way to the left at an x distance of -(h/2). The y is set relative to the rotated position. If you move it down (to the right) it will be a positive number (towards the y Axis). If you move it up (to the left) it will be a negative number (away from the y Axis). svg. append( text )  . attr( transform ,  rotate(-90) )  . attr( x , -(h/2))  . attr( y , 15)  . style( text-anchor ,  middle )  . text( Population );The barchart looks like this: (This is an image) The code with the labels is now this: var w = 500;var h = 300;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  var xAxis = d3. axisBottom(xScale);  var yAxis = d3. axisLeft(yScale);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal );  // Add the x Axis  svg. append( g )    . attr( class ,  x axis )    . attr( transform ,  translate(0,  + (h - padding) +  ) )    . call(xAxis);  // Label for x Axis  svg. append( text )    . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )    . style( text-anchor ,  middle )    . text( Year );  // Add the y Axis  svg. append( g )    . attr( class ,  y axis )    . attr( transform ,  translate(  + padding +  ,0) )    . call(yAxis);  // Label for y Axis  svg. append( text )    . attr( transform ,  rotate(-90) )    . attr( x , -(h/2))    . attr( y , 15)    . style( text-anchor ,  middle )    . text( Population );  svg. append( text )    . attr( x , w/2)    . attr( y , padding)    . attr( text-anchor ,  middle )    . style( font-size ,  16px )    . text( Awesome Barchart );});Adding Margins: The label for the y Axis is too close to the left. I had to plug a few different numbers in . attr( y , 15) to make it fit. // Label for y Axissvg. append( text )  . attr( transform ,  rotate(-90) )  . attr( x , -(h/2))  . attr( y , 15)  . style( text-anchor ,  middle )  . text( Population );We can add the margins like this: var margin = {top: 20, right: 20, bottom: 20, left: 20},  w = 500 - margin. left - margin. right,  h = 300 - margin. top - margin. bottom;Perhaps the labels are too big. They are the same size as the title. I added a font size for the labels, changed the position of the y label, and added the margins. The barchart looks like this: (This is an image) The code with the margins is now this: var margin = {top: 20, right: 20, bottom: 20, left: 20},    w = 500 - margin. left - margin. right,    h = 300 - margin. top - margin. bottom;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(d3. range(dataset. length))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, function(d) { return d. population; })])        . range([h - padding, padding]);  var xAxis = d3. axisBottom(xScale);  var yAxis = d3. axisLeft(yScale);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , function(d, i) {      return xScale(i);    })    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal );  // Add the x Axis  svg. append( g )    . attr( class ,  x axis )    . attr( transform ,  translate(0,  + (h - padding) +  ) )    . call(xAxis);  // Label for x Axis  svg. append( text )    . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Year );  // Add the y Axis  svg. append( g )    . attr( class ,  y axis )    . attr( transform ,  translate(  + padding +  ,0) )    . call(yAxis);  // Label for y Axis  svg. append( text )    . attr( transform ,  rotate(-90) )    . attr( x , -(h/2))    . attr( y , 10)    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Population );  svg. append( text )    . attr( x , w/2)    . attr( y , padding)    . attr( text-anchor ,  middle )    . style( font-size ,  16px )    . text( Awesome Barchart );});Troubleshooting the Year on the x Axis: The barchart still doesn’t look correct. The ticks on the x Axis are supposed to be years and not integers starting at 1. (This is an image) I need to modify the scaleBand() so the domain maps to the years on the converted Date objects from the CSV file. It currently looks like this: xScale = d3. scaleBand()      . domain(d3. range(dataset. length))      . rangeRound([padding, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);I changed it to this: xScale = d3. scaleBand()      . domain(dataset. map(d =&gt; d. year. getFullYear()))      . rangeRound([padding, w])      . paddingInner(0. 1)      . paddingOuter(0. 1);You can test what this does on the console dataset. map(d =&gt; d. year. getFullYear()) inside this section: d3. csv( file. csv , rowConverter). then(function(dataset) {  console. log(dataset. map(d =&gt; d. year. getFullYear()))The output should be: Array(4) [ 1950, 1951, 1955, 1959 ]Then modify implementing the xScale in this section. From this return xScale(i): svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    return xScale(i);  })  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal );Modified to this return xScale(d. year. getFullYear()): svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , function(d, i) {    return xScale(d. year. getFullYear());  })  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal );The corrected barchart looks like this: (This is an image) The code with a bit more arrow functions: var margin = {top: 20, right: 20, bottom: 20, left: 20},    w = 500 - margin. left - margin. right,    h = 300 - margin. top - margin. bottom;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(dataset. map(d =&gt; d. year. getFullYear()))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, d =&gt; d. population)])        . range([h - padding, padding]);  var xAxis = d3. axisBottom(xScale);  var yAxis = d3. axisLeft(yScale);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , d =&gt; xScale(d. year. getFullYear()))    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal );  // Add the x Axis  svg. append( g )    . attr( class ,  x axis )    . attr( transform ,  translate(0,  + (h - padding) +  ) )    . call(xAxis);  // Label for x Axis  svg. append( text )    . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Year );  // Add the y Axis  svg. append( g )    . attr( class ,  y axis )    . attr( transform ,  translate(  + padding +  ,0) )    . call(yAxis);  // Label for y Axis  svg. append( text )    . attr( transform ,  rotate(-90) )    . attr( x , -(h/2))    . attr( y , 10)    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Population );  svg. append( text )    . attr( x , w/2)    . attr( y , padding)    . attr( text-anchor ,  middle )    . style( font-size ,  16px )    . text( Awesome Barchart );});D3 Mouse Event Handler: I want the barchart to change colors when you mouse over a bin to highlight what you are pointing at. Add two functions here: svg. selectAll( rect )  . data(dataset)  . enter()  . append( rect )  . attr( x , d =&gt; xScale(d. year. getFullYear()))  . attr( y , d =&gt; yScale(d. population))  . attr( width , xScale. bandwidth())  . attr( height , d =&gt; h - padding - yScale(d. population))  . attr( fill ,  teal )  . on( mouseover , handleMouseOver)  . on( mouseout , handleMouseOut);Then add the functions to process each interaction at the end, before the closing of d3. csv: function handleMouseOver(d, i) {  d3. select(this)   . attr( fill ,  red );}function handleMouseOut(d, i) {  d3. select(this)   . attr( fill ,  teal );}The barchart with mouse event handler looks like this: (This is a D3 vis) The final code is this: var margin = {top: 20, right: 20, bottom: 20, left: 20},    w = 500 - margin. left - margin. right,    h = 300 - margin. top - margin. bottom;var barPadding = 3;var padding = 40;var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);var parseTime = d3. timeParse( %Y );var rowConverter = function(d) {  return {    year: parseTime(d. year),    population: +d. population  }}d3. csv( file. csv , rowConverter). then(function(dataset) {  xScale = d3. scaleBand()        . domain(dataset. map(d =&gt; d. year. getFullYear()))        . rangeRound([padding, w])        . paddingInner(0. 1)        . paddingOuter(0. 1);  yScale = d3. scaleLinear()        . domain([0, d3. max(dataset, d =&gt; d. population)])        . range([h - padding, padding]);  var xAxis = d3. axisBottom(xScale);  var yAxis = d3. axisLeft(yScale);  svg. selectAll( rect )    . data(dataset)    . enter()    . append( rect )    . attr( x , d =&gt; xScale(d. year. getFullYear()))    . attr( y , d =&gt; yScale(d. population))    . attr( width , xScale. bandwidth())    . attr( height , d =&gt; h - padding - yScale(d. population))    . attr( fill ,  teal )    . on( mouseover , handleMouseOver)    . on( mouseout , handleMouseOut);  // Add the x Axis  svg. append( g )    . attr( class ,  x axis )    . attr( transform ,  translate(0,  + (h - padding) +  ) )    . call(xAxis);  // Label for x Axis  svg. append( text )    . attr( transform ,  translate(  + (w/2) +   ,  + (h-10) +  ) )    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Year );  // Add the y Axis  svg. append( g )    . attr( class ,  y axis )    . attr( transform ,  translate(  + padding +  ,0) )    . call(yAxis);  // Label for y Axis  svg. append( text )    . attr( transform ,  rotate(-90) )    . attr( x , -(h/2))    . attr( y , 10)    . style( text-anchor ,  middle )    . style( font-size ,  12px )    . text( Population );  svg. append( text )    . attr( x , w/2)    . attr( y , padding)    . attr( text-anchor ,  middle )    . style( font-size ,  16px )    . text( Awesome Barchart );  function handleMouseOver(d, i) {    d3. select(this)     . attr( fill ,  red );  }  function handleMouseOut(d, i) {    d3. select(this)     . attr( fill ,  teal );  }});Embedding D3 Visualization on a blog post: You can also add a D3 visualization to a blog post. My blog follows this directory structure: assets/ csv/  file. csv js/  d3. min. js  d3-barchart-vis. js_posts/ this-blog-post. mdAdd a div with a descriptive id to the section of your blog post where you want to add your visualization: &lt;div id= d3-barchart-vis &gt;&lt;/div&gt;In the d3-barchart-vis. js, the svg selects this &lt;div id: var svg = d3. select( #d3-barchart-vis )      . append( svg )      . attr( preserveAspectRatio ,  xMinYMin meet )      . attr( viewBox ,  0 0   + w +     + h);The csv file is called like this so it points at the correct directory: d3. csv( . . /assets/csv/file. csv , rowConverter). then(function(dataset) {At the end of the blog post, after all content, add a reference to the JavaScript files: &lt;script type= text/javascript  src= . . /assets/js/d3. min. js &gt;&lt;/script&gt;&lt;script type= text/javascript  src= . . /assets/js/d3-barchart-vis. js &gt;&lt;/script&gt;"
    }, {
    "id": 161,
    "url": "https://www.tomordonez.com/data-wrangling-openrefine-linux/",
    "title": "Data Wrangling with OpenRefine on Linux",
    "body": "2020/02/01 - OpenRefine is an open source desktop application for data wrangling. Source:  Download InstallOpenRefine on Windows:  Download the file Unzip and run the executable To stop the web server, on the command line do Ctrl C. OpenRefine on Linux:  Download the tar file. Size is about 100 MB Tar the file. For example: tar xzf openrefine-linux-3. 2. tar. gz Open the directory: cd openrefine-3. 2 Start: . /refine(Shut down the webserver with Ctrl C) Upon start the terminal shows: WARNING: OpenRefine is not tested and not recommended for use with Java versions greater than 12. You have 7653M of free memory. Your current configuration is set to use 1400M of memory. OpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:https://github. com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-MemoryStarting OpenRefine at 'http://127. 0. 0. 1:3333/'10:40:59. 487 [refine_server] Starting Server bound to '127. 0. 0. 1:3333' (0ms)10:40:59. 488 [refine_server] refine. memory size: 1400M JVM Max heap: 1468006400 (1ms)10:40:59. 499 [refine_server] Initializing context: '/' from '/home/tom/Documents/projects/openrefine-3. 2/webapp' (11ms)10:41:01. 007 [refine_server] Creating new workspace directory /home/tom/. local/share/openrefine (1508ms)SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/home/tom/Documents/projects/openrefine-3. 2/server/target/lib/slf4j-log4j12-1. 7. 18. jar!/org/slf4j/impl/StaticLoggerBinder. class]SLF4J: Found binding in [jar:file:/home/tom/Documents/projects/openrefine-3. 2/webapp/WEB-INF/lib/slf4j-log4j12-1. 7. 18. jar!/org/slf4j/impl/StaticLoggerBinder. class]SLF4J: See http://www. slf4j. org/codes. html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org. slf4j. impl. Log4jLoggerFactory]10:41:01. 390 [refine] Starting OpenRefine 3. 2 [55c921b]. . . (383ms)10:41:01. 390 [refine] initializing FileProjectManager with dir (0ms)10:41:01. 390 [refine] /home/tom/. local/share/openrefine (0ms)10:41:01. 417 [FileProjectManager] Failed to load workspace from any attempted alternatives. (27ms)WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org. python. core. PySystemState (file:/home/tom/Documents/projects/openrefine-3. 2/webapp/extensions/jython/module/MOD-INF/lib/jython-standalone-2. 7. 1. jar) to method java. io. Console. encoding()WARNING: Please consider reporting this to the maintainers of org. python. core. PySystemStateWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseFontconfig warning:  /usr/share/fontconfig/conf. avail/05-reset-dirs-sample. conf , line 6: unknown element  reset-dirs Opening in existing browser session. 10:41:19. 486 [refine] POST /command/core/load-language (18069ms)10:41:19. 578 [refine] GET /command/core/get-preference (92ms)10:41:19. 624 [refine] POST /command/core/load-language (46ms)10:41:19. 641 [refine] POST /command/core/load-language (17ms)10:41:19. 833 [refine] POST /command/core/get-importing-configuration (192ms)10:41:19. 885 [refine] GET /command/core/get-all-project-tags (52ms)10:41:19. 975 [refine] GET /command/core/get-all-project-metadata (90ms)10:41:20. 077 [refine] GET /command/core/get-languages (102ms)10:41:20. 342 [refine] GET /command/core/get-version (265ms)10:41:20. 566 [refine] GET /command/database/saved-connection (224ms)10:41:20. 632 [refine] POST /command/core/set-preference (66ms)10:41:20. 664 [refine] POST /command/core/set-preference (32ms)On the browser OpenRefine opens on: http://127. 0. 0. 1:3333/Uploading data to OpenRefine: Get data from:  This computer Web Addresses Clipboard Database Google dataGetting data from your computer:  Use upload This creates a preview with some options.  On the top right corner Create ProjectText Facet to Group data: Go to a column drop down.  Go to Facet Then Text FacetOn the left side, it groups together identical cells for that column, including the number of cells for each group. As seen on this Youtube video. Selecting Text Facet for one of the columns to review all the data related FFP. For example, selecting one of the groups that has FFP and a count of 512, filters the data on the right view. The next group is also called FFP with a count of 1. Click on edit for that group to see why. It has an empty trailing whitespace at the end of the word FFP_. Remove the blank space and click apply. The groups then will merge. Now the FFP group has 513 rows and a total of 814 groups. Remove leading and trailing whitespace: Go to a column drop down.  Go to Edit cells Then Common transforms Then Trim leading and trailing whitespaceFollowing the same video. After removing leading and trailing whitespace. The group count changed from 814 to 785. The column is renamed from FFP to Firm Fixed Price and then apply. Sort the Facets: On the left view, sort the facets by name or by count to see the largest groups. The video shows that there are all sorts of combinations of the word Firm Fixed Price which can be changed to group all the data better. Cluster Facets: After sorting by count, next to it there is a Cluster button. This feature tries to group the groups based on Keying Functions. The Keying Functions show a drop down with:  fingerprint ngram-fingerprint double-metaphoneSelect the groups that you want to merge. There is a checkbox for each group. Next to the checkbox there is a field New Cell Value if you want to change the name of the cell. If you feel wild, you can Select All on the bottom left and click on Merge Selected &amp; Close on the bottom right. Correcting Mistakes: On the left view, there is a tab Undo/Redo. Then select one of the rows that made a previous change. Then click Apply. Create a Numeric Facet: On the video, one of the columns says Total value.  Drop down Facet Numeric FacetOn the left view, it shows a range of the values, going from 0. 0 to 20,000,000. 00. It also shows a mini visualization of the distribution. In this case the distribution only shows a rectangle on the leftmost side. On the top right of this visualization, click on change. A popup window says Edit Facet's Expression based on Column Total value. There is a field called Expression Type value. log(). It shows a preview of the change…then click OK. It now shows a normal distribution. It has some options below:  Numeric 5173 (checked) Non-numeric 0 (not checked) Blank 0 (not checked) Error 27 (checked)Select only the Error rows. Which shows all the values as zero. When calculating value. log() of 0, the result is negative infinity. In the video she asks, why is there a zero value for total value? You can drag the margins of the distribution visualization to preview data on the right view. Getting data from a Wikipedia list: This video blew my mind. I didn’t know you could do this: Go to this page that shows awards about movies. Scroll down to the section that says Winners and nominees. Click on the edit to the right of this title. It now shows some code as seen here: ==Winners and nominees=={| class= wikitable |+ Table key|-!scope= row  style= text-align:center;  style= background:#FAEB86; height:20px; width:20px | Indicates the winner|}===1950s==={| class= wikitable sortable  rowspan=2 style= text-align: left;  border= 2  cellpadding= 5 |-!scope= col  style= width:3%; text-align:center; | Year!scope= col  style= width:3%;text-align:center; | Photos of winners!scope= col  style= width:15%;text-align:center; | Actor!scope= col  style= width:15%;text-align:center; | Role(s)!scope= col  style= width:15%;text-align:center; | Film|-! scope= row  rowspan=2 style= text-align:center  | 1954 &lt;br /&gt;&lt;small&gt;[[1st Filmfare Awards|(1st)]] &lt;/small&gt;|rowspan=2 style= text-align:center || style= background:#FAEB86;  || style= background:#FAEB86;  || style= background:#FAEB86;  | '''''[[Baiju Bawra (film)|Baiju Bawra]]'''''Copy all the code that appears on that window and save it into a txt file. Import this file into OpenRefine. For the option When parsing text files uncheck Split into columns. For Header lines change to 0. Then Create Project. Cleaning Wikipedia data code: Remove the rows that show years such as this: ===1950s===On the column drop down, select Text filter. On the left view that has a field titled Column. Type ===, which filters the table on the right to all rows that contain these characters. On this filtered view go to drop down:  Edit rows Remove all matching rows Then clear the text filter on the left view (close the Column window)On the video, she explains the strategy for using OpenRefine as:  Isolating all the rows you want to change using filters and facets Change them all at the same time. Remove all the characters that describe a bolded text. In the code, bold text is enclosed within three single quotes such as this: '''content&lt;article class= post h-entry  itemscope itemtype= http://schema. org/BlogPosting &gt;  Python Jupyter Notebook in Linux     Jan 14, 2020   •      Update conda $ conda update condaCreate a yml file, use a name and dependencies. Example: File: env_py_3. 7. 6. yml Contents of this file: name: awesome_namedependencies:- python=3. 7. 6- jupyterCreate the environment using this file: $ conda env create --file env_py_3. 7. 6. yml$ conda activate awesome_nameCheck Python version $ which python$ python -VRun Jupyter Notebook: $ jupyter notebookIf this doesn’t work. Try it like this: $ jupyter notebook --ip=127. 0. 0. 1Cell timer for Jupyter notebook: As seen here $ pip install jupyter_contrib_nbextensions$ jupyter contrib nbextension install --user$ jupyter nbextension enable execute_time/ExecuteTime   Related Posts:        Python Adding Days to Date and Python Pandas     Python, Files, and OS Module     Install Miniconda on Linux     Make a Static Website with Python and Github Pages     Web Scraping with Python     Please enable JavaScript to view the comments powered by Disqus. &lt;/article&gt;highlighter_prefixhighlighter_suffixjekyllJekyll::Drops::JekyllDroplayout{“layout”=&gt;”default”}pageOpenRefine is an open source desktop application for data wrangling. Source:  Download InstallOpenRefine on Windows:  Download the file Unzip and run the executable To stop the web server, on the command line do Ctrl C. OpenRefine on Linux:  Download the tar file. Size is about 100 MB Tar the file. For example: tar xzf openrefine-linux-3. 2. tar. gz Open the directory: cd openrefine-3. 2 Start: . /refine(Shut down the webserver with Ctrl C) Upon start the terminal shows: WARNING: OpenRefine is not tested and not recommended for use with Java versions greater than 12. You have 7653M of free memory. Your current configuration is set to use 1400M of memory. OpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:https://github. com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-MemoryStarting OpenRefine at 'http://127. 0. 0. 1:3333/'10:40:59. 487 [refine_server] Starting Server bound to '127. 0. 0. 1:3333' (0ms)10:40:59. 488 [refine_server] refine. memory size: 1400M JVM Max heap: 1468006400 (1ms)10:40:59. 499 [refine_server] Initializing context: '/' from '/home/tom/Documents/projects/openrefine-3. 2/webapp' (11ms)10:41:01. 007 [refine_server] Creating new workspace directory /home/tom/. local/share/openrefine (1508ms)SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/home/tom/Documents/projects/openrefine-3. 2/server/target/lib/slf4j-log4j12-1. 7. 18. jar!/org/slf4j/impl/StaticLoggerBinder. class]SLF4J: Found binding in [jar:file:/home/tom/Documents/projects/openrefine-3. 2/webapp/WEB-INF/lib/slf4j-log4j12-1. 7. 18. jar!/org/slf4j/impl/StaticLoggerBinder. class]SLF4J: See http://www. slf4j. org/codes. html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org. slf4j. impl. Log4jLoggerFactory]10:41:01. 390 [refine] Starting OpenRefine 3. 2 [55c921b]. . . (383ms)10:41:01. 390 [refine] initializing FileProjectManager with dir (0ms)10:41:01. 390 [refine] /home/tom/. local/share/openrefine (0ms)10:41:01. 417 [FileProjectManager] Failed to load workspace from any attempted alternatives. (27ms)WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org. python. core. PySystemState (file:/home/tom/Documents/projects/openrefine-3. 2/webapp/extensions/jython/module/MOD-INF/lib/jython-standalone-2. 7. 1. jar) to method java. io. Console. encoding()WARNING: Please consider reporting this to the maintainers of org. python. core. PySystemStateWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseFontconfig warning:  /usr/share/fontconfig/conf. avail/05-reset-dirs-sample. conf , line 6: unknown element  reset-dirs Opening in existing browser session. 10:41:19. 486 [refine] POST /command/core/load-language (18069ms)10:41:19. 578 [refine] GET /command/core/get-preference (92ms)10:41:19. 624 [refine] POST /command/core/load-language (46ms)10:41:19. 641 [refine] POST /command/core/load-language (17ms)10:41:19. 833 [refine] POST /command/core/get-importing-configuration (192ms)10:41:19. 885 [refine] GET /command/core/get-all-project-tags (52ms)10:41:19. 975 [refine] GET /command/core/get-all-project-metadata (90ms)10:41:20. 077 [refine] GET /command/core/get-languages (102ms)10:41:20. 342 [refine] GET /command/core/get-version (265ms)10:41:20. 566 [refine] GET /command/database/saved-connection (224ms)10:41:20. 632 [refine] POST /command/core/set-preference (66ms)10:41:20. 664 [refine] POST /command/core/set-preference (32ms)On the browser OpenRefine opens on: http://127. 0. 0. 1:3333/Uploading data to OpenRefine: Get data from:  This computer Web Addresses Clipboard Database Google dataGetting data from your computer:  Use upload This creates a preview with some options.  On the top right corner Create ProjectText Facet to Group data: Go to a column drop down.  Go to Facet Then Text FacetOn the left side, it groups together identical cells for that column, including the number of cells for each group. As seen on this Youtube video. Selecting Text Facet for one of the columns to review all the data related FFP. For example, selecting one of the groups that has FFP and a count of 512, filters the data on the right view. The next group is also called FFP with a count of 1. Click on edit for that group to see why. It has an empty trailing whitespace at the end of the word FFP_. Remove the blank space and click apply. The groups then will merge. Now the FFP group has 513 rows and a total of 814 groups. Remove leading and trailing whitespace: Go to a column drop down.  Go to Edit cells Then Common transforms Then Trim leading and trailing whitespaceFollowing the same video. After removing leading and trailing whitespace. The group count changed from 814 to 785. The column is renamed from FFP to Firm Fixed Price and then apply. Sort the Facets: On the left view, sort the facets by name or by count to see the largest groups. The video shows that there are all sorts of combinations of the word Firm Fixed Price which can be changed to group all the data better. Cluster Facets: After sorting by count, next to it there is a Cluster button. This feature tries to group the groups based on Keying Functions. The Keying Functions show a drop down with:  fingerprint ngram-fingerprint double-metaphoneSelect the groups that you want to merge. There is a checkbox for each group. Next to the checkbox there is a field New Cell Value if you want to change the name of the cell. If you feel wild, you can Select All on the bottom left and click on Merge Selected &amp; Close on the bottom right. Correcting Mistakes: On the left view, there is a tab Undo/Redo. Then select one of the rows that made a previous change. Then click Apply. Create a Numeric Facet: On the video, one of the columns says Total value.  Drop down Facet Numeric FacetOn the left view, it shows a range of the values, going from 0. 0 to 20,000,000. 00. It also shows a mini visualization of the distribution. In this case the distribution only shows a rectangle on the leftmost side. On the top right of this visualization, click on change. A popup window says Edit Facet's Expression based on Column Total value. There is a field called Expression Type value. log(). It shows a preview of the change…then click OK. It now shows a normal distribution. It has some options below:  Numeric 5173 (checked) Non-numeric 0 (not checked) Blank 0 (not checked) Error 27 (checked)Select only the Error rows. Which shows all the values as zero. When calculating value. log() of 0, the result is negative infinity. In the video she asks, why is there a zero value for total value? You can drag the margins of the distribution visualization to preview data on the right view. Getting data from a Wikipedia list: This video blew my mind. I didn’t know you could do this: Go to this page that shows awards about movies. Scroll down to the section that says Winners and nominees. Click on the edit to the right of this title. It now shows some code as seen here: ==Winners and nominees=={| class= wikitable |+ Table key|-!scope= row  style= text-align:center;  style= background:#FAEB86; height:20px; width:20px | Indicates the winner|}===1950s==={| class= wikitable sortable  rowspan=2 style= text-align: left;  border= 2  cellpadding= 5 |-!scope= col  style= width:3%; text-align:center; | Year!scope= col  style= width:3%;text-align:center; | Photos of winners!scope= col  style= width:15%;text-align:center; | Actor!scope= col  style= width:15%;text-align:center; | Role(s)!scope= col  style= width:15%;text-align:center; | Film|-! scope= row  rowspan=2 style= text-align:center  | 1954 &lt;br /&gt;&lt;small&gt;[[1st Filmfare Awards|(1st)]] &lt;/small&gt;|rowspan=2 style= text-align:center || style= background:#FAEB86;  || style= background:#FAEB86;  || style= background:#FAEB86;  | '''''[[Baiju Bawra (film)|Baiju Bawra]]'''''Copy all the code that appears on that window and save it into a txt file. Import this file into OpenRefine. For the option When parsing text files uncheck Split into columns. For Header lines change to 0. Then Create Project. Cleaning Wikipedia data code: Remove the rows that show years such as this: ===1950s===On the column drop down, select Text filter. On the left view that has a field titled Column. Type ===, which filters the table on the right to all rows that contain these characters. On this filtered view go to drop down:  Edit rows Remove all matching rows Then clear the text filter on the left view (close the Column window)On the video, she explains the strategy for using OpenRefine as:  Isolating all the rows you want to change using filters and facets Change them all at the same time. Remove all the characters that describe a bolded text. In the code, bold text is enclosed within three single quotes such as this: '''{{sort|Gauri|Gauri}}'''Go to:  Drop down Edit cells TransformIt opens a popup window Custom text transform on column Column. Inside Expression field, type value. replace( ''' ,   ), which means to replace three single quotes  '''  with empty   . Click OK to apply. Create another column from existing column: The current code is a bit different than the one shown on the video. On the video, it shows that each winner is on a bullet point, and the nominees on an indented sub bullet point. On the code, the idented rows start with two stars **. Create a column that describes if the row corresponds to a winner or a nominee.  Drop down Edit column Add column based on this column New column name: Is Winner Expression: not(value. startsWith( ** )) Click OKOn the new column Is Winner  Drop down Facet Text facet On the left view, select True to view the winner rows. Extract the years  Column drop down Edit column Add column based on this column New column name: Year Expression: value[1,5]Remove the years from the original column  Column drop down Edit cells Transform Expression: value. substring(6) This means send to value the substring starting from character 6. Remove the Is Winner facet to view the new columns created with respective rows. The nominees are missing the year, however they appear right after a winner of the same year.  Year (column) drop down Edit cells Fill down This copies each year into the blank cells belowSeparate the first column into the columns Actress, Film, Character.  Column drop down Edit column Split into several columns By separator: ` - ` (space dash space) Split into 2 columns at most A Column 2 is created. Separate the data from Column 2  Column2 drop down Edit column Split into several columns Separator ` as ` (space as space) Split into 2 columns at mostThen rename the columns. Drop down, edit column, Rename this column  Column 1: Actress Column 2: Film Column 3: CharacterExport the data into Wiki syntax  Top right Export The default is JSON Follow the video to see how to setup the Wiki syntaxExtract operations: The Undo/Redo allows the extract the operations done to a dataset.  Click on Extract Popup window Extract Operation History Operations selected on the left are encoded as JSON (preview on the right) Copy the JSON code Switch back to another project you want to clean Go to Undo/Redo Click Apply Paste the JSON code on that window Click on Perform OperationsGet latitude and longitude for address rows from an external source: This video shows a dataset with a column Headquarter that has the addresses for each Company column. She wants to get the latitude and longitude for each address using the API from nominatim. openstreetmap. org  Headquarters column Drop down Edit column Add column by fetching URLs Expression: 'http://nominatim. openstreetmap. org/search?format=json&amp;email=somemail@gmail. com%app=google-refine&amp;q=' + escape(value, 'url') Throttle delay: 1500 New column name: json Then it takes some time to load the data. A new column json appears on the right view with values for each row.  json column Drop down Edit column Add column based on this column Expression: with(value. parseJson()[0], pair, pair. lat + ',' + pair. lon) New column name: lat/lonTranslate rows from an external source: If the dataset has rows in different languages in a column called text. Follow the API requirements to construct the URL.  Drop down Edit column Add column by fetching URLs Expression:  http://ajax. googleapis. com/ajax/services/language/detect?. . .   + escape(value. substring(0, 128),  url ) New column name: json Throttle delay: 100A new column json is loaded  json column Drop down Edit column Add column based on this column Expression: value. parseJson(). responseData. language New column name: languageReconcile data with external sources: The video also shows a way to reconcile your data with external sources. This dataset has two columns film and rating  film column Drop down Reconcile Start reconciling Freebase reconciliation service It loads a list of types Select the type that matches this film column Select the type Film Click on Start ReconcilingFetch more data from Freebase  film column drop down edit column Add columns from Freebase Options appear for Suggested Properties Select Directed by and Netflix ID Click OKOpenRefine Docs: Please follow these resources:  OpenRefine wiki ScreencastspaginatorsiteJekyll::Drops::SiteDrop’’’Go to:  Drop down Edit cells TransformIt opens a popup window Custom text transform on column Column. Inside Expression field, type value. replace( ''' ,   ), which means to replace three single quotes  '''  with empty   . Click OK to apply. Create another column from existing column: The current code is a bit different than the one shown on the video. On the video, it shows that each winner is on a bullet point, and the nominees on an indented sub bullet point. On the code, the idented rows start with two stars **. Create a column that describes if the row corresponds to a winner or a nominee.  Drop down Edit column Add column based on this column New column name: Is Winner Expression: not(value. startsWith( ** )) Click OKOn the new column Is Winner  Drop down Facet Text facet On the left view, select True to view the winner rows. Extract the years  Column drop down Edit column Add column based on this column New column name: Year Expression: value[1,5]Remove the years from the original column  Column drop down Edit cells Transform Expression: value. substring(6) This means send to value the substring starting from character 6. Remove the Is Winner facet to view the new columns created with respective rows. The nominees are missing the year, however they appear right after a winner of the same year.  Year (column) drop down Edit cells Fill down This copies each year into the blank cells belowSeparate the first column into the columns Actress, Film, Character.  Column drop down Edit column Split into several columns By separator: ` - ` (space dash space) Split into 2 columns at most A Column 2 is created. Separate the data from Column 2  Column2 drop down Edit column Split into several columns Separator ` as ` (space as space) Split into 2 columns at mostThen rename the columns. Drop down, edit column, Rename this column  Column 1: Actress Column 2: Film Column 3: CharacterExport the data into Wiki syntax  Top right Export The default is JSON Follow the video to see how to setup the Wiki syntaxExtract operations: The Undo/Redo allows the extract the operations done to a dataset.  Click on Extract Popup window Extract Operation History Operations selected on the left are encoded as JSON (preview on the right) Copy the JSON code Switch back to another project you want to clean Go to Undo/Redo Click Apply Paste the JSON code on that window Click on Perform OperationsGet latitude and longitude for address rows from an external source: This video shows a dataset with a column Headquarter that has the addresses for each Company column. She wants to get the latitude and longitude for each address using the API from nominatim. openstreetmap. org  Headquarters column Drop down Edit column Add column by fetching URLs Expression: 'http://nominatim. openstreetmap. org/search?format=json&amp;email=somemail@gmail. com%app=google-refine&amp;q=' + escape(value, 'url') Throttle delay: 1500 New column name: json Then it takes some time to load the data. A new column json appears on the right view with values for each row.  json column Drop down Edit column Add column based on this column Expression: with(value. parseJson()[0], pair, pair. lat + ',' + pair. lon) New column name: lat/lonTranslate rows from an external source: If the dataset has rows in different languages in a column called text. Follow the API requirements to construct the URL.  Drop down Edit column Add column by fetching URLs Expression:  http://ajax. googleapis. com/ajax/services/language/detect?. . .   + escape(value. substring(0, 128),  url ) New column name: json Throttle delay: 100A new column json is loaded  json column Drop down Edit column Add column based on this column Expression: value. parseJson(). responseData. language New column name: languageReconcile data with external sources: The video also shows a way to reconcile your data with external sources. This dataset has two columns film and rating  film column Drop down Reconcile Start reconciling Freebase reconciliation service It loads a list of types Select the type that matches this film column Select the type Film Click on Start ReconcilingFetch more data from Freebase  film column drop down edit column Add columns from Freebase Options appear for Suggested Properties Select Directed by and Netflix ID Click OKOpenRefine Docs: Please follow these resources:  OpenRefine wiki Screencasts"
    }, {
    "id": 162,
    "url": "https://www.tomordonez.com/python-jupyter-notebook-linux/",
    "title": "Python Jupyter Notebook in Linux",
    "body": "2020/01/14 - Update conda $ conda update condaCreate a yml file, use a name and dependencies. Example: File: env_py_3. 7. 6. yml Contents of this file: name: awesome_namedependencies:- python=3. 7. 6- jupyterCreate the environment using this file: $ conda env create --file env_py_3. 7. 6. yml$ conda activate awesome_nameCheck Python version $ which python$ python -VRun Jupyter Notebook: $ jupyter notebookIf this doesn’t work. Try it like this: $ jupyter notebook --ip=127. 0. 0. 1Cell timer for Jupyter notebook: As seen here $ pip install jupyter_contrib_nbextensions$ jupyter contrib nbextension install --user$ jupyter nbextension enable execute_time/ExecuteTime"
    }, {
    "id": 163,
    "url": "https://www.tomordonez.com/increase-boot-size-linux/",
    "title": "Increase Boot Size in Linux Fedora",
    "body": "2020/01/11 - I keep having this issue on a local Linux server. When I want to update the system, after it downloads the updates and it’s ready to install them, it says the boot size is too small. Review the boot size: $ df -hFilesystem  Size Used Avail Use% Mounted on/dev/sdb1  190M 151M 26M  86%  /bootIncrease Boot Size: According to this discussion on the Redhat forum. Increasing the boot size is possible but it’s not a supported method, as it might create instability in the disk. The recommendation is backing up the system and choosing a larger partition with a fresh install. Redhat recommends at least 1GB for /boot. It is obviously a valid advice. But doing this could take hours. These are the options to reduce the size of Used space. Remove Old Kernels: As seen here Check kernels: $ dnf list kernelThis can take a few minutes. My output was: Installed Packageskernel. x86_64 5. 3. 8-200. fc30  @updateskernel. x86_64 5. 3. 11-300. fc31  @updateskernel. x86_64 5. 3. 14-300. fc31  @updatesAvailable Packageskernel. x86_64 5. 4. 8-200. fc31  updatesRemove the number of kernels: $ sudo dnf remove $(dnf repoquery --installonly --latest-limit=-2 -q)If this doesn’t work. Try restarting. Then run again. Then try updating system again. You can also try limiting the number of kernels to 1: $ sudo dnf remove $(dnf repoquery --installonly --latest-limit=-1 -q)Now it shows: $ df -hFilesystem  Size Used Avail Use% Mounted on/dev/sdb1  190M 114M 63M  65%  /boot"
    }, {
    "id": 164,
    "url": "https://www.tomordonez.com/troubleshooting-building-android-app/",
    "title": "Troubleshooting Building an Android App",
    "body": "2019/12/19 - Using Android Studio 3. 5, I had to load a project that was built in another version. ERROR: failed to find target with hash string ‘android-22’: When building the app it got stuck on this error: ERROR: failed to find target with hash string 'android-22' in: /home/tom/Android/SdkInstall missing platform(s) and sync projectI clicked on Install missing platform(s) Packages to install: - Android SDK Platform 22 (platforms;android-22)Preparing  Install Android SDK Platform 22 (revision: 2) . Downloading https://dl. google. com/android/repository/android-22_r02. zip Install Android SDK Platform 22 (revision: 2)  ready. Installing Android SDK Platform 22 in /home/tom/Android/Sdk/platforms/android-22 Install Android SDK Platform 22 (revision: 2)  complete.  Install Android SDK Platform 22 (revision: 2)  finished. ERROR: failed to find Build Tools revision 23. 0. 2: Then I got this error ERROR: failed to find Build Tools revision 23. 0. 2Install Build Tools 23. 0. 2 and sync projectUpgrade plugin to version 3. 5. 0 and sync projectI clicked on Install Build Tools 23. 0. 2 Packages to install: - Android SDK Build-Tools 23. 0. 2 (build-tools;23. 0. 2)Preparing  Install Android SDK Build-Tools 23. 0. 2 (revision: 23. 0. 2) . Downloading https://dl. google. com/android/repository/build-tools_r23. 0. 2-linux. zip Install Android SDK Build-Tools 23. 0. 2 (revision: 23. 0. 2)  ready. Installing Android SDK Build-Tools 23. 0. 2 in /home/tom/Android/Sdk/build-tools/23. 0. 2 Install Android SDK Build-Tools 23. 0. 2 (revision: 23. 0. 2)  complete.  Install Android SDK Build-Tools 23. 0. 2 (revision: 23. 0. 2)  finished. ERROR: Failed to resolve: com. android. support:appcompat-v7:22. 2. 1: Then I got these errors ERROR: Failed to resolve: com. android. support:appcompat-v7:22. 2. 1Add Google Maven repository and sync projectShow in Project Structure dialogAffected Modules: appERROR: Failed to resolve: com. android. support:design:22. 2. 1Add Google Maven repository and sync projectShow in Project Structure dialogAffected Modules: appI clicked on Add Google Maven repository and sync project. This opened a tab Refactoring preview and pointed to a file build. gradle. Here are two great resources:  Could not find com. android. tools. build:gradle:3. 0. 0 Why does the Google maven repository exist and when should I use it?As explained in the above resources, there are many repositories when building an Android app:  jcenter() google() The Android repository installed by the SDK Manager Other repositoriesjcenter() is used for open source libraries, like the Android plugin for gradle. google() is used for support libraries. More about this one at Google’s Maven Repository. There are gradle libraries listed inside com. android. tools. build. One of the resources also said that for Android Studio 3. 0 and higher, google() should already be added. Comparing with an app that compiled correctly: I read somewhere on StackOverflow that a good way for troubleshooting was looking at the differences between an app that compiled correctly with the one that didn’t. I checked a project I previously created and compiled correctly. This is what build. gradle looked like: build. gradle(Project: Test App)// Top-level build file where you can addconfiguration options common to all sub-projects/modules. buildscript {  repositories {    google()    jcenter()     }  dependencies {    classpath 'com. android. tools. build:gradle:3. 5. 0'        // NOTE: Do not place your application dependencies here; they belong    // in the individual module build. gradle files  }}allprojects {  repositories {    google()    jcenter()  }}task clean(type: Delete) {  delete rootProject. buildDir}Back to the project that was not compiling correctly. I opened the build. gradle file: buildscript {  repositories {    jcenter()  }  dependencies {    classpath 'com. android. tools. build:gradle:1. 5. 0'    // NOTE: Do not place your application dependencies here; they belong    // in the individual module build. gradle files  }}allprojects {  repositories {    jcenter()  }}I can clearly see that two things are different:  google() is missing.  Replaced this classpath 'com. android. tools. build:gradle:1. 5. 0' with 3. 5. 0. I added google() to both buildscript and allprojects. Then towards the top right on the blue warning Try Again. ERROR: Could not find method google() for arguments [] on repository container: Then I got this error: ERROR: Could not find method google() for arguments [] on repository container. Here is another resource:  gradle could not find methodIt has these instructions:  Go to the Android Project tab Gradle Scripts Open the file gradle-wrapper. propertiesIt had the following: distributionUrl=https\://services. gradle. org/distributions/gradle-2. 8-all. zipComparing the Android app that built correctly. The same file had this: distributionUrl=https\://services. gradle. org/distributions/gradle-5. 4. 1-all. zipSaved the file and clicked on Try Again Now it built the App without errors. Gradle Sync Issues: There are still a few warnings after compiling the app.  Configuration ‘compile’ is obsolete and has been replaced with ‘implementation’ and ‘api’ Configuration ‘testCompile’ is obsolete and has been replaced with ‘testImplementation’ The specified Android SDK Build Tools version (23. 0. 2) is ignored, as it is below the minimum supported version (28. 0. 3) for Android Gradle Plugin 3. 5. 0Reviewing the build. gradle(Module: app) files for the app that compiled correctly with the one that didn’t. The App that compiled correctly shows this: apply plugin: 'com. android. application'android {  compileSdkVersion 29  buildToolsVersion  29. 0. 2   defaultConfig {    applicationId  com. example. myapplicationforexample     minSdkVersion 23    targetSdkVersion 29    versionCode 1    versionName  1. 0     testInstrumentationRunner  androidx. test. runner. AndroidJUnitRunner   }  buildTypes {    release {      minifyEnabled false      proguardFiles getDefaultProguardFile('proguard-android-optimize. txt'), 'proguard-rules. pro'    }  }}dependencies {  implementation fileTree(dir: 'libs', include: ['*. jar'])  implementation 'androidx. appcompat:appcompat:1. 0. 2'  implementation 'androidx. constraintlayout:constraintlayout:1. 1. 3'  testImplementation 'junit:junit:4. 12'  androidTestImplementation 'androidx. test:runner:1. 1. 1'  androidTestImplementation 'androidx. test. espresso:espresso-core:3. 1. 1'}Compared to the App that didn’t compile apply plugin: 'com. android. application'android {  compileSdkVersion 22  buildToolsVersion  23. 0. 2   defaultConfig {    applicationId  mooc. vandy. java4android. shapes     minSdkVersion 22    targetSdkVersion 22    versionCode 1    versionName  1. 0     multiDexEnabled true  }  buildTypes {    release {      minifyEnabled false      proguardFiles getDefaultProguardFile('proguard-android. txt'), 'proguard-rules. pro'    }  }  compileOptions {    sourceCompatibility JavaVersion. VERSION_1_7    targetCompatibility JavaVersion. VERSION_1_7  }  testOptions {    unitTests. returnDefaultValues = true    unitTests. all{      ignoreFailures = true      maxHeapSize =  1024m     }  }  packagingOptions {    exclude 'META-INF/LICENSE. txt'    exclude 'META-INF/NOTICE. txt'  }}dependencies {  compile fileTree(dir: 'libs', include: ['*. jar'])  testCompile 'junit:junit:4. 12'  compile 'com. android. support:appcompat-v7:22. 2. 1'  compile 'com. android. support:design:22. 2. 1'  compile ( org. apache. commons:commons-lang3:3. 3. 2 )  compile ( com. google. guava:guava:17. 0 )}A few things are different. App that compiled: compileSdkVersion 29buildToolsVersion  29. 0. 2 minSdkVersion 23targetSdkVersion 29implementation fileTree(dir: 'libs', include: ['*. jar'])testImplementation 'junit:junit:4. 12'App that didn’t compile: compileSdkVersion 22buildToolsVersion  23. 0. 2 minSdkVersion 22targetSdkVersion 22compile fileTree(dir: 'libs', include: ['*. jar'])testCompile 'junit:junit:4. 12'compile 'com. android. support:appcompat-v7:22. 2. 1'compile 'com. android. support:design:22. 2. 1'// compile is deprecated use implementationI made the changes and compiled again. Configuration ‘compile’ is obsolete: Only one issue remains:  Configuration ‘compile’ is obsolete and has been replaced with ‘implementation’ and ‘api’For this section. I replaced compile with implementation and testCompile with testImplementation. dependencies {	  compile fileTree(dir: 'libs', include: ['*. jar'])	  testCompile 'junit:junit:4. 12'	  compile 'com. android. support:appcompat-v7:22. 2. 1'	  compile 'com. android. support:design:22. 2. 1'	  compile ( org. apache. commons:commons-lang3:3. 3. 2 )	  compile ( com. google. guava:guava:17. 0 )	}GradleCompatible error in build. gradle(Module: app): After making this change, yet another error showed up, underlined red on these two lines: compile 'com. android. support:appcompat-v7:22. 2. 1'compile 'com. android. support:design:22. 2. 1'The error says: Issue id: GradleCompatibleVersion 28 (intended for Android Pie and below) is the last version of the legacy support library, so we recommend that you migrate to AndroidX libraries when using Android Q and moving forward. The IDE can help with this: Refactor &gt; Migrate to AndroidX. Inspection info: There are some combinations of libraries, or tools and libraries, that are incompatible, or can lead to bugs. One such incompatibility is compiling with a version of the Android support libraries that is not the latest version (or in particular, a version lower that your targetSdkVersion)As seen here it says that AndroidX is a major improvement to the original Android Support Library, which is no longer maintained. And this one about migrating to AndroidX, which basically follows the same warning above to use Refactor &gt; Migrate to AndroidX. The Refactoring Preview shows References to be changed: Usage in import in MainActivity. java import android. support. v7. app. AppCompatActivity;Usage in Gradle build script in build. gradle(Module:app) implementation 'com. android. support:appcompat-v7:22. 2. 1'implementation 'com. android. support:design:22. 2. 1'Clicked on Do Refactor. It made these changes: In MainActivity. java: import androidx. appcompat. app. AppCompatActivity;In build. gradle(Module:app): implementation 'androidx. appcompat:appcompat:1. 0. 0'implementation 'com. google. android. material:material:1. 0. 0'No more errors. "
    }, {
    "id": 165,
    "url": "https://www.tomordonez.com/install-java-fedora-openjdk-android/",
    "title": "Install Java on Fedora with OpenJDK",
    "body": "2019/12/14 - How to install Java on Fedora with OpenJDK. Also how to setup Android and a review of JShell. The Python shell has been a great learning tool and I wanted a similar experience in Java. There is a module called JShell, however, it’s not available for the default Java 8 installed on my Linux. I had to at least upgrade to Java 9. Versions for Fedora, Java, Android Studio: Fedora Version:  Fedora 31Java Version: $ java -versionopenjdk version  1. 8. 0_232 OpenJDK Runtime Environment (build 1. 8. 0_232-b09)OpenJDK 64-Bit Server VM (build 25. 232-b09, mixed mode)Which Java bin: $ which java/usr/bin/javaAndroid version:  Android Studio 3. 5Android SDK Location:  Open Android Project Structure: Menu &gt; File &gt; Project Structure &gt; SDK Location Android SDK Location: /home/tom/Android/Sdk Android NDK Location: empty JDK Location: /usr/local/android-studio/jreTesting Android JShell Console:  Menu &gt; Tools &gt; JShell console Entered: out. println( Hello world ); Hit Run An error shows: JDK version is 8. JDK 9 or higher is needed to run JShell. Installation Summary:  Download OpenJDK 13 from here using wget… Following this tutorial here Change the Android JDK location as seen here Test JShell on Android Studio and the command line as seen hereWhat could go wrong…right? Installing OpenJDK 13 on Fedora: Download OpenJDK 13 from here and following this tutorial. $ cd /opt/$ sudo wget https://download. java. net/java/GA/jdk13. 0. 1/cec27d702aa74d5a8630c65ae61e4305/9/GPL/openjdk-13. 0. 1_linux-x64_bin. tar. gz$ sudo tar -xvf openjdk-13. 0. 1_linux-x64_bin. tar. gzCheck the Java version $ /opt/jdk-13. 0. 1/bin/java -versionopenjdk version  13. 0. 1  2019-10-15OpenJDK Runtime Environment (build 13. 0. 1+9)OpenJDK 64-Bit Server VM (build 13. 0. 1+9, mixed mode, sharing)Setup environment by editing your . bashrc. Add these to the end of the file. # openjdk version  13. 0. 1 export JAVA_HOME=/opt/jdk-13. 0. 1/export PATH=/opt/jdk-13. 0. 1/bin:$PATHThen run $ source ~/. bashrc. Restart the terminal. I tested the Java version: $ java -versionopenjdk version  13. 0. 1  2019-10-15OpenJDK Runtime Environment (build 13. 0. 1+9)OpenJDK 64-Bit Server VM (build 13. 0. 1+9, mixed mode, sharing)$ which java/opt/jdk-13. 0. 1/bin/javaAlternative installation procedure: I tried the official Fedora docs as seen here $ sudo dnf search openjdk$ sudo dnf install java-latest-openjdk. x86_64$ sudo alternatives --config javaSelect option: 2$ export JAVA_HOME=/usr/lib/jvm/java-13-openjdk-13. 0. 1. 9-2. rolling. fc31. x86_64$ export PATH=$JAVA_HOME/bin:$PATH(Learned about the alternatives command here) However after multiple attempts I saw that the jshell module was missing and couldn’t figure out how to install it. It was supposed to come with the JDK already. Troubleshooting your PATH: I struggled with this for hours, trying to edit the PATH variable. I previously added the JDK 13 path to the end causing trouble. The system would check first the default Java installed in /usr/bin because this path was added before. If you wish to edit your PATH variable. You can use this code: PATH=$(echo  $PATH  | sed -e 's/\/usr\/bin:\/opt\/jdk-13. 0. 1\/bin$/\/opt\/jdk-13. 0. 1\/bin:\/usr\/bin/')Following this syntax while escaping the forward slashes for the directories: PATH=$(echo  $PATH  | sed -e 's/replace_this/with_this/')Change the Android JDK Location: Here is where I learned the hard way that I couldn’t change the version of JDK in Android Studio. Open Android Project Structure: Menu &gt; File &gt; Project Structure &gt; SDK Location  Current JDK Location: /usr/local/android-studio/jreI tried to open this location: /opt/jdk-13. 0. 1/However, I got this error: Please choose a valid JDK 8 directoryMore details here. For now I just wanted to test JShell. Test JShell on Android Studio: In Android Studio: Tools &gt; JShell Console. An error says JShell: JDK version is 8. JDK 9 or higher is needed to run JShell. I don’t see how JShell can be used in Android Studio if it won’t allow to choose a JDK other than version 8. Test JShell on the Command Line: The Command Line is what I’ve used for the Python shell. I am happy enough to use the CLI for this Java shell. $ jshell| Welcome to JShell -- Version 13. 0. 1| For an introduction type: /help introjshell&gt;I like it already. Exit with: jshell&gt; /exitThe next few lines follow this tutorial. Start jshell in verbose mode to provide feedback about what you typed. $ jshell -vjshell&gt; int x = 45;x ==&gt; 45| created variable x : intjshell&gt; String twice(String s) {  . . . &gt;   return s + s;  . . . &gt; }| created method twice(String)jshell&gt; twice( Ocean );$3 ==&gt;  OceanOcean | created scratch variable $3 : Stringjshell&gt; String x;x ==&gt; null| replaced variable x : String|  update overwrote variable x : intUse tab to autocomplete or show available methods: jshell&gt; System. cclass clearProperty( console() currentTimeMillis()Use tab before entering a method’s parameter: jshell&gt; twice($3    twice(  x    Signatures:String twice(String s)&lt;press tab again to see documentation&gt;String twice(String s)&lt;no documentation found&gt;&lt;press tab again to see all possible completions;total possible completions: 542&gt;Use vim as an editor within JShell: jshell&gt; /set editor vim| Editor set to: vimjshell&gt; /editIt opens vim, which has everything I typed so far. I added a method, then saved, and closed the file: | created method cube(double)jshell&gt; cube(2);$6 ==&gt; 8. 0| created scratch variable $6 : double"
    }, {
    "id": 166,
    "url": "https://www.tomordonez.com/build-android-app-linux-flutter/",
    "title": "Build an Android App in Linux with Flutter",
    "body": "2019/09/17 - I have been thinking of buildind an app:  An app to help me study Machine Learning.  An app related to mobile payments. I’ve been researching the evolution of online payments in Latin America.  A stock market or finance advisor.  An app about motivation.  An app about research in recruiting. I learned of the Flutter framework last year. Even though I could refresh my knowledge of Java and build a plain-old Java Android app. I wanted to try something new. This is a tutorial to setup the environment to build an Android app in Linux using the Flutter framework. Setting up an environment is usually what causes the most frustration and where most people drop out from moving forward. I added the output of every command for reference. Tools:  Linux Fedora 30 Nexus 5 phoneInstall Flutter: As seen here:https://flutter. dev/docs/get-started/install/linux Download the Flutter SDK tar file (490M). I created a directory called droidev. $ cd ~/Documents/ &amp;&amp; mkdir droidev &amp;&amp; cd droidevTar the file $ tar xf ~/Downloads/flutter. . . tar. xzIt extracts a directory called flutter. Update the Path $ vim ~/. bashrcAdd the Path: export PATH= $PATH:$HOME/Documents/droidev/flutter/bin Verify: $ source ~/. bashrc$ echo $PATH$ which flutterPredownload binaries. I did this but it didn’t seem to do anything: $ flutter precacheDisable Google Analytics crash reports: $ flutter config --no-analyticsAnalytics reporting disabled. Check if there are dependencies missing: $ flutter doctorOutput was: Doctor summary (to see all details, run flutter doctor -v):[✓] Flutter (Channel stable, v1. 9. 1+hotfix. 2, on Linux, locale en_US. UTF-8)[✗] Android toolchain - develop for Android devices  ✗ Unable to locate Android SDK.  Install Android Studio from: https://developer. android. com/studio/index. html On first launch it will assist you in installing the Android SDK components.  (or visit https://flutter. dev/setup/#android-setup for detailed instructions).  If the Android SDK has been installed to a custom location, set ANDROID_HOME to that location.  You may also want to add it to your PATH environment variable. [!] Android Studio (not installed)[!] Connected device  ! No devices available! Doctor found issues in 3 categories. Install Android Studio on Linux: As seen here:https://developer. android. com/studio/install If running on 64-bit: $ sudo dnf install zlib. i686 ncurses-libs. i686 bzip2-libs. i686Output: Installing: zlib  i686 1. 2. 11-18. fc30  updates 94 k bzip2-libs i686 1. 0. 6-29. fc30 fedora 38 k ncurses-libs i686 6. 1-10. 20180923. fc30 fedora 308 kUpgrading: glibc  x86_64 2. 29-22. fc30 updates 4. 0 M glibc-all-langpacks x86_64 2. 29-22. fc30 updates 25 M glibc-common x86_64 2. 29-22. fc30 updates 838 k glibc-devel  x86_64 2. 29-22. fc30 updates 1. 0 M glibc-headers x86_64 2. 29-22. fc30 updates 473 k glibc-langpack-en x86_64 2. 29-22. fc30 updates 818 k libgcc x86_64 9. 2. 1-1. fc30 updates 91 kInstalling dependencies: glibc  i686 2. 29-22. fc30 updates 3. 9 M libgcc i686 9. 2. 1-1. fc30 updates 98 kDownloaded the tar file. Unpack in Downloads and move to /usr/local $ sudo mv ~/Downloads/android-studio/ /usr/local/Run Installation script $ cd /usr/local/android-studio/bin/$ . /studio. shA popup window says Import Android Studio Settings From. I chose Do not import settings. For Data Sharing I put Don't send. It starts the Setup Wizard:  Standard Select UI theme: DarculaCurrent Settings output: Setup Type: StandardSDK Folder: /home/tom/Android/SdkTotal Download Size: 515 MBSDK Components to Download: Android Emulator: 205 MBAndroid SDK Build-Tools 29. 0. 2: 39. 7 MBAndroid SDK Platform 29: 74. 6 MBAndroid SDK Platform-Tools: 8. 33 MBAndroid SDK Tools: 147 MBSDK Patch Applier v4: 1. 74 MBSources for Android 29: 37. 6 MBEmulator Settings said: We have detected that your system can run the Android emulator in an accelerated performance mode. . . Linux-based systems support virtual machine acceleration through the KVM software package. Configure Hardware Acceleration: As seen here:https://bytefreaks. net/android/fedora-configure-hardware-acceleration-for-the-android-emulator $ egrep '^flags. *(vmx|svm)' /proc/cpuinfo;$ sudo dnf group install --with-optional virtualization;The output is: Installing group/module packages: guestfs-browser x86_64 0. 2. 3-15. fc30 fedora  1. 6 M libguestfs-tools  noarch 1:1. 40. 2-4. fc30 fedora  33 k python3-libguestfs x86_64 1:1. 40. 2-4. fc30 fedora  211 k virt-install noarch 2. 1. 0-2. fc30  fedora  64 k virt-manager noarch 2. 1. 0-2. fc30  fedora  582 k virt-top  x86_64 1. 0. 8-34. fc30 fedora  717 k virt-viewer  x86_64 8. 0-1. fc30 fedora  390 kInstalling dependencies: gnutls-dane  x86_64 3. 6. 8-1. fc30  updates  25 k gnutls-utils x86_64 3. 6. 8-1. fc30  updates 299 k libvirt-bash-completion x86_64 5. 1. 0-9. fc30  updates  12 k libvirt-client  x86_64 5. 1. 0-9. fc30  updates 310 k python3-libvirt x86_64 5. 1. 0-2. fc30  updates 288 k hexedit x86_64 1. 2. 13-15. fc30  fedora  39 k hivex  x86_64 1. 3. 18-4. fc30 fedora  98 k libguestfs x86_64 1:1. 40. 2-4. fc30 fedora  2. 7 M libguestfs-tools-c x86_64 1:1. 40. 2-4. fc30 fedora  5. 5 M libldm  x86_64 0. 2. 4-4. fc30  fedora  53 k lsscsi  x86_64 0. 30-2. fc30  fedora  63 k ocaml-camomile-data  x86_64 0. 8. 7-5. fc30  fedora  2. 1 M perl-Sys-Guestfs  x86_64 1:1. 40. 2-4. fc30 fedora  316 k perl-Sys-Virt  x86_64 5. 1. 0-1. fc30  fedora  289 k perl-hivex x86_64 1. 3. 18-4. fc30 fedora  50 k python3-argcomplete  noarch 1. 9. 5-1. fc30  fedora  57 k scrub  x86_64 2. 5. 2-14. fc30 fedora  42 k supermin  x86_64 5. 1. 20-3. fc30 fedora  446 k virt-manager-common  noarch 2. 1. 0-2. fc30  fedora  1. 1 M zerofree  x86_64 1. 1. 1-3. fc30  fedora  27 kInstalling weak dependencies: libguestfs-xfs  x86_64 1:1. 40. 2-4. fc30 fedora  15 kInstalling Groups: VirtualizationStart the service and verify: $ sudo systemctl start libvirtd;$ sudo systemctl enable libvirtd;$ lsmod | grep kvmOutput is: kvm_intel 303104 0kvm 741376 1 kvm_intelirqbypass 16384 1 kvmAfter all this for some reason my switch-windows shortcut alt+tab stopped working. So I ran this: $ killall nautilusSetup an Android device: Following the docs:https://flutter. dev/docs/get-started/install/linux I had this enabled already on this phone:  Developer options USB debuggingThen connected the phone and entered: $ flutter devicesThat didn’t work. I got this: No devices detected. Run 'flutter emulators' to list and startany available device emulators. Or, if you expected your device to be detected, please run  flutter doctor  to diagnose potential issues, or visit https://flutter. dev/setup/ for troubleshooting tips. I ran this again: $ flutter doctorAnd got this: Doctor summary (to see all details, run flutter doctor -v):[✓] Flutter (Channel stable, v1. 9. 1+hotfix. 2, on Linux, locale en_US. UTF-8) [!] Android toolchain - develop for Android devices (Android SDK version 29. 0. 2)  ✗ Android licenses not accepted.  To resolve this, run: flutter doctor --android-licenses[!] Android Studio (version 3. 5)  ✗ Flutter plugin not installed; this adds Flutter specific functionality.   ✗ Dart plugin not installed; this adds Dart specific functionality. [!] Connected device  ! No devices available! Doctor found issues in 3 categories. Entered the suggested command: $ flutter doctor --android-licensesGot this and approved the 5 licenses: Warning: File /home/tom/. android/repositories. cfg could not be loaded.      5 of 5 SDK package licenses not accepted. 100% Computing updates. . .       Review licenses that have not been accepted (y/N)?Troubleshooting No devices available: Again: $ flutter doctorOutput: [✓] Flutter (Channel stable, v1. 9. 1+hotfix. 2, on Linux, locale en_US. UTF-8)[✓] Android toolchain - develop for Android devices (Android SDK version 29. 0. 2)[!] Android Studio (version 3. 5)  ✗ Flutter plugin not installed; this adds Flutter specific functionality.   ✗ Dart plugin not installed; this adds Dart specific functionality. [!] Connected device  ! No devices availableInstalled libmtp. As seen here: https://wiki. archlinux. org/index. php/Media_Transfer_Protocolhttp://libmtp. sourceforge. net/ $ sudo dnf reinstall libmtp$ sudo dnf install libmtp-examplesSolution Ran flutter devices but no luck. After a few hours I saw this: https://askubuntu. com/questions/417323/my-mtp-capable-device-is-not-detected-what-can-i-do-about-that The main answer starts with making sure you have the right cable and not just the one for charging the device. I had the USB cable I used to charge the doorbell. It’s a small orange cable. I changed the cable to the one for the kindle. That was it. Run again: $ flutter devices1 connected device:Nexus 5 • android-arm • Android 6. 0. 1 (API 23)Run flutter doctor: Doctor summary (to see all details, run flutter doctor -v):[✓] Flutter (Channel stable, v1. 9. 1+hotfix. 2, on Linux, locale en_US. UTF-8)[✓] Android toolchain - develop for Android devices (Android SDK version 29. 0. 2)[!] Android Studio (version 3. 5)  ✗ Flutter plugin not installed; this adds Flutter specific functionality.   ✗ Dart plugin not installed; this adds Dart specific functionality. [✓] Connected device (1 available)Setup the Android Emulator: As seen here:https://flutter. dev/docs/get-started/install/linux Enable VM acceleration. I did this already. See above Configure Hardware Acceleration Launch Android Studio:  Tools &gt; AVD Manager Create Virtual Device Select hardware: Nexus 5 (choose your own)Select system image. I selected Q: Android 10 and clicked on Download. A pop up window says Component Installer (1GB). Installing Requested ComponentsSDK Path: /home/tom/Android/SdkPackages to install: - Google Play Intel x86 Atom System Image (system-images;android-29;google_apis_playstore;x86)Preparing  Install Google Play Intel x86 Atom System Image (revision: 7) . Downloading https://dl. google. com/android/repository/sys-img/google_apis_playstore/x86-29_r07-linux. zip Install Google Play Intel x86 Atom System Image (revision: 7)  ready. Installing Google Play Intel x86 Atom System Image in /home/tom/Android/Sdk/system-images/android-29/google_apis_playstore/x86 Install Google Play Intel x86 Atom System Image (revision: 7)  complete.  Install Google Play Intel x86 Atom System Image (revision: 7)  finished. I also downloaded Marshmallow x86_64 Android 6. 0(Google APIs). I checked the phone and the Android version is 6. 0. 1 (600MB). Packages to install: - Google APIs Intel x86 Atom_64 System Image (system-images;android-23;google_apis;x86_64)Preparing  Install Google APIs Intel x86 Atom_64 System Image (revision: 31) . Downloading https://dl. google. com/android/repository/sys-img/google_apis/x86_64-23_r31. zipFor future reference, this is the doc about managing AVD (Android Virtual Device): https://developer. android. com/studio/run/managing-avds Back in the System Image window. Make sure that Q is selected. The right side says:  API level: 29 Android: 10. 0 Google Inc System image: x86 Next…A new window says Android Virtual Device:  AVD Name: Nexus 5 API 29 Nexus 5: 4. 951080x1920 xxhdpi Q: Android 10. 0 x86 Startup orientation: Portrait Emulated performance: Graphics &gt; Automated (The doc says Hardware -GLES 2. 0 but there is no option for this) Device Frame: Checked Enable Device Frame Finish…Run the Emulator: A new window says Your Virtual Devices. Click on the play button. You can also find this from the top menu bar Tools &gt; AVD Manager. A phone emulator opens. It takes some time to boot up. On the actual phone an application says My Application with a Hello world. I think this is a sample application I created when I first started Android Studio. Closed the emulator and AVD Manager. Install the Flutter and Dart Plugins: As seen here:https://flutter. dev/docs/get-started/editor Inside Android Studio:  File &gt; Settings &gt; Plugins Find Flutter and install 3rd party plugin data privacy: Accept, the other choice is Cancel The plugin you want to install requires Dart: Yes. Click on Restart IDE. One last check on the terminal: $ flutter doctorDoctor summary (to see all details, run flutter doctor -v):[✓] Flutter (Channel stable, v1. 9. 1+hotfix. 2, on Linux, locale en_US. UTF-8)[✓] Android toolchain - develop for Android devices (Android SDK version 29. 0. 2)[✓] Android Studio (version 3. 5)[✓] Connected device (1 available)• No issues found!Test Drive Flutter: As seen here:https://flutter. dev/docs/get-started/test-drive  Open Android Studio Start a New Flutter project Flutter application Project name: firstapp. You can only use lowercase and underscore Flutter SDK Path: This is where I unzipped flutter in ~/Documents/droidev/flutter.  Project location: ~/AndroidStudioProjectsFor the Create project offline option see this. It says In offline mode, it will need to have all dependencies already available in the pub cache to succeed. :https://github. com/flutter/flutter/issues/15199 The next window says Set the package name. There is an important note about selecting your domain name before releasing the app. The default says example. com, and the package name is renamed as com. example. firstapp. These options are checked for Platform channel language:  Include Kotlin support for Android code Include Swift support for iOS codeThe next thing I did was changing the font of the editor. It was microscopic. Run the app: Click on the play run button. Launching lib/main. dart on Nexus 5 in debug mode. . . Initializing gradle. . . Resolving dependencies. . . Running Gradle task 'assembleDebug'. . . Built build/app/outputs/apk/debug/app-debug. apk. Installing build/app/outputs/apk/app. apk. . . Syncing files to device Nexus 5. . . On the phone an app says Flutter Demo Home Page. Click on the Stop button when done. "
    }, {
    "id": 167,
    "url": "https://www.tomordonez.com/install-miniconda-linux/",
    "title": "Install Miniconda on Linux",
    "body": "2019/09/10 - I loved virtualenv and then I found Miniconda. With Miniconda you can create a Python environment with a specific Python version and/or specific library versions. Miniconda is a light version of Anaconda, a Python distribution from continuum. You can install Miniconda with an installer or use the command line.  Install Miniconda on Linux: Long story short:  Create the conda directory in your home folder: mkdir ~/. conda Use the script to install Miniconda.  Reopen the terminal and check conda was installed conda --versionSee complete details below. Conda Cheat Sheet:  Conda version: conda --version List of conda environments: conda info --envs or conda env list Activate a conda: conda activate name_of_env Switch to default env: conda activate Create env with a yml file: conda env create --file environment. ymlInstalling Miniconda Step by Step: I tested this installing Miniconda on Ubuntu and Fedora. If you already tried installing Miniconda and found issues, see troubleshooting at the end. To avoid issues before installing Miniconda. Create the . conda directory in your home folder: $ mkdir ~/. condaThen use the script to install Miniconda. Find your install file from Conda docs Minconda for Linux $ wget https://repo. anaconda. com/miniconda/Miniconda3-latest-Linux-x86_64. sh$ bash Miniconda3-latest-Linux-x86_64. sh  Click to see Output        Welcome to Miniconda3 4. 7. 10  In order to continue the installation process, please review the license agreement.   Please, press ENTER to continue  ===================================  Miniconda End User License Agreement  . . .   Cryptography Notice  This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to anothe  r country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use  , and re-export of encryption software, to see if this is permitted. See the Wassenaar Arrangement http://www. wassenaar. org/ for more information.   Anaconda, Inc. has self-classified this software as Export Commodity Control Number (ECCN) 5D992b, which includes mass market information security software using or performi  ng cryptographic functions with asymmetric algorithms. No license is required for export of this software to non-embargoed countries. In addition, the Intel(TM) Math Kernel  Library contained in Anaconda, Inc. 's software is classified by Intel(TM) as ECCN 5D992b with no license required for export to non-embargoed countries.   The following packages are included in this distribution that relate to cryptography:  `openssl`    The OpenSSL Project is a collaborative effort to develop a robust, commercial-grade, full-featured, and Open Source toolkit implementing the Transport Layer Security (TL  S) and Secure Sockets Layer (SSL) protocols as well as a full-strength general purpose cryptography library.   `pycrypto`    A collection of both secure hash functions (such as SHA256 and RIPEMD160), and various encryption algorithms (AES, DES, RSA, ElGamal, etc. ).   `pyopenssl`    A thin Python wrapper around (a subset of) the OpenSSL library.   `kerberos` (krb5, non-Windows platforms)    A network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography.   `cryptography`    A Python library which exposes cryptographic recipes and primitives.   Accept the license terms. Do you accept the license terms?[no] &gt;&gt;&gt; yesReview the default installation location. Miniconda3 will now be installed into a default location. Press `Enter` or change to your custom location:/home/tom/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location belowPress ENTER if you want the default location. [/home/tom/miniconda3] &gt;&gt;&gt;  Click to see Output        PREFIX=/home/tom/miniconda3  Unpacking payload . . .   Collecting package metadata (current_repodata. json): done  Solving environment: done  environment location: /home/tom/miniconda3  added / updated specs:  - _libgcc_mutex==0. 1=main  - asn1crypto==0. 24. 0=py37_0  - bzip2==1. 0. 8=h7b6447c_0  - ca-certificates==2019. 5. 15=0  - certifi==2019. 6. 16=py37_0  - cffi==1. 12. 3=py37h2e261b9_0  - chardet==3. 0. 4=py37_1  - conda-package-handling==1. 3. 11=py37_0  - conda==4. 7. 10=py37_0  - cryptography==2. 7=py37h1ba5d50_0  - idna==2. 8=py37_0  - idna==2. 8=py37_0  - libarchive==3. 3. 3=h5d8350f_5  - libedit==3. 1. 20181209=hc058e9b_0  - libffi==3. 2. 1=hd88cf55_4  - libgcc-ng==9. 1. 0=hdf63c60_0  - libstdcxx-ng==9. 1. 0=hdf63c60_0  - libxml2==2. 9. 9=hea5a465_1  - lz4-c==1. 8. 1. 2=h14c3975_0  - lzo==2. 10=h49e0be7_2  - ncurses==6. 1=he6710b0_1  - openssl==1. 1. 1c=h7b6447c_1  - pip==19. 1. 1=py37_0  - pycosat==0. 6. 3=py37h14c3975_0  - pycparser==2. 19=py37_0  - pyopenssl==19. 0. 0=py37_0  - pysocks==1. 7. 0=py37_0  - python-libarchive-c==2. 8=py37_11  - python==3. 7. 3=h0371630_0  - readline==7. 0=h7b6447c_5  - requests==2. 22. 0=py37_0  - ruamel_yaml==0. 15. 46=py37h14c3975_0  - setuptools==41. 0. 1=py37_0  - six==1. 12. 0=py37_0  - sqlite==3. 29. 0=h7b6447c_0  - tk==8. 6. 8=hbc83047_0  - tqdm==4. 32. 1=py_0  - urllib3==1. 24. 2=py37_0  - wheel==0. 33. 4=py37_0  - xz==5. 2. 4=h14c3975_4  - yaml==0. 1. 7=had09818_2  - zlib==1. 2. 11=h7b6447c_3  - zstd==1. 3. 7=h0b5b093_0  The following NEW packages will be INSTALLED:  _libgcc_mutex   pkgs/main/linux-64::_libgcc_mutex-0. 1-main  asn1crypto     pkgs/main/linux-64::asn1crypto-0. 24. 0-py37_0  bzip2       pkgs/main/linux-64::bzip2-1. 0. 8-h7b6447c_0  ca-certificates  pkgs/main/linux-64::ca-certificates-2019. 5. 15-0  certifi      pkgs/main/linux-64::certifi-2019. 6. 16-py37_0  cffi        pkgs/main/linux-64::cffi-1. 12. 3-py37h2e261b9_0  chardet      pkgs/main/linux-64::chardet-3. 0. 4-py37_1  conda       pkgs/main/linux-64::conda-4. 7. 10-py37_0  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1. 3. 11-py37_0  cryptography    pkgs/main/linux-64::cryptography-2. 7-py37h1ba5d50_0  idna        pkgs/main/linux-64::idna-2. 8-py37_0  libarchive     pkgs/main/linux-64::libarchive-3. 3. 3-h5d8350f_5  libedit      pkgs/main/linux-64::libedit-3. 1. 20181209-hc058e9b_0  libffi       pkgs/main/linux-64::libffi-3. 2. 1-hd88cf55_4  libgcc-ng     pkgs/main/linux-64::libgcc-ng-9. 1. 0-hdf63c60_0  libstdcxx-ng    pkgs/main/linux-64::libstdcxx-ng-9. 1. 0-hdf63c60_0  libxml2      pkgs/main/linux-64::libxml2-2. 9. 9-hea5a465_1  lz4-c       pkgs/main/linux-64::lz4-c-1. 8. 1. 2-h14c3975_0  lzo        pkgs/main/linux-64::lzo-2. 10-h49e0be7_2  ncurses      pkgs/main/linux-64::ncurses-6. 1-he6710b0_1  openssl      pkgs/main/linux-64::openssl-1. 1. 1c-h7b6447c_1  pip        pkgs/main/linux-64::pip-19. 1. 1-py37_0  pycosat      pkgs/main/linux-64::pycosat-0. 6. 3-py37h14c3975_0  pycparser     pkgs/main/linux-64::pycparser-2. 19-py37_0  pyopenssl     pkgs/main/linux-64::pyopenssl-19. 0. 0-py37_0  pysocks      pkgs/main/linux-64::pysocks-1. 7. 0-py37_0  python       pkgs/main/linux-64::python-3. 7. 3-h0371630_0  python-libarchive~ pkgs/main/linux-64::python-libarchive-c-2. 8-py37_11  readline      pkgs/main/linux-64::readline-7. 0-h7b6447c_5  requests      pkgs/main/linux-64::requests-2. 22. 0-py37_0  ruamel_yaml    pkgs/main/linux-64::ruamel_yaml-0. 15. 46-py37h14c3975_0  setuptools     pkgs/main/linux-64::setuptools-41. 0. 1-py37_0  six        pkgs/main/linux-64::six-1. 12. 0-py37_0  sqlite       pkgs/main/linux-64::sqlite-3. 29. 0-h7b6447c_0  tk         pkgs/main/linux-64::tk-8. 6. 8-hbc83047_0  tqdm        pkgs/main/noarch::tqdm-4. 32. 1-py_0  urllib3      pkgs/main/linux-64::urllib3-1. 24. 2-py37_0  wheel       pkgs/main/linux-64::wheel-0. 33. 4-py37_0  xz         pkgs/main/linux-64::xz-5. 2. 4-h14c3975_4  yaml        pkgs/main/linux-64::yaml-0. 1. 7-had09818_2  zlib        pkgs/main/linux-64::zlib-1. 2. 11-h7b6447c_3  zstd        pkgs/main/linux-64::zstd-1. 3. 7-h0b5b093_0  Preparing transaction: done  Executing transaction: done  installation finished.   Answer the question “initialize Miniconda3 by running conda init?” Do you wish the installer to initialize Miniconda3 by running conda init?[no] &gt;&gt;&gt; yes  Click to see Output        no change   /home/tom/miniconda3/condabin/conda  no change   /home/tom/miniconda3/bin/conda  no change   /home/tom/miniconda3/bin/conda-env  no change   /home/tom/miniconda3/bin/activate  no change   /home/tom/miniconda3/bin/deactivate  no change   /home/tom/miniconda3/etc/profile. d/conda. sh  no change   /home/tom/miniconda3/etc/fish/conf. d/conda. fish  no change   /home/tom/miniconda3/shell/condabin/Conda. psm1  no change   /home/tom/miniconda3/shell/condabin/conda-hook. ps1  no change   /home/tom/miniconda3/lib/python3. 7/site-packages/xontrib/conda. xsh  no change   /home/tom/miniconda3/etc/profile. d/conda. csh  modified   /home/tom/. bashrc  Final Output: ==&gt; For changes to take effect, close and re-open your current shell. &lt;==If you'd prefer that conda's base environment not be activated on startup, set the `auto_activate_base` parameter to false:conda config --set auto_activate_base falseThank you for installing Miniconda3!Reopen Terminal: Then try: $ conda --versionconda 4. 7. 10After reopening the Terminal, the prompt changes to this: (base) $This shows that your current conda environment is base. If you don’t wish to see this in your Terminal. For instance, if you write in other languages. You can disable this: conda config --set auto_activate_base falseUpdate conda: Use this: $ conda update condaOutput: Collecting package metadata (current_repodata. json): doneSolving environment: doneenvironment location: /home/tom/miniconda3added / updated specs:- condaThe following packages will be downloaded: package          |      build---------------------------|-----------------ca-certificates-2019. 5. 15 |        1     134 KBcertifi-2019. 6. 16     |      py37_1     156 KBchardet-3. 0. 4       |    py37_1003     173 KBconda-4. 7. 11        |      py37_0     3. 0 MBpip-19. 2. 2         |      py37_0     1. 9 MBpython-libarchive-c-2. 8  |     py37_13     23 KB------------------------------------------------------------                    Total:     5. 4 MBThe following packages will be UPDATED: ca-certificates  2019. 5. 15-0 --&gt; 2019. 5. 15-1certifi      2019. 6. 16-py37_0 --&gt; 2019. 6. 16-py37_1chardet      3. 0. 4-py37_1 --&gt; 3. 0. 4-py37_1003conda       4. 7. 10-py37_0 --&gt; 4. 7. 11-py37_0pip        19. 1. 1-py37_0 --&gt; 19. 2. 2-py37_0python-libarchive~ 2. 8-py37_11 --&gt; 2. 8-py37_13Create a conda environment: As seen on the official Conda docs. You could try this: $ conda create --name snowflakes pandasIt will show an output with these headers:  The following packages will be downloaded The following NEW packages will be INSTALLEDActivate the conda environment: $ conda activate snowflakesGo back to base: $ conda activateCreate a conda environment with a yml file: This is a yml file for a class I was taking: name: ml4tdependencies:- python=3. 6- cycler=0. 10. 0- kiwisolver=1. 1. 0- matplotlib=3. 0. 3- numpy=1. 16. 3- pandas=0. 24. 2- pyparsing=2. 4. 0- python-dateutil=2. 8. 0- pytz=2019. 1- scipy=1. 2. 1- seaborn=0. 9. 0- six=1. 12. 0- joblib=0. 13. 2- pytest=5. 0- future=0. 17. 1- pip- pip: - pprofile==2. 0. 2 - jsons==0. 8. 8Save the file as environment. yml. Run like this: $ conda env create --file environment. ymlThen activate: $ conda activate ml4tCheck where python is located and the version to confirm that this is set correctly: $ which python$ python --versionTo deactivate and go back to base: $ conda activateconda env list: Or use conda info --envs to list your conda environments. conda help: Run this: $ conda helpOutput: usage: conda [-h] [-V] command . . . conda is a tool for managing and deploying applications, environments and packages.   Click to see Output        Options:  positional arguments:   command    clean    Remove unused packages and caches.     config    Modify configuration values in . condarc. This is modeled           after the git config command. Writes to the user . condarc           file (/home/tom/. condarc) by default.     create    Create a new conda environment from a list of specified           packages.     help     Displays a list of available conda commands and their help           strings.     info     Display information about current conda install.     init     Initialize conda for shell interaction. [Experimental]    install   Installs a list of packages into a specified conda           environment.     list     List linked packages in a conda environment.     package   Low-level conda package utility. (EXPERIMENTAL)    remove    Remove a list of packages from a specified conda environment.     uninstall  Alias for conda remove.     run     Run an executable in a conda environment. [Experimental]    search    Search for packages and display associated information. The           input is a MatchSpec, a query language for conda packages.           See examples below.     update    Updates conda packages to the latest compatible version.     upgrade   Alias for conda update.   optional arguments:   -h, --help   Show this help message and exit.    -V, --version Show the conda version number and exit.   conda commands available from other packages:   env  Troubleshooting installation: Executing transaction: WARNING conda. core. envs_manager:register_env(46): Unable to register environment.  Path not writable or missing. environment location: /home/tom/miniconda3registry file: /home/tom/. conda/environments. txtAs seen here If you see this error try this:  Remove your miniconda folder such as: rm -rf miniconda3 Remove what the install added to your bashrc Create the . conda directory.  ReinstallMiniconda on Windows: Here are some resources to install Miniconda on Windows:  Conda Docs installing Miniconda on Windows Setup Python on Windows with Miniconda "
    }, {
    "id": 168,
    "url": "https://www.tomordonez.com/remote-access-linux-fedora/",
    "title": "Remote Access Linux Fedora Desktop in LAN",
    "body": "2019/04/24 - A mini tutorial about: remote access in Linux. This works if you have another computer in your LAN running a Linux Fedora Desktop. Tested on Fedora 27. 1. Enable Remote Access in the host: Make sure that vino is installed, as seen here: rpm -q vinoIf it says it’s not installed then install with: sudo dnf install vinoGo to Settings &gt; Sharing. Turn Sharing to ON. Then Remote Login to ON. It will show a message to use ssh something. local to connect to this host. Where something might be the host name. 2. Test Remote from a guest: From another computer’s terminal. Type the command that was shown on the host. ssh something. localIt will ask to accept the signature and to login. The prompt will change to your prompt $. You could add \h to your bashrc config so you don’t get confused. Then use exit to go back to your local terminal. 3. SSH to IP address: I found that connecting to something. local for the first time it worked with no problems. Further times it wouldn’t connect. I made sure the remote computer wouldn’t go into suspend mode. That didn’t solve the problem. Using the remote IP address works every time. Find the remote IP and connect using SSH. ssh remote-IP-address"
    }, {
    "id": 169,
    "url": "https://www.tomordonez.com/learning-github/",
    "title": "Learning Github",
    "body": "2019/04/21 - Updated Jan 11, 2020 This is a setup to practice git and Github. It requires the following:  Two Github accounts.  Two computers in the same network or a computer and a remote server.  A static website. For a while I have been thinking of an interactive process to learn and improve skills in git and Github. Static Website Platform: For this Git/Github learning lab I am using a static website platform. Here is a tutorial to install a static website with Python. The static website uses Github pages. The goal is to have an interaction with two github accounts. Use a development environment to write the blog posts and create pull requests. Then a server environment to approve requests and update the website. Two Github Accounts: For this setup I have two computers in the same network. Server  I use it as a server to approve requests and update the website.  Setup with my Main Github account.  It has the static website platform installed. Development  I use it as development to write the blog posts and create pull requests.  Setup with my 2nd Github account.  Forked repo from Main Github account. Writing blog posts in Development: I created a new branch called newposts.  Write/update the blog posts Push to forked repo on the 2nd Github account. First, check status: $ git statusIf new changes were made on the server, then sync the forked repo. As seen on installing a static website with Python. Follow the section “Fork and sync a local with a remote repo. ” $ git fetch upstream$ git checkout master$ git merge upstream/masterAs seen here. To merge master with newposts: $ git checkout newposts$ git merge masterResolve conflicts then commit: $ git commit -m  merged master Then add and push the commits $ git add . $ git commit -m  new blog post draft $ git push -u origin newpostsError on Git push: You might get this error if there are conflicts ! [rejected]    newposts -&gt; newposts (fetch first)error: failed to push some refs to 'https://github. com/. . . hint: Updates were rejected because the remote contains work that you dohint: not have locally. This is usually caused by another repository pushinghint: to the same ref. You may want to first integrate the remote changeshint: (e. g. , 'git pull . . . ') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details. Create a pull request (2nd Github account): Login to the 2nd Github account:  Navigate to the original repo on Main Github account.  Create a pull request. Using this format with no comments added: [newpost] name of the blog postMerge pull request (Main Github account): Login to the Main Github account:  Go to the repo.  Review if a pull request was sent.  If there are no conflicts, merge the commit into master. SSH to from Development to Server: This is how I remote access Linux. From my Local computer, I SSH into my LAN server using the server’s IP address. $ ssh server-IPI attach to my Server tmux environment: (server)$ tmuxinator tomUsing already a Local tmux, inside Server tmux I have to use the prefix twice Ctrl+a Ctrl+a before any binding. Then I pull the changes and publish the blog. (server)$ git pull(server)$ make html &amp;&amp; make publishPublishing the blog in Server: Once I generate the static pages, I just follow my usual process. Push the HTML output. (server)$ cd output(server)$ git add . (server)$ git commit -m  new post (server)$ git push -u origin masterThen push the source. (server)$ cd . . (server)$ git add . (server)$ git commit -m  new post (server)$ git push -u origin masterExit server: When the website is published into Github pages. Detach from tmux Ctrl+a Ctrl+a + D. Then exit: (server)$ exit$"
    }, {
    "id": 170,
    "url": "https://www.tomordonez.com/my-vimrc-config/",
    "title": "My vimrc config file",
    "body": "2019/04/20 - Relevant articles:  Install a plugin in Vim Vim stuck in insert mode Go to the previous directory in VimA short description of my vimrc config file. Edit with: vim ~/. vimrcI want to see the line number always: set numberShow the line number, the column number, and the relative position of the cursor in the file, as a percentage. More details here. set rulerShow the current static always. set laststatus=2I like writing blog posts with lines wrapped at 80 characters without creating a new line. set wrapset columns=80set linebreakI also have a few Plugins. Read this tutorial to install a plugin in Vim.  Markdown with correct code highlight A better JSON for Vim Color schemesAnd my favorite: Autosave in Vim. More details: set numberset ruler  See the current static alwaysset laststatus=2  Wrap at 80 characters, don't create a new lineset wrapset columns=80set linebreak   breaks by word instead of characterset nocompatiblefiletype offset rtp+=~/. vim/bundle/Vundle. vimcall vundle#begin()  let Vundle manage Vundle, requiredPlugin 'VundleVim/Vundle. vim'  Plugins  To install plugins launch vim and :PluginInstall  To list plugins launch vim and :PluginListPlugin 'godlygeek/tabular'  Markdown with correct code highlight  and spell check disabled as seen on:  https://github. com/gabrielelana/vim-markdown/issues/49au FileType markdown setlocal nospellPlugin 'gabrielelana/vim-markdown'  A better JSON for Vim as seen on:  https://github. com/elzr/vim-jsonPlugin 'elzr/vim-json'  Color schemesPlugin 'tomasr/molokai'Plugin 'flazz/vim-colorschemes'  AutosavePlugin '907th/vim-auto-save'let g:auto_save = 1let g:auto_save_in_insert_mode = 0call vundle#end()filetype plugin indent on  To ignore plugin indent changes, instead use:  filetype plugin on"
    }, {
    "id": 171,
    "url": "https://www.tomordonez.com/remote-tmux-inside-local-tmux/",
    "title": "Remote Tmux Inside a Local Tmux",
    "body": "2019/04/19 - How to work with an SSH remote Tmux inside a local Tmux. To provide some context, these are relevant blog posts:  How to Install Tmux on Linux Remote Access a Linux Fedora In Your LAN Reload TmuxinatorI connect to another Linux laptop in my LAN using SSH. Then I use Tmux on that machine. As seen here. You can dettach from a remote tmux session, if you are already in a local tmux session. Inside your remote tmux SSH session. You have to use your prefix twice. My prefix is set to Ctrl+a, and I have my Ctrl key mapped to Caps. After connecting SSH and opening tmux on that machine. You can dettach by using: prefix prefix dIn my case I use: Ctrl+a Ctrl+a dThis will dettach my remote tmux. You can follow the same process to interact with your remote tmux SSH session. To move to the next tab I use Ctrl+a n. To move to the next vertical split in the same tab I use Ctrl+a l to move to the right, and Ctrl+a h to move to the left. For the remote tmux SSH session I do this: Ctrl+a Ctrl+a nCtrl+a Ctrl+a lCtrl+a Ctrl+a h"
    }, {
    "id": 172,
    "url": "https://www.tomordonez.com/make-static-website-python-github-pages/",
    "title": "Make a Static Website with Python and Github Pages",
    "body": "2019/04/18 - Updated Jan 11, 2020 Follow this tutorial to make a static website with Python and Github Pages. It uses the Pelican static website generator. Hosted with Github pages using Cloudflare DNS and CDN. I followed this procedure to setup the blog using Python and Pelican. https://fedoramagazine. org/make-github-pages-blog-with-pelican/ Create accounts: Make sure you have accounts at Github and Cloudflare. Creating repos on Github: You need to create 2 repos for your source files and your output files. The source files can be written in Markdown. Then you use a command to convert these source files into output HTML files. For source files: youruser. github. io-srcFor output files: youruser. github. ioSetup virtualenv: pip install virtualenvTest your installation with: virtualenv --versionCreate a directory such as: mkdir blogcd blogCreate the environment with: virtualenv envActivate it with: source env/bin/activateThe prompt will change to: env $Follow the steps to install packages. If you want to leave the environment deactivate it with: deactivateInstall Pelican: Inside the environment, install Pelican: env $ pip install pelicanenv $ pip install Markdownenv $ pip install typogrifySetup the source repo env $ git clone https://github. com/youruser/youruser. github. io-src. git ghpagesenv $ cd ghpagesSetup the output repo env $ git submodule add https://github. com/youruser/youruser. github. io. git outputRun the Pelican setup: env $ pelican-quickstartCaching your Github user and password: Later on when we are ready to publish to Github. When you git push. It will ask for your Github user and password. Everytime that you push it will ask for your credentials. Which can be annoying if you want to push a few times within a short period. You can cache your Github credentials using the following setup. Inside the Pelican root directory make sure you are on the right repo: env $ git remote -vThis should give you the . io-src. git repo. env $ git config --global credential. helper 'cache --timeout=3600'This will cache your credentials for 1 hour. Now do the same for the output repo. env $ cd outputenv $ git config --global credential. helper 'cache --timeout=3600'env $ cd . . Pelican Setup: Answer the questions like this:  Where to create the new site: Click Enter URL: https://youruser. github. io Generate Makefile: Yes Autoreload &amp; simpleHTTP: Yes Upload mechanism: Select No for all except for Github, select Yes.  Is this your personal page: YesAccording to this tutorial: https://fedoramagazine. org/make-github-pages-blog-with-pelican/If you get an error saying that the output directory already exists, that this is fine. Development and Production: There are config files. The development config file is called pelicanconf. py. The production config file is called publishconf. py. Update publishconf. py file: Open the file publishconf. py using your favorite editor, such as vim. env $ vim publishconf. pyAdd or edit this line so it’s set to False DELETE_OUTPUT_DIRECTORY = FalseUpdate pelicanconf. py file:: Open the file pelicanconf. py. env $ vim pelicanconf. pyThe default file usually looks like this: # -*- coding: utf-8 -*- #from __future__ import unicode_literalsAUTHOR = u'Your Name'SITENAME = u'Your Site Title'SITEURL = ''PATH = 'content'TIMEZONE = 'America/New_York'DEFAULT_LANG = u'en'# Feed generation is usually not desired when developingFEED_ALL_ATOM = NoneCATEGORY_FEED_ATOM = NoneTRANSLATION_FEED_ATOM = NoneAUTHOR_FEED_ATOM = NoneAUTHOR_FEED_RSS = None# BlogrollLINKS = (('Pelican', 'http://getpelican. com/'),     ('Python. org', 'http://python. org/'),     ('Jinja2', 'http://jinja. pocoo. org/'),     ('You can modify those links in your config file', '#'),)# Social widgetSOCIAL = (('You can add links in your config file', '#'),     ('Another social link', '#'),)# Uncomment following line if you want document-relative URLs when developing#RELATIVE_URLS = TrueCreate structure of the blog: By default Pelican is setup as a blog, where the home page shows a list of blog posts with their summary. To setup content of type Pages. Create a directory inside content such as: env $ mkdir content/pagesTo setup images that you can use for Posts or Pages. Create a directory inside content such as: env $ mkdir content/imagesIf you will use a custom domain. Create another directory like this: env $ mkdir content/extraTo setup pagination. Edit pelicanconf. py and add this line: DEFAULT_PAGINATION = 5To create the path to images and to CNAME add these lines to pelicanconf. py: STATIC_PATHS = ['images', 'extra/CNAME']EXTRA_PATH_METADATA = {'extra/CNAME': {'path': 'CNAME'},}Create a CNAME file: If you have a custom domain, you need to create a CNAME file env $ vim content/extra/CNAMEAdd your domain such as: www. yourdomain. comCreate a blog post: To create a workflow from draft to published you need to edit pelicanconf. py: DEFAULT_METADATA = { 'status': 'draft', }Create the blog posts inside content using markdown such as: env $ vim content/awesome-blog-post. mdThen use this format: Title: Awesome Blog Post TitleDate: 2017-01-01 15:00Category: Awesome categoryTags: tag1, tag2, tag3Slug: awesome-blog-postAuthor: Homer SimpsonStatus: draftSummary: Summary of this awesome blog post. This will be the SEO descriptionStart your awesome blog post here. Blablablablaetc. . . Build the html and run the server: Save the blog post and run this: env $ make html &amp;&amp; make serveOpen the browser in localhost:8000. Since you set the status of the blog post to draft. You won’t see it on the home page. You would have to open it at the slug location such as: localhost:8000/awesome-blog-postTo stop the server use Ctrl+C. The draft to published workflow works well if you want others to review your content without having these posts appear on the home page. Otherwise just set the Status to published. Create a blog post from a template: It’s pretty hard to remember the header for each blog post. I created a markdown template called new_post_template. md in my working directory and added this to . gitignore. Then I create a new blog post by copying this template such as: $ cp new_post_template. md content/name_of_new_blogpost. mdThe template has this content: Title: Add TitleDate: 2019-04-18 20:00Category: LinuxTags: tag1, tag2Slug: title-seo-with-dashesAuthor: Tom OrdonezStatus: publishedSummary: Short SEO summary. Add content here. Create a page: You can create static pages such as the typical about page. This type of content has to go inside the content/pages directory. env $ vim content/pages/about. mdThen follow a similar markdown format and status. Title: AboutAuthor: Your nameStatus: publishedSummary: awesome summaryStart writing your awesome page here. Markdown crash course: Use this notation for hyperlinks: [square bracket for text](parenthesis for URL)If you want to open the hyperlink in a new tab/window. This cannot be done with markdown. Instead you have the follow the html format: &lt;a href= awesome-url  target= _blank &gt;Awesome text&lt;/a&gt;Use this notation to add images: ![exclamation point and bracket for text](/assets/images/image-file. jpg)On the notation above you literally have to put {static}. Then the location of the image, which can be jpg or png. Formatting code, use: 4 leading spaces or `backticks` for inline. For headlines use # hashtags. For an h2 headline use ## 2 hashtags. For bullet points use * stars for ul or numbers for ol. Adding a theme: To see the location of default themes: pelican-themes -lvThis might change depending on your setup but in my case I got this: /usr/lib/python2. 7/site-packages/pelican/themes/notmyidea/usr/lib/python2. 7/site-packages/pelican/themes/simpleTo get more themes go to this Pelican themes site. Go to the repo for the one you like. Git clone the repo. For example: env $ (cd ~/Downloads/ &amp;&amp; git clone https://awesome-repo)Let’s say that the theme is called homer. The location will be ~/Downloads/homer/. Install the theme using this: env $ pelican-themes --install ~/Downloads/homer/ --verboseWhat this will do is just copy the whole directory into your virtualenv: ~/. . . /env/lib/python2. 7/site-packages/pelican/themes/homerNow you have to add something to the pelicanconf. py config file: THEME = 'homer'Depending on the theme there might be a requirement to specify the css file. In which case you would need to add something like this: COLOR_SCHEME_CSS = 'awesome. css'Troubleshooting: gitmodules, multiple configurations found: You might get this error when pushing after making theme changes: gitmodules, multiple configurations found for 'submodule. themes/name-of-theme. path'. Skipping second one!Open your . gitmodules file. In my case it looked like this: [submodule  themes/medius ]path = themes/mediusurl = https://github. com/onur/medius. git[submodule  themes/pelican-alchemy ]path = themes/pelican-alchemyurl = https://github. com/nairobilug/pelican-alchemyMy current theme is pelican-alchemy. I removed the first three lines with the other theme references. Deploy to Github: Edit your . gitignore file and add this line: *. pycUse this to generate the site: env $ make publishThis will generate the site using the production settings. Which is the publishconf. py file. This will also run the development config file because this file has this line: from pelicanconf import *Deploying to Github requires to git push both repos. The source and the output. Deploy the output. env $ cd outputenv $ git add . env $ git commit -m  Describe your commit here env $ git push -u origin masterThis will ask for your Github credentials. They will be cached for 1 hour. Now deploy the source. env $ cd . . env $ git add . env $ git commit -m  Describe your commit here env $ git push -u origin masterBrowse to you blog: https://youruser. github. ioCustom Domain: This requires to make some changes to Github, your DNS provider and Cloudflare Change the DNS settings where you have your website registered. Create these records: Type  Name        ContentALIAS  yoursite. com    youruser. github. ioCNAME  www. yoursite. com  youruser. github. ioTXT   yoursite. com    youruser. github. ioChange your DNS name server to the Cloudflare ones: aria. ns. cloudflare. comjay. ns. cloudflare. comChange the Github settings: Go to your output repo github. io. Go to Settings. Under Custom domain enter: www. yoursite. com Save. Change the Cloudflare settings: Add A records as seen here: https://help. github. com/articles/setting-up-an-apex-domain/ Verify with Google webmaster tools.  Add property Add TXT google verification codeThanks to this awesome post. This is how you setup Cloudflare. Setup these Page rules. https://www. yoursite. com/*Cache Level: Cache Everythinghttps://yoursite. com/*Forwarding URL: (Status Code: 301 - Permanent Redirect, URl: https://www. yoursite. com$1)http://www. yoursite. com/*Always Use HTTPSIn your Overview dashboard make sure you have these settings (if you are on the free plan)  Security level: medium SSL: Full Caching level: StandardChange the Pelican production config file: Inside publishconf. py change this setting from: SITEURL = 'https://youruser. github. io'to: SITEURL = 'https://www. yoursite. com'Push Custom domain changes to Github: Deploy the output. env $ make html &amp;&amp; make publishenv $ cd outputenv $ git add . env $ git commit -m  Custom domain update env $ git push -u origin masterThis might ask for your Github credentials. Unless they were cached. Now deploy the source. env $ cd . . env $ git add . env $ git commit -m  Custom domain update env $ git push -u origin masterSetup SEO: robots. txt, favicon. ico, sitemap. xml: Modify your pelicanconf. py file so it looks like this: STATIC_PATHS = [    'images',    'extra/CNAME',    'extra/robots. txt',    'extra/favicon. ico',    ]EXTRA_PATH_METADATA = {    'extra/CNAME': {'path': 'CNAME'},    'extra/robots. txt': {'path': 'robots. txt'},    'extra/favicon. ico': {'path': 'favicon. ico'},    }### PluginsPLUGIN_PATHS = [    'pelican-plugins'     ]PLUGINS = [     'sitemap',      ]# SitemapSITEMAP = {      'format': 'xml',      'priorities': {        'articles': 0. 5,        'indexes': 0. 5,        'pages': 0. 5      },      'changefreqs': {        'articles': 'weekly',        'indexes': 'daily',        'pages': 'monthly'      }}Create the robots. txt: Go to your Pelican root directory and create the robots. txt file: env $ vim content/extra/robots. txtAdd this line to the file: User-agent: *Create the favicon. ico: Go to the directory content/extra/ and add a favicon. ico image. Create a directory for the plugins in the Pelican root directory: env $ mkdir pelican-pluginsCreate the sitemap: Create a directory for the sitemap plugin: env $ mkdir pelican-plugins/sitemapAs seen on the Sitemap page. Copy the 3 files from this repo into the pelican-plugins/sitemap directory. The directory should contain these files: Readme. rst__init__. pysitemap. pyGenerate the site: env $ make publishVerify that the sitemap. xml was generated inside the output directory. Git Push SEO changes to Github: Deploy the output. env $ cd outputenv $ git add . env $ git commit -m  sitemap, robots, favicon env $ git push -u origin masterThis might ask for your Github credentials. Unless they were cached. Now deploy the source. env $ cd . . env $ git add . env $ git commit -m  sitemap, robots, favicon env $ git push -u origin masterTroubleshooting: using {filename} instead of {static}: I recently had issues with my blog when publishing posts. After calling make html &amp;&amp; make publish. I started getting errors that I was using {filename} instead of {static}. My mistake. I had the incorrect syntax or perhaps just outdated in multiple posts, when linking to static images inside the content/images directory. WARNING: {filename} used for linking to staticcontentFollowing this solution to replace a word on multiple files, I ran this: $ cd path/to/content folder$ sed -i 's/filename/static/g' *. mdLike magic, it updated all the files with the correct syntax. I also had to run the same command inside content/pages for all . md files. Fork and sync a local with a remote repo: As seen on my learning Github blog post. I have a setup to create blog posts on one machine and deploy the changes from a local server. I do this to experiment and learn with a sort of Git/Github lab. The local server has installed the static web server in production, following all the steps as mentioned before. It’s linked to a sort of master Github account: Master repo: https://github. com/original_owner/original_repo. git The development machine has a forked repo from the master repo and it uses a different Github account. Forked repo: https://github. com/your_username/your_fork. git Configure a remote for a fork as seen here Review the remotes you have: $ git remove -v&gt; origin https://github. com/your_username/your_fork. git (fetch)&gt; origin https://github. com/your_username/your_fork. git (push)Create a remote upstream repo with the original $ git remote add upstream https://github. com/original_owner/original_repo. gitCheck that the new remote was created $ git remote -vSync your fork with the original upstream as seen here $ git fetch upstreamIt creates a new branch that stores master into upstream/master: &gt; * [new branch]   master   -&gt; upstream/masterCheckout the local master branch: $ git checkout masterMerge the changes from upstream/master into local master: $ git merge upstream/masterTroubleshooting: Merging is not possible because you have unmerged files: When running: $ git merge upstream/masterYou might get this error: error: Merging is not possible because you have unmerged files.                                                        hint: Fix them up in the work tree, and then use 'git add/rm &lt;file&gt;'                                                     hint: as appropriate to mark resolution and make a commit.                                                          fatal: Exiting because of an unresolved conflict. Check: $ git statusI had these: Unmerged paths: (use  git add &lt;file&gt;. . .   to mark resolution)    both modified:  . gitignoreUntracked files: (use  git add &lt;file&gt;. . .   to include in what will be committed)    . gitmodules_bk    new_post_template. mdI added these files and committed the changes from the git fetch upstream $ git add . $ git commit  merged upstream/master "
    }, {
    "id": 173,
    "url": "https://www.tomordonez.com/sublime-text-typewriter-auto-scroll/",
    "title": "Sublime Text Typewriter Autoscroll",
    "body": "2019/02/02 - How to set Sublime text typewriter mode and autoscroll in Sublime Text 3 Do you get that feeling that you are continually looking at the bottom of your screen? I use Sublime Text to type almost everything. Keep notes, documents, and code. Since I started studying human-computer interaction, I started noticing that I spend a lot of time focusing my attention towards the bottom of the screen, and spend a few seconds moving the mouse or the arrows to scroll up. A few seconds is not much time from the keyboard to the mouse. However, doing this action over and over again throughout the day becomes very noticeable. As seen here. This is a Sublime text typewriter mode plugin. What it does is autoscroll in Sublime text to the middle of the screen. Instead of keeping your attention to the bottom of the screen, it autoscrolls to the middle of the screen. Installing Sublime Text Typewriter mode: Go to Install Packages and search for Typewriter. As easy as that. There are two modes.  Typewriter scrolling, to auto scroll to the middle of the screen.  Typewriter typing, to remain at the end of the file. Keep in mind that in typewriter typing, it disables mouse clicks, including selecting text. Toggle between two modes by Ctrl+Shift+P and search Typewriter. Then toggle scrolling or typing. The interface is not switching but toggling. If you toggle scrolling and want to try typing, you have to untoggle scrolling. Then toggle typing. Changing scrolling offset: The middle of the screen is ambiguous depending on the size of your screen. For my cheap improvised stand-up desk, the default setting is ideal. However, for my two-monitor setup. The middle is more 2/3 towards the bottom. To change the scrolling offset. Edit the file: Typewriter Settings - User. Under the menu: Preferences &gt; Package Settings &gt; Typewriter. {	 typewriter_mode_scrolling_offset : 10}Where a positive number moves the cursor up, and a negative moves the cursor down. You can try playing with this number to see how it works. For more details about this plugin. Go here "
    }, {
    "id": 174,
    "url": "https://www.tomordonez.com/tableau-tutorial-visualization-co2-world-dataset/",
    "title": "Tableau Tutorial Visualization - CO2 World Dataset",
    "body": "2019/01/06 - A Tableau tutorial to build a visualization of emissions from a CO2 world dataset.  This Tableau tutorial is based on Tableau Public Overview Video 1 as seen here Using Tableau 10. Connect to File &gt; Excel: Drag the sheet CO2 Data Cleaned: Go to Worksheet. : Rename the sheet: CO2 Per Capita. Drag Dimensions &gt; Country Name to the middle of the canvas. The measure, longitude is added to columns. The measure, latitude is added to rows. A world map is created with dots labeling each country from the dataset.  Drag a Measure to the canvas: Drag Measure &gt; CO2 Per Capita to the middle of the canvas. Click on the USA dot. It shows this data:  Country Name: United States CO2 per capita (metric tons): 1,004On the popup click on View Data It has two tabs:  Summary Full dataThe summary shows longitude, latitude, and CO2 of 1,003. 78. By default, Tableau calculates the SUM of all data points for this country. Review Full Data:  52 rows From 1960 to 2011 CO2 per capita is on average 19 metric tons. Drag a Dimension to Filters: Drag Year to Filters.  On Filters &gt; Year &gt; Dropdown. Switch from Continuous to Discrete. A new popup shows this Dimension as checkboxes. Click on the Year dropdown again and select Show Filter. The filter now appears on the right margin. Add Color and Size: Drag the Measure CO2 per Capita to the Marks &gt; Color button. The filter appears on the right margin. Use the dropdown to Edit Colors. On the Palette switch from Automatic to Red Black Diverging and select Reversed. The most robust colors should be:  Qatar UAE LuxembourgSize was added as default when we previously dragged CO2 Per Capita. Filter by Year: Select the last year on the dataset: 2011. The brightest colors are:  Qatar: 44. 02 Kuwait: 28. 10 Trinidad and Tobago: 37. 14 Aruba: 23. 92 Brunei: 24. 39I wondered why. Here is what I found about Trinidad and Tobago:  Most industrialized economy in the English-speaking Caribbean.  Top Caribbean producer of oil and gas (40% of GDP, 80% of exports) Manufactures food products, beverages, cement. In 2011, Trinidad and Tobago produced twice as much CO2 than the US. Isn’t that insane? TandT has a population of 1. 3 million people, just the population of Dallas. They pollute more than these countries:  US and Canada combined Mexico, Central America and South America combined Six times China All Africa combined.  30 times India. CO2 Emissions over time: This dataset only goes up to 2011. Here is the visualization of CO2 emissions over time. Create a new sheet. Rename to CO2 Emissions Over Time. Drag CO2 Per Capita to Rows. Add Year to Columns. Drag Country Name to Marks &gt; Detail. Drag CO2 Per Capita to Marks &gt; Color. Edit the Color from Automatic to Red Black Diverging and select Reversed.  Here are some interesting things:  Qatar went from 3 to 99 metric tons in one year.  Trinidad and Tobago went from 22 (2005) to 35. 6 (2006). Here is another one that is interesting:  Brunei CO2 went from 4 mt (1969) to 63 (1970).  10 years later went down to 7 metric tons.  In 2006 it produced 11 metric tons.  In 2007 it went up to 24. Create a Dashboard: Go to the icon next to create a new worksheet. Drag both sheets from the left window to the canvas. On the right margin on the filters, right click on each window and click on Remove from Dashboard. From the bottom left. Go to Objects and drag a Text to add a title.  "
    }, {
    "id": 175,
    "url": "https://www.tomordonez.com/web-scraping-with-python/",
    "title": "Web Scraping with Python",
    "body": "2018/12/09 - This is a tutorial on web scraping with Python. Learn to scrape websites with Python and BeautifulSoup.   Setup web scraping with Python.  Web scraping target and expected result.  Setup logging in Python.  Setup BeautifulSoup and export to CSV.  Scrape one page.  Scrape multiple pages. To scrape or not to scrape: There are tools with a user interface that allow you to point to content on a page and they scrape everything for you. There is also the question of scraping or not. Can you easily copy/paste the content and modify it with Excel? Or with a text editor like Sublime Text? While some tools can scrape things for you, sometimes they are hard to customize, or you have to pay for additional features. This tutorial assumes that you want the freedom to scrape anything you want and customize the tool however you think is best for your needs. 1. Setup web scraping with Python. : For this tutorial on web scraping I am using Python 3. There are a lot of tutorials online on how to install Python 3. If you are new to Python. This tutorial might not be the best first step for you. Here are some good resources:  Official Python Docs Coursera Python for EverybodyI am also using a virtual environment with miniconda  Install MinicondaIf you are not familiar with Unix commands. Here are some resources:  Strongly recommend a book called “Unix for the Beginning Mage”. You can download it online.  You can practice using this interactive shell on the browser: https://www. learnshell. org/. I am using Linux Fedora. The output of the commands you see in this tutorial might be different than yours. Once you have Python3 and miniconda installed then you can setup a virtual environment like this: $ conda create --name your-projectActivate: $ conda activate your-projectGo back to base: $ conda activate2. Web scraping target and expected result. : If you made it to this point. It is downhill from here…that was a joke. It’s not :) Unless you have some experience in Python. Then this should be easy. What do you want to scrape? A page or multiple pages? What output do you want? For this tutorial we are going to scrape speakers from a conference and the output is a CSV file with data about these speakers.  Let’s open the first speaker in another tab to see what content is there:  Name of speaker Title Company BioScraping metadata: What is else is there besides what we can read on the page? Right click, Inspect. It has meta name= keywords . Do we need these? Maybe.  It has a meta name= description  with a shorter bio. Scraping web content: The name of the speaker is under this tag: &lt;h1 class= global-hero__title &gt;I see a problem. The title and company are on the same sentence, separated by a comma and it is under this tag: &lt;h2 class= alt &gt;Head of Studio, SoftBank Robotics&lt;/h2&gt;There is no separate tag between title name and company name. On the other hand, the speakers list has title name and company name on different tags. &lt;h4 class= speaker-title &gt;Head of Studio&lt;/h4&gt;&lt;h4 class= speaker-company &gt;SoftBank Robotics&lt;/h4&gt;Can I extract this content by just copy/pasting? It doesn’t seem so. The result is: nametitlecompanynametitlecompanyThere is no way of knowing how to break this into a CSV file but we could take the risk of following the pattern of creating a speaker row for every 3 lines. Although I don’t see how to do this without writing some code. We could also scrape the Speakers page but it doesn’t have the bios. It just has:  name title companyTo bio or not to bio: On the speakers page we have:  name title company speaker conference url no bio Title and company are on different html tags. On each speaker page we have:  name title company bio Title and company are on the same html tag, separated by a comma. Can we scrape the title and company from the same html tag? Then do some magic (regex) to separate the data? What if the title already has a comma? Such as:  Name: Homer Simpson Title: Nuclear Engineer, Research Company: ACME, IncAnd the html tag has this: &lt;h2 class= alt &gt;Nuclear Engineer, Research, ACME, Inc&lt;/h2&gt;. This would be impossible to extract if speakers have different variations of where the comma is placed to separate title and company. If we only scrape the speakers page then we will miss the bio. 3. Setup logging in Python. : Here is a Python logging tutorial to get all the details. We want to capture a log file when running the script. That way we can see what worked and what didn’t. You could use print statements but it will be hard to read the terminal output after running the script a few times. For this blog post we just have to add this code towards the top: import logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter(('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))fh. setFormatter(formatter)logger. addHandler(fh)Then logger can be used as this: Example 1: for url in urls:  logger. info('Reading URL: %s', url)Example 2: page_title = soup. find('title'). textlogger. info('page title captured: %s', page_title)Then we have to close the log file at the end of the script: for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)4. Setup BeautifulSoup and export to CSV. : For both files we need to add this code: import urllib. request, urllib. parse, urllib. errorfrom bs4 import BeautifulSoup as bsimport csvWhy use urllib and not requests? It works and I always used it. But you are free to use any other library. Followed by the logging setup import logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter(('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))fh. setFormatter(formatter)logger. addHandler(fh)Followed by closing the logging file: for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)In between the logging setup we need to add our code. Setup BeautifulSoup: Given a url: url = 'https://www. ces. tech/Conference/Speaker-Directory. aspx'read_url = urllib. request. urlopen(url). read()soup = bs(read_url, 'html. parser')Read more about BeautifulSoup here:  Python Lambda and BeautifulSoupExport data to a CSV file: The usual is to export the scraped data to a CSV file. with open('leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['column1', 'column2', 'column3'])Then after saving data into variables. Use writerow to save the data to the CSV file. 5. Scrape one page: So far we have this: import urllib. request, urllib. parse, urllib. errorfrom bs4 import BeautifulSoup as bsimport csvimport logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter(('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))fh. setFormatter(formatter)logger. addHandler(fh)with open('leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['column1', 'column2', 'column3'])for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)Let’s create two scripts and save this code to each:  one_page. py many_pages. pyScrape the page that has the list of speakers::  Name Title Company Speaker conference url No bioModify the name of the log file. From output. log to one_page_output. log. Add the url and create a BeautifulSoup object. url = 'https://www. ces. tech/Conference/Speaker-Directory. aspx'read_url = urllib. request. urlopen(url). read()soup = bs(read_url, 'html. parser')Then create the CSV file: with open('one_page_leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['name', 'title', 'company', 'url'])Instead of running the script multiple times to try to scrape the data. I like to work on the Python shell to see the results and then add the code to the script. Test scraping using the shell:: (env)$ python&gt;&gt;&gt;Then setup the script with BeautifulSoup: &gt;&gt;&gt; from bs4 import BeautifulSoup as bs&gt;&gt;&gt; import urllib. request, urllib. parse, urllib. error&gt;&gt;&gt; url = 'https://www. ces. tech/Conference/Speaker-Directory. aspx'&gt;&gt;&gt; urlopen = urllib. request. urlopen(url). read()&gt;&gt;&gt; soup = bs(urlopen, 'html. parser')Now we need to Inspect the HTML on the page and use soup methods to extract the data.  To view soup methods you can try: &gt;&gt;&gt; dir(soup)It will show a list of all methods. Get some help about a method using help: &gt;&gt;&gt; help(soup. find)It shows the following syntax: find(name=None, attrs={}, recursive=True, text=None, **kwargs)Each speaker has an HTML that looks like this: &lt;aside class= speaker-photo directory small &gt;  &lt;a href= /conference/speaker-directory/Omar-Abdelwahed &gt;    &lt;picture class= head-shot  style= background-image: url('https://hubb. blob. core. windows. net/e955a157-fba9-4f75-9465-67396ae15f0e-profile/546100') &gt;&lt;/picture&gt;  &lt;/a&gt;      &lt;a href= /conference/speaker-directory/Omar-Abdelwahed &gt;      &lt;h3 class= speaker-name &gt;Omar Abdelwahed &lt;/h3&gt;    &lt;/a&gt;    &lt;h4 class= speaker-title &gt;Head of Studio&lt;/h4&gt;    &lt;h4 class= speaker-company &gt;SoftBank Robotics&lt;/h4&gt;  &lt;/aside&gt;The parent tag is: &lt;aside class= speaker-photo directory small &gt;The child tags have this content:  &lt;h3 class= speaker-name &gt; &lt;h4 class= speaker-title &gt; &lt;h4 class= speaker-company &gt; &lt;a href= /conference/speaker-directory/. . . Which correspond to name, title, company and url. Scrape one name: &gt;&gt;&gt; soup. find('h3', attrs={'class': 'speaker-name'})The output is: &lt;h3 class= speaker-name &gt;Omar Abdelwahed &lt;/h3&gt;This is correct but we only want the text inside the tag. &gt;&gt;&gt; soup. find('h3', attrs={'class': 'speaker-name'}). text'Omar Abdelwahed 'But this only finds one name. Scrape all the names: This doesn’t work: &gt;&gt;&gt; soup. find_all('h3', attrs={'class': 'speaker-name'}). textIt has this error: Traceback (most recent call last):AttributeError: ResultSet object has no attribute 'text'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?Then try this: &gt;&gt;&gt; soup. find_all('h3', attrs={'class': 'speaker-name'})The output is a list of soup objects with the &lt;h3 class= speaker-name &gt; tag and text. Scraping and matching rows: I thought of this. We can scrape the list of names. But what about the other data that corresponds to each speaker. The names could be: names = [name1, name2, name3, . . . ]Then titles: titles = [title1, title2, title3,. . . ]But how do we join these two lists? What if the data doesn’t match? We need to find the correct data structure to put all the scraped data. Scraping the parent tag: The parent tag that has the data for each speaker is: &lt;aside class= speaker-photo directory small &gt;We can use this code to find the first speaker: &gt;&gt;&gt; soup. find('aside', attrs={'class': 'speaker-photo directory small'})&lt;aside class= speaker-photo directory small &gt;&lt;a href= /conference/speaker-directory/Omar-Abdelwahed &gt;&lt;picture class= head-shot  style= background-image: url('https://hubb. blob. core. windows. net/e955a157-fba9-4f75-9465-67396ae15f0e-profile/546100') &gt;&lt;/picture&gt;&lt;/a&gt;&lt;caption class= speaker-info &gt;&lt;a href= /conference/speaker-directory/Omar-Abdelwahed &gt;&lt;h3 class= speaker-name &gt;Omar Abdelwahed &lt;/h3&gt;&lt;/a&gt;&lt;h4 class= speaker-title &gt;Head of Studio&lt;/h4&gt;&lt;h4 class= speaker-company &gt;SoftBank Robotics&lt;/h4&gt;&lt;/caption&gt;&lt;/aside&gt;Use this code to put the HTML content for all speakers into a list: &gt;&gt;&gt; soup. find_all('aside', attrs={'class': 'speaker-photo directory small'})Assign to a variable: &gt;&gt;&gt; speakers = soup. find_all('aside', attrs={'class': 'speaker-photo directory small'})Use this: speakers[0] to see the content of the first speaker. &lt;aside class= speaker-photo directory small &gt;&lt;a href= /conference/speaker-directory/Omar-Abdelwahed &gt;&lt;picture class= head-shot  style= background-image: url('https://hubb. blob. core. windows. net/e955a157-fba9-4f75-9465-67396ae15f0e-profile/546100') &gt;&lt;/picture&gt;&lt;/a&gt;&lt;caption class= speaker-info &gt;&lt;a href= /conference/speaker-directory/Omar-Abdelwahed &gt;&lt;h3 class= speaker-name &gt;Omar Abdelwahed &lt;/h3&gt;&lt;/a&gt;&lt;h4 class= speaker-title &gt;Head of Studio&lt;/h4&gt;&lt;h4 class= speaker-company &gt;SoftBank Robotics&lt;/h4&gt;&lt;/caption&gt;&lt;/aside&gt;Now we can get separate the data for each speaker… Actually I am not sure what methods I can use. Let’s use help: &gt;&gt;&gt; help(speakers[0])Help on Tag in module bs4. element object:class Tag(PageElement)| Represents a found HTML tag with its attributes and contents. | | Method resolution order:|   Tag|   PageElement|   builtins. object| | Methods defined here:This documentation is long. Here is an extract: | | find(self, name=None, attrs={}, recursive=True, text=None, **kwargs)|   Return only the first child of this Tag matching the given|   criteria. | | findAll = find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)| | findChild = find(self, name=None, attrs={}, recursive=True, text=None, **kwargs)| findChildren = find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)| | find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)|   Extracts a list of Tag objects that match the given|   criteria.  You can specify the name of the Tag and any|   attributes you want the Tag to have. |   |   The value of a key-value pair in the 'attrs' map can be a|   string, a list of strings, a regular expression object, or a|   callable that takes a string and returns whether or not the|   string matches for some custom definition of 'matches'. The|   same is true of the tag name. | | get(self, key, default=None)|   Returns the value of the 'key' attribute for the tag, or|   the value given for 'default' if it doesn't have that|   attribute. | | getText = get_text(self, separator='', strip=False, types=(&lt;class 'bs4. element. NavigableString'&gt;, &lt;class 'bs4. element. CData'&gt;))| | get_attribute_list(self, key, default=None)|   The same as get(), but always returns a list. | get_text(self, separator='', strip=False, types=(&lt;class 'bs4. element. NavigableString'&gt;, &lt;class 'bs4. element. CData'&gt;))|   Get all child strings, concatenated using the given separator. | Scrape the URL: Here is the code to capture the URL bio from speakers[0]: &gt;&gt;&gt; speakers[0]. find('a'). get('href')'/conference/speaker-directory/Omar-Abdelwahed'Scrape the name: This is the HTML that contains the name: &lt;h3 class= speaker-name &gt;Omar Abdelwahed &lt;/h3&gt;Use this code to scrape the name: &gt;&gt;&gt; speakers[0]. find('h3', attrs={'class': 'speaker-name'}). text'Omar Abdelwahed 'I see it has whitespace at the end of the string. But not sure if this is just this one or all of them. Not a big deal for now. Scrape the title: This HTML has the title: &lt;h4 class= speaker-title &gt;Head of Studio&lt;/h4&gt;Use this code: &gt;&gt;&gt; speakers[0]. find('h4', attrs={'class': 'speaker-title'}). text'Head of Studio'Scrape the company: This HTML has the company: &lt;h4 class= speaker-company &gt;SoftBank Robotics&lt;/h4&gt;Use the code: &gt;&gt;&gt; speakers[0]. find('h4', attrs={'class': 'speaker-company'}). text'SoftBank Robotics'Add this code to the script one_page. py: We don’t need to use speakers[0] anymore. We are going to find all speakers into a list. Then use a loop to scrape the data from each speaker. Then write the row to CSV. We are going to use exceptions in case a data value from a speaker is not found. And we will add logging for each step. import urllib. request, urllib. parse, urllib. errorfrom bs4 import BeautifulSoup as bsimport csvimport logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('one_page_output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter(('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))fh. setFormatter(formatter)logger. addHandler(fh)url = 'https://www. ces. tech/Conference/Speaker-Directory. aspx'read_url = urllib. request. urlopen(url). read()soup = bs(read_url, 'html. parser')with open('one_page_leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['name', 'title', 'company', 'url'])  speakers = soup. find_all('aside', attrs={'class': 'speaker-photo directory small'})  for speaker in speakers:    logger. info('Scraping a speaker. . . ')    print('Scraping a speaker. . . ')    try:      url = speaker. find('a'). get('href')    except:      logger. info('No URL found')      url = 'Not found'    try:      name = speaker. find('h3', attrs={'class': 'speaker-name'}). text    except:      logger. info('Name not found')      name = 'Not found'    try:      title = speaker. find('h4', attrs={'class': 'speaker-title'}). text    except:      logger. info('Title not found')      title = 'Not found'    try:      company = speaker. find('h4', attrs={'class': 'speaker-company'}). text    except:      logger. info('Company not found')      company = 'Not found'    logger. info('Saving %s to CSV', name)    csvwriter. writerow([name, title, company, url])for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)Running the script one_page. py: Run the script with: $ python one_page. pyIt will take a few seconds to capture the page and then it will print to the terminal: Scraping a speaker. . . Scraping a speaker. . . Scraping a speaker. . . Scraping a speaker. . . Scraping a speaker. . . . . . Review the log file: We saved the log file with this name: one_page_output. log It looks like it scraped all the data. Review the CSV file: We named the CSV file: one_page_leads. csv.  It looks good. But the dilemma continues. We don’t have the bios. 6. Scrape multiple pages. : We can add more to the script if we really want the bio for each speaker. Let’s just copy the script over to a new file: many_pages. py. But first let’s test how to get a bio using the Python shell. The first URL from the CSV file is this: /conference/speaker-directory/Omar-AbdelwahedGo to the shell: &gt;&gt;&gt; from bs4 import BeautifulSoup as bs&gt;&gt;&gt; import urllib. request, urllib. parse, urllib. error&gt;&gt;&gt; url = 'https://www. ces. tech/conference/speaker-directory/Omar-Abdelwahed'&gt;&gt;&gt; urlopen = urllib. request. urlopen(url). read()&gt;&gt;&gt; soup = bs(urlopen, 'html. parser')Previously we saw that there were 2 bios. Short bio: This metadata has a short bio: &lt;meta name= description  content= Omar Abdelwahed is Head of Studio at SoftBank Robotics America where he is responsible for leading the development of robotics applications in America, and the overall user experience, globally &gt;Scrape it with: &gt;&gt;&gt; soup. find('meta', attrs={'name': 'description'}). get('content')'Omar Abdelwahed is Head of Studio at SoftBank Robotics America where he is responsible for leading the development of robotics applications in America, and the overall user experience, globally'Long bio: This tag has a long bio: &lt;article class= speaker-bio &gt;&lt;p&gt;Omar Abdelwahed is Head of Studio at SoftBank Robotics America where he is responsible for leading the development of robotics applications in America, and the overall user experience, globally. Previously, Omar was VP of Engineering at Mighty Play, a game developer in San Francisco. Omar has over 20 years of experience as an engineer with a deep background in entertainment and technology. Omar’s career has spanned large video game publishers, retailers, startups, and his own independent work. Omar is the founder of Agent Disco, an independent mobile games developer. He is a frequent conference speaker on technology topics, including AI, robotics, data privacy, and product development. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;/article&gt;Scrape it with: &gt;&gt;&gt; soup. find('article', attrs={'class': 'speaker-bio'}). find('p'). text'Omar Abdelwahed is Head of Studio at SoftBank Robotics America where he is responsible for leading the development of robotics applications in America, and the overall user experience, globally. Previously, Omar was VP of Engineering at Mighty Play, a game developer in San Francisco. Omar has over 20 years of experience as an engineer with a deep background in entertainment and technology. Omar’s career has spanned large video game publishers, retailers, startups, and his own independent work. Omar is the founder of Agent Disco, an independent mobile games developer. He is a frequent conference speaker on technology topics, including AI, robotics, data privacy, and product development. 'Looks like the short bio is the first sentence up to the first period. For others it might or might not be the same. Maybe we should scrape both just in case. I opened a few profiles and noticed some have additional data. Linkedin URL: Found that this profile: https://www. ces. tech/conference/speaker-directory/Charlie--Ackerman. aspxHas a Linkedin URL: &lt;a href= https://www. linkedin. com/in/charlie-ackerman-56499013/  target= _blank  class= fab fa-linkedin &gt;&lt;/a&gt;Scrape it with: &gt;&gt;&gt; soup. find('a', attrs={'class': 'fab fa-linkedin'}). get('href')'https://www. linkedin. com/in/charlie-ackerman-56499013/'Modifying the script many_pages. py: Same setup as before but change log file name and csv file name: import urllib. request, urllib. parse, urllib. errorfrom bs4 import BeautifulSoup as bsimport csvimport logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('many_pages_output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter(('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))fh. setFormatter(formatter)logger. addHandler(fh)url = 'https://www. ces. tech/Conference/Speaker-Directory. aspx'read_url = urllib. request. urlopen(url). read()soup = bs(read_url, 'html. parser')with open('many_pages_leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['name', 'title', 'company', 'url'])for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)We need to make some additional changes. Once we capture each URL we need to scrape the data:  short bio long bio Linkedin URLChange the column names of the CSV file: with open('many_pages_leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['name', 'title', 'company', 'url', 'short_bio', 'long_bio', 'linkedin'])A scraped URL has this form: /conference/speaker-directory/Omar-AbdelwahedTo scrape this page we need the complete URL. Maybe add this variable: root_url = 'https://www. ces. tech'The final script is this: Ideally this could be broken into modules but for now this is the final script: import urllib. request, urllib. parse, urllib. errorfrom bs4 import BeautifulSoup as bsimport csvimport logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('many_pages_output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter(('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))fh. setFormatter(formatter)logger. addHandler(fh)root_url = 'https://www. ces. tech'url = 'https://www. ces. tech/Conference/Speaker-Directory. aspx'urlopen = urllib. request. urlopen(url). read()soup = bs(urlopen, 'html. parser')with open('many_pages_leads. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)  csvwriter. writerow(['name', 'title', 'company', 'url', 'short_bio', 'long_bio', 'linkedin'])  speakers = soup. find_all('aside', attrs={'class': 'speaker-photo directory small'})  for speaker in speakers:    logger. info('Scraping a speaker. . . ')    print('Scraping a speaker. . . ')        try:      name = speaker. find('h3', attrs={'class': 'speaker-name'}). text    except:      logger. info('Name not found')      name = 'Not found'    try:      title = speaker. find('h4', attrs={'class': 'speaker-title'}). text    except:      logger. info('Title not found')      title = 'Not found'    try:      company = speaker. find('h4', attrs={'class': 'speaker-company'}). text    except:      logger. info('Company not found')      company = 'Not found'        try:      url = speaker. find('a'). get('href')      url_speaker = urllib. request. urlopen(root_url + url). read()      soup_speaker = bs(url_speaker, 'html. parser')      # Get short bio      try:        short_bio = soup_speaker. find('meta', attrs={'name': 'description'}). get('content')      except:        logger. info('Short bio not found')        short_bio = 'Not found'      # Get long bio      try:        long_bio = soup_speaker. find('article', attrs={'class': 'speaker-bio'}). find('p'). text      except:        logger. info('Long bio not found')        long_bio = 'Not found'      # Get Linkedin      try:        linkedin = soup_speaker. find('a', attrs={'class': 'fab fa-linkedin'}). get('href')      except:        logger. info('Linkedin URL not found')        linkedin = 'Not found'    except:      logger. info('URL not found')      url = 'Not found'    logger. info('Saving %s to CSV', name)    csvwriter. writerow([name, title, company, url, short_bio, long_bio, linkedin])for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)Reviewing results: You can open the log file in Sublime to see progress. Or just monitor the file size and you will see the log file increasing size. Open the CSV file to see the output so far: To module or not to module: This looks like a one-time script. A sort of scrape-on-demand. Scrape a specific page and move on. Another scrape project would have a different HTML structure depending on what you want to scrape. You could use the same setup for logging and CSV but there is room for growth if you break this into modules:  Ask for user input instead of hardcoding the URLs.  Use a main function for the setup.  Use modules for scraping.  Use a random function to ease on the scraping. of hardcoding the URLs. "
    }, {
    "id": 176,
    "url": "https://www.tomordonez.com/usb-stuck-read-only-linux/",
    "title": "USB Stuck in Read Only in Linux",
    "body": "2018/10/31 - Fixing a usb stuck in read only in Linux As seen here. I have an external drive that I used for backing up some files. I plugged it in again and tried to copy some files but got an error saying that the drive was in “read-only”. This is applicable to Linux. The solution was doing this: $ killall nautilus"
    }, {
    "id": 177,
    "url": "https://www.tomordonez.com/vim-stuck-insert-mode/",
    "title": "Vim Stuck in Insert Mode",
    "body": "2018/10/22 - Fixing vim stuck in insert mode. As seen here. I find out that when I copy/paste something in insert mode. I make the mistake of typing ctrl + s. To get out, use ctrl + q. "
    }, {
    "id": 178,
    "url": "https://www.tomordonez.com/python-unit-testing-tutorial/",
    "title": "Python Unit Testing Tutorial",
    "body": "2018/10/21 - This is a tutorial about unit testing in Python. Here are some great resources that helped me write this tutorial:  Official Docs.  This Youtube video about unit testing. Python Unit Testing: Unit testing files follow this convention:  If the module name is calculator. py.  Then the test file should be test_calculator. py. Create a virtual environment. Create the file calculator. py: class Calculator:  def __init__(self):    pass  def add(self, x, y):    return(x + y)  def subtract(self, x, y):    return(x - y)  def multiply(self, x, y):    return(x * y)  def divide(self, x, y):    return(x / y)You could run this from the Python shell: &gt;&gt;&gt; from calculator import *&gt;&gt;&gt; a = Calculator()&gt;&gt;&gt; a. add(2, 3)5&gt;&gt;&gt; a. subtract(2, 3)-1&gt;&gt;&gt; a. multiply(2, 3)6&gt;&gt;&gt; a. divide(2, 3)0. 6666Create the test file: Create the test file test_calculator. py: import unittestfrom calculator import *Create a test class that inherits from unittest. TestCase: class TestCalculator(unittest. TestCase):Create methods following the same convention test_: class TestCalculator(unittest. TestCase):    def test_add(self):TestCase assert methods: The official docs show that TestCase provides many assert methods we can use. They use the format assertSomething(first, second, msg=None). The methods can accept a msg as an argument to output an error message.  assertEqual(a, b) assertNotEqual(a, b) assertTrue(x) assertFalse(x) assertIs(a, b): a is b assertIsNot(a, b) assertIsNone(x) assertIsNotNone(x) assertIn(a, b): a in b assertNotIn(a, b) assertIsInstance(a, b) assertNotIsInstane(a, b) assertRaises(exception, callable, *args, **kwds)Using the method assertEqual:: def test_add(self):  result = Calculator(). add(1, 2)  self. assertEqual(result, 3)Running the test file: Run unittest as the main module: $ python -m unittest test_calculator. pyOr modify the test file: if __name__ == '__main__':  unittest. main()Then run the file like this: $ python test_calculator. pyThis will show: . -------------------Ran 1 test in 0. 00sThis is what we have so far: This is what we have for test_calculator. py: import unittestfrom calculator import *class TestCalculator(unittest. TestCase):  def test_add(self):  result = Calculator(). add(1, 2)  self. assertEqual(result, 3)if __name__ == '__main__':  unittest. main()Then run the test file like this: $ python test_calculator. pyRefactoring: Add the result variable into the assert statement: def test_add(self):  self. assertEqual(Calculator(). add(1, 2), 3)Check edge cases: Add more assert statements: def test_add(self):  self. assertEqual(Calculator(). add(1, 2), 3)  self. assertEqual(Calculator(). add(-1, 2), 1)  self. assertEqual(Calculator(). add(-1, -2), -3)  self. assertEqual(Calculator(). add(1, -2), -1)Using assertRaises(): This method uses this format: assertRaises(exception, callable, *args, **kwds)Following the example of division by 0. Inside calculator. py: def divide(self, x, y):  if y == 0:    raise ValueError('Cannot divide by zero')  return(x / y)Inside test_calculator. py: def test_divide(self):  self. assertRaises(ValueError, Calculator(). divide, 1, 0)I don’t really like this syntax. I don’t understand why this cannot be done instead: self. assertRaises(ValueError, Calculator(). divide(1, 0))Following the video tutorial. The alternative is to do: with self. assertRaises(ValueError):  Calculator(). divide(1,0)Using assertIsInstance(): Try a different example to follow and unfollow users on Twitter. Do not mind the calculations. They don’t make much sense :) Given user. py: class User:  follow_rate = 1. 2	def __init__(self, name, following, followers):	  self. name = name	  self. following = following	  self. followers = followers	def screen_name(self):	  return('@{}'. format(self. name))	def auto_follow(self):	  if self. following &lt; self. followers:	    self. following = self. followers * self. follow_rate      return(self. following)    def unfollow(self):    if self. following &gt; self. followers:      self. following = (self. following - self. followers)\               * self. follow_rate      return(self. following)And test_user. py: import unittestfrom user import Userclass TestUser(unittest. TestCase):  def test_screen_name(self):    pass  def test_auto_follow(self):    passif __name__ == '__main__':  unittest. main()The test is not doing anything right now. But it should pass: $ python test_user. py. . ------Ran 2 tests in 0. 000sAdd some tests: def test_screen_name(self):    user1 = User('homer', 50, 100)  user2 = User('bart', 20, 10)  self. assertIsInstance(user1. name, str)  self. assertIsInstance(user1. following, int)  self. assertIsInstance(user1. followers, int)def test_auto_follow(self):  passRunning: . . ----Ran 2 tests in 0. 00sUsing assertEqual(): Add the method to test_screen_name: def test_screen_name(self):  	user1 = User('homer', 50, 100)	self. assertIsInstance(user1. name, str)	self. assertIsInstance(user1. following, int)	self. assertIsInstance(user1. followers, int)	self. assertEqual(self. screen_name, '@homer')def test_auto_follow(self):  passJust to show what a failure looks like: $ python test_user. py. E============ERROR: test_screen_name(__main__. TestUser)-------------Traceback (most recent call last): File  test_user. py , line 14, in test_screen_name  self. assertEqual(self. screen_name, '@homer')AttributeError: 'TestUser' objet has no attribute'screen_name'------------Ran 2 tests in 0. 00sFAILED (errors=1)To correct this. Replace: self. assertEqual(self. screen_name, '@homer')With: self. assertEqual(user1. screen_name(), '@homer')Running: . . ------Ran 2 tests in 0. 00sAdd another test: def test_auto_follow(self):  	user1 = User('homer', 50, 100)	user1. auto_follow()	self. assertEqual(user1. follow, 50*1. 2)Running: . . ------Ran 2 tests in 0. 001sUsing setUp() and tearDown(): This is what we have so far for test_user. py: import unittestfrom user import Userclass TestUser(unittest. TestCase):  def test_screen_name(self):        # User(name, following, followers)    # screen_name returns @name    user1 = User('homer', 50, 100)    user2 = User('bart', 20, 10)    self. assertIsInstance(user1. name, str)    self. assertIsInstance(user1. following, int)    self. assertIsInstance(user1. followers, int)    self. assertEqual(user1. screen_name(), '@homer')    self. assertEqual(user2. screen_name(), '@bart')  def test_auto_follow(self):        # if following &lt; followers    # self. follow = following * follow_rate    # follow_rate = 1. 2    user1 = User('homer', 50, 100)    user2 = User('bart', 20, 10)    user1. auto_follow()    self. assertEqual(user1. follow, 50*1. 2)    user2. auto_follow()    self. assertEqual(user2. unfollow, (20-10)*1. 2) if __name__ == '__main__':  unittest. main()The test cases user1 and user2 are repeated for both tests. Instead use this: class TestUser(unittest. TestCase):  def setUp(self):    pass  def tearDown(self):    passCreate the users in setUp(): class TestUser(unittest. TestCase):  def setUp(self):    self. user1 = User('homer', 50, 100)    self. user2 = User('bart', 20, 10)The other methods need to be changed to instance attributes: From this: self. assertEqual(user1. screen_name(), '@homer')self. assertEqual(user2. screen_name(), '@bart')To this: self. assertEqual(self. user1. screen_name(), '@homer')self. assertEqual(self. user2. screen_name(), '@bart')Isolated tests: Here is something interesting. If you add print statements after each method. You can see that setUp and tearDown run after each test such as: setUptest_screen_nametearDownsetUptest_auto_followtearDownAlso, the tests might not run in order. Using tearDown(): Use case could be when setting up a database. Use setUp() to create the database and use tearDown() to delete it. Using setUpClass() and tearDownClass(): Since setUp runs before each test and tearDown after each test. There is a way to run a different kind of setup before all tests (setUpClass) and a kind of teardown after all tests (tearDownClass). class TestUser(unittest. TestCase):  @classmethod  def setUpClass(cls):    pass  @classmethod  def tearDownClass(cls):    passYou can also add print statements to these methods to see where they run. As seen here. With @classmethod, the class is passed as the first argument, instead of the instance of the class. You can use the class and properties inside that method. The setUpClass(cls) could be used to populate a database. So that this is not done for each separate test. The tests can read from the database. Then use the tearDownClass(cls). Python Mocking: The use case is connecting to a website. We want the test to fail only if there is something wrong with the code but not when the server is down. Install requests inside your virtual environment: env$ pip install requestsTo test that requests was installed: env$ python&gt;&gt;&gt; import requests&gt;&gt;&gt; response = requests. get('https://github. com/tomordonez')The variable response is a requests Response object: &gt;&gt;&gt; type(response)&lt;class 'requests. models. Response'&gt;You can review its methods using: &gt;&gt;&gt; dir(response)These are some of the methods  response. headers: Returns a dictionary response. encoding: ‘utf-8’ response. status_code: 200 response. ok: True response. text: Returns the content of the pageAdding the method to user. py: import requestsclass User:  follow_rate = 1. 2  def __init__(self, name, following, followers):    self. name = name    self. following = following    self. followers = followers  def get_user(self):    self. response = requests. get('https://twitter. com/{}'\                   . format(self. name))    if self. response. ok:      return(self. response. headers)    else:      return('Bad Response')  def screen_name(self):    return('@{}'. format(self. name))  def auto_follow(self):    if self. following &lt; self. followers:      self. following = self. followers * self. follow_rate      return(self. following)    def unfollow(self):    if self. following &gt; self. followers:      self. following = (self. following - self. followers)\               * self. follow_rate      return(self. following)To read the source code for mock go here. These are some of the classes:  class Mock(CallableMixin, NonCallableMock) class CallableMixin(Base) class NonCallableMock(Base) class Base(object) class MagicMock(MagicMixin, Mock) class MagicMixin(object)The Mock Class:  Mocks are callable and create attributes.  MagicMock is a subclass of Mock with implementations of most of the magic methods.  Python magic methods are special methods that you can define to add magic to your classes. Always enclosed in double underscore. For example: __init__. More details here.  The patch() decorator is used to replace classes in a module with a Mock object. It will create a MagicMock.  return_value is a Mock argument. It’s the value returned when the mock is called. I guess this is why we are using patch here: from unittest. mock import patchSince patch() creates a MagicMock and this one is a subclass of Mock. Get all the Mock details from the official docs here. Add the test to test_user. py:  mocked_get is a Mock object.  mocked_get. return_value is the return value when mocked_get is called.  mocked_get. assert_called_with is Mock method. Test a good response: def test_get_user(self):  with patch('user. requests. get') as mocked_get:    mocked_get. return_value. ok = True    mocked_get. return_value. headers =  Success     response = self. user1. get_user()    mocked_get. assert_called_with('https://twitter. com/{}'\                    . format(self. user1. name))    self. assertEqual(response,  Success )Test a bad response: def test_get_user(self):  with patch('user. requests. get') as mocked_get:    mocked_get. return_value. ok = True    mocked_get. return_value. headers =  Success     response = self. user1. get_user()    mocked_get. assert_called_with('https://twitter. com/{}'\                    . format(self. user1. name))    self. assertEqual(response,  Success )    mocked_get. return_value. ok = False    response = self. user1. get_user()    mocked_get. assert_called_with('https://twitter. com/{}'\                    . format(self. user1. name))    self. assertEqual(response,  Bad response )Review all the code for test_user. py: import unittestfrom unittest. mock import patchfrom user import Userclass TestUser(unittest. TestCase):  def setUp(self):    self. user1 = User('homer', 100, 200)  def tearDown(self):    pass  def test_get_user(self):    with patch('user. requests. get') as mocked_get:      mocked_get. return_value. ok = True      mocked_get. return_value. headers =  Success       response = self. user1. get_user()      mocked_get. assert_called_with('https://twitter. com/{}'\                      . format(self. user1. name))      self. assertEqual(response,  Success )      mocked_get. return_value. ok = False      response = self. user1. get_user()      mocked_get. assert_called_with('https://twitter. com/{}'\                      . format(self. user1. name))      self. assertEqual(response,  Bad response )  def test_screen_name(self):    self. assertIsInstance(self. user1. name, str)    self. assertIsInstance(self. user1. following, int)    self. assertIsInstance(self. user1. followers, int)    self. assertEqual(self. user1. screen_name(), '@homer')  def test_auto_follow(self):    self. assertEqual(self. user1. auto_follow(), self. user1. followers\             * self. user1. follow_rate)  def test_unfollow(self):    self. assertEqual(self. user1. unfollow(), (self. user1. followers\             - self. user1. following) * self. user1. follow_rate) if __name__ == '__main__':  unittest. main()I don’t fully understand Mock but here are more resources:  Get all the Mock details from the official docs here. "
    }, {
    "id": 179,
    "url": "https://www.tomordonez.com/bash-commands-for-productivity/",
    "title": "Bash Commands for Productivity",
    "body": "2018/10/19 - Here are some interesting bash commands to run from the shell for productivity. Make changes to many directories: If you want to create the same file in multiple directories. For example. I have these directories: $ pwd~/languages/$ lspython java cppI wanted to create an index. rst inside each directory: $ for i in */; do touch  $i /index. rst; doneNow these directories have that file:  ~/languages/python/index. rst ~/languages/java/index. rst ~/languages/cpp/index. rstI moved one level up on my directory: $ cd . . $ lslanguages books tools frameworksInside these directories I also have an index. rst such as:  ~/languages/index. rst ~/books/index. rst ~/tools/index. rst ~/frameworks/index. rstOoops I forgot to add a title inside the index. rst for the directories inside languages: $ for in in */; do echo  Title  &gt;&gt;  $i /index. rst; doneOh wait. I made a mistake. I am still inside my main directory: $ pwd~/$ lslanguages books tools frameworksI only wanted to add “Title” inside index. rst for the subdirectories of languages:  ~/languages/python/index. rst ~/languages/java/index. rst ~/languages/cpp/index. rstAnd NOT for the directories of home:  ~/languages/index. rst ~/books/index. rst ~/tools/index. rst ~/frameworks/index. rstIf I open this: ~/languages/index. rst Now I have my index content and the last line is Title. This sucks. How do I remove the last line of multiple files now? Remove the last line of multiple files: Let me see where I am: $ pwd~/$ lslanguages books tools frameworksAs seen here. This is a solution to remove the last line from a file: sed -i '$ d' foo. txtApplied to many files: $ for i in */index. rst; do sed -i '$ d'  $i ; doneNow the last line is gone. Let’s go to the correct directory: $ cd languages/$ for in in */; do echo  Title  &gt;&gt;  $i /index. rst; doneone. "
    }, {
    "id": 180,
    "url": "https://www.tomordonez.com/installing-minecraft-server-mac/",
    "title": "Installing a Minecraft Server on Mac",
    "body": "2018/10/14 - Awesome tutorial for installing a Minecraft server on Mac. I am reading a book to apply Python to Minecraft. The book is called “Learn to Program with Minecraft” by Craig Richardson. This blog post was inspired by the procedure explained on the book. Starting the server: Used for reference. Don’t start here:  Right click &gt; Open the file Start_Server Open Minecraft Play &gt; drop down &gt; 1. 11. 2 Multiplayer Select pycraft-server. Join Server. Setup a Mac environment: Start here. We need to install 5 components:  Minecraft Python3 JDK Minecraft Python API Spigot Minecraft ServerMinecraft: You need to get an account and buy Minecraft. Installing is really easy. Open the program and login. Click Play. Go to Single Player &gt; Create New World. I called it pycraft. Python 3: There are a lot of tutorials about this. I haven’t used the Mac for a long time so I had some issues. This is what I did: $ sudo pip3 install --upgrade pipJDK: I thought I previously had this installed. To check if you have it. Go to System Preferences and there should be an icon that says Java. But later in the process when I ran the Minecraft server it said: No Java runtime presentGo to: https://www. oracle. com/technetwork/java/javase/downloads/index. html Click the button: Java Download. It’s just a square and it is easy to miss. Scroll down and choose the radio button to accept the license. Then download the dmg for macOS. Follow the GUI install process. Install the API: Here is a summary of the process from the book:  https://nostarch. com/programwithminecraft/ Download setup files for MAC: https://sourceforge. net/projects/program-with-minecraft-mac/ Download The file downloaded is: Minecraft_Tools_Mac. zip.  Extract the content: Minecraft Tools Mac. This folder has these:  Directory: py3minepi-master Directory: server File: Start_Server File: Install_API. commandInstall the API by running this file: Install_API. command. This opens the terminal and might ask for the admin password. If this doesn’t work. Go to the terminal and locate the file: $ sudo . /Install_API. commandThe output was this: The directory '/Users/tomordonez/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag. The directory '/Users/tomordonez/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag. Processing . /py3minepi-masterInstalling collected packages: py3minepiRunning setup. py install for py3minepi . . . doneSuccessfully installed py3minepi-0. 0. 1I tried this: sudo chown -R $USER /Users/$USER/Library/Caches/pipBut it said the file didn’t exist. I checked this. Not sure how to solve this for now. Run the server: Spigot is a single-player Minecraft server. Inside your Minecraft Tools Mac directory. Right click &gt; Open the file Start_Server. Minecraft launch options: The book says to find the server version: Starting minecraft server version 1. 11. 2Launch Minecraft. Go to Menu &gt; Launch Options &gt; Add New. Enter a name. I entered: pycraft. Select the version: 1. 11. 2. Save and close the Launch Options. Click the logo to go to the main screen. Go to PLAY, drop down now has the option: pycraft 1. 11. 2. It will start downloading this version. Add the server to Minecraft: When Minecraft opens. Go to Multiplayer. Then Add Server. For server name I put: pycraft-server. For server address: localhost. Then Done. The added server will show up. Click on it and Join Server. Minecraft will launch the world. The server output shows: [15:51:17 INFO]: UUID of player my-username is 3b1c5. . . [15:51:18 INFO]: my-username[/127. 0. 0. 1:51952] logged in with entity id 1633 at ([world]-49. 5, 61. 0, -154. 5)Navigating the world: I am not an expert in Minecraft. I only played it a few times. To leave the window. Click ESC. It says that the world is set to Creative Mode.  Fly: Tap space twice Up: Press space Down: Shift Stop Flying: Tap space twice again. Create a new world: The book has some tips about creating a new world.  In Minecraft, leave the game to go back to the menu.  Close the server shell.  Create a copy of the folder Minecraft Tools Mac.  Go to the server folder Delete the folders: world, world_nether, world_the_end Start the server on the new copy. Play offline: Close the server by typing ‘stop’. Then close the shell. Inside the server folder:  Open the file server. properties.  Change this online-mode=true To this online-mode=false. Switch from Creative to Survival: The book says that it is a good idea to test Python programs in Survival mode. By default the config is in Creative mode. Open server. properties. Change gamemode=1 to gamemode=0.  gamemode=1 is Creative mode.  gamemode=0 is Survival mode. Test your setup:  Start the server Start Minecraft in multiplayer mode and selecting the server.  Open another shell Open the python shell: $ python3 Import the module: &gt;&gt;&gt; from mcpi. minecraft import Minecraft.  Create a Minecraft object: &gt;&gt;&gt; mc = Minecraft. create()Minecraft class methods: The Minecraft class has these methods: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'create', 'getBlock', 'getBlockWithData', 'getBlocks', 'getHeight', 'getPlayerEntityIds', 'postToChat', 'restoreCheckpoint', 'saveCheckpoint', 'setBlock', 'setBlocks', 'setting']Minecraft object methods: The Minecraft object mc has these methods: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'camera', 'conn', 'create', 'entity', 'events', 'getBlock', 'getBlockWithData', 'getBlocks', 'getHeight', 'getPlayerEntityIds', 'player', 'postToChat', 'restoreCheckpoint', 'saveCheckpoint', 'setBlock', 'setBlocks', 'setting']"
    }, {
    "id": 181,
    "url": "https://www.tomordonez.com/pip-install-ssl-module-python-is-not-available/",
    "title": "pip install SSL Module in Python Is Not Available",
    "body": "2018/10/11 - Working inside a virtual environment and running pip install anything was giving me this error: pip is configured with locations that require TLS/SSL,however the ssl module in Python is not available. Also this error: Can't connect to HTTPS URL because the SSL moduleis not availableI wanted to install requests such as: (env)$ pip install requestsI deactivated the virtualenv and ran: $ pip install -U pipI got this error: Permission denied: 'usr/bin/pip'Then ran this. As recommended on the output: $ pip install -U pip --userThis worked. Now I wanted to install requests: $ pip install requests --userThis worked too. Remove and Create a new env: I removed the virtualenv and created a new one: $ virtualenv -p /usr/bin/python3 env$ source env/bin/activateInstalling requests: env$ pip install requestsThis works now. "
    }, {
    "id": 182,
    "url": "https://www.tomordonez.com/python-context-manager/",
    "title": "Python Context Manager",
    "body": "2018/10/09 - A short tutorial about Python context manager: “with” statement. Sources:  This blog post about context managers.  The Python docs. Here is a popular example: import csvwith open('output. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)The context manager with is used for allocation and releasing of resources. This is alternative to doing this: import csvcsvfile = open('output. csv', 'w', newline='')csvwriter = csv. writer(csvfile)csvfile. close()Which is also similar to: import csvcsvfile = open('output. csv', 'w', newline='')try:  csvwriter = csv. writer(csvfile)finally:  csvfile. close()The blog post from the sources list has a good example: setup()try:  do_something()finally:  teardown()Similar to: contextmanager. __enter__()try:  do_something()finally:  contextmanager. __exit__()Context Manager protocol: The context manager protocol follows two methods:  __enter__ __exit__As seen in the Python docs. It “defines a runtime context that is entered before the statement body is executed and exited when the statement ends”. Following the same example: import csvwith open('output. csv', 'w', newline='') as csvfile:  csvwriter = csv. writer(csvfile)For contextmanager. __enter__():  It returns an object assigned to the variable csvfile after asFor contextmanager. __exit__():  Exits the runtime context.  Returns a boolean flag indicating if an exception should be supressed. "
    }, {
    "id": 183,
    "url": "https://www.tomordonez.com/screen-recording-linux-obs-studio/",
    "title": "Screen Recording in Linux with OBS Studio",
    "body": "2018/10/04 - OBS Studio is an open source software for screen recording. Applicable to Windows, MAC and Linux. Installing OBS Studio on Linux: I am on Linux Fedora. These are the two commands to follow: sudo dnf install https://download1. rpmfusion. org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora). noarch. rpm https://download1. rpmfusion. org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora). noarch. rpmThen install with: sudo dnf install obs-studioOutput shows: Installing: obs-studioInstalling dependencies: obs-studio-libs x264Total download size: 4. 1 MInstalled size: 11 MOBS Studio Configuration: Once you open the program, it might have a popup window called “Auto-Configuration Wizard”. If you tap on “Yes”. Usage information:  Optimize for streaming, recording is secondary.  Optimize for recording, I will not be streaming. I chose option 2. Video settings:  Base (Canvas) Resolution: Use current 1366x768 FPS: Either 60 or 30, but prefer 60 when possible. Those were the default values. Final results:  The program is now executing a set of tests. Nothing to do here. Just wait for the test to be completed. Then “Apply Settings”. Adding Screen, Camera and Audio: Add the Screen:  Inside the Sources window.  Tap the plus sign.  Select Screen Capture Create new. A popup window shows:  Properties for “Screen Capture” Capture Cursor: Selected by default Tap on OKAdd the camera:  Sources window.  Plus sign.  Video capture device.  Pop window.  Review defaults.  OK Reisize camera layer. Add audio:  Desktop audio is already added. Mute or leave the default.  Mic is also already added.  For the Mic, go to the wheel to open settings.  Choose Filters.  Filters window. Plus sign to add a new filter.  Select compressor. Review defaults.  Add new filter. Noise supression.  You can play with these filter settings once you do some screen capture testing. Controls settings:  Bottom right. Controls menu. Settings.  Output. Recording path. Leave default or modify. I like to change to my Videos folder.  Output. Recording format. Default is flv. I like mp4 instead. Choosing this will show a warning.  Hotkeys. Start Recording. I like to use Ctrl+Shift+R. Then set Stop Recording to the same. Apply and OK Record the screen: Test screen capture:  Bottom right menu Start recording Test what you want to capture Test the mic Stop recording Review video Modify Mic settings. "
    }, {
    "id": 184,
    "url": "https://www.tomordonez.com/python-logging-tutorial/",
    "title": "Python Logging Tutorial",
    "body": "2018/09/29 - This is a short tutorial about logging in Python. Follow the official documentation about logging This tutorial by Fang-Pen Lin has been really helpful as well. More here Add logging to Python:: from datetime import datetimeimport logginglogger = logging. getLogger(__name__)logger. setLevel(logging. DEBUG)fh = logging. FileHandler('output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter('%(asctime)s  - %(name)s - %(levelname)s - %(message)s')fh. setFormatter(formatter)logger. addHandler(fh)Replace a print statement with a logger. info. if len(x) == 1 and len(y) == 1: logger. info('%s and %s are same length.   Return %s * %s',x,y,x,y)Don’t use format for string concatenation. The docs for Python3 say “This is for backwards compatibility: the logging package pre-dates newer formatting options such as str. format()” Logging errors:: Follow this example: try:  logger. info('Compute ac = product(%s, %s)',a,c)  ac = product(a, c)  logger. info('Product(a,c): %s, %s = %s',a,c,ac)except(SystemExit, KeyboardInterrupt):  raiseexcept Exception:  logger. error('Cannot compute ac', exc_info=True)More info aka The Details: From the official docs. These are the logging objects:  Loggers: create log records.  Handlers: send the log records to an output.  Filters: specify which log records to output.  Formatters: specify layout of log records. An event is a descriptive message that can contain variable data. Loggers: create log records: The class is logging. Logger. Loggers are not instantiated directly. Don’t use: logger = logging. Logger(). Instead use the module-level function: logger = logging. getLogger(__name__)Returns a logger with specified name __name__. logger. setLevel(): Use like this: logger. setLevel(logging. DEBUG)This is the list of logging levels:  CRITICAL: The program might stop running.  ERROR: The program might not run some function.  WARNING: The program might have problems in the future.  INFO: Program is working as expected.  DEBUG: Detailed information.  NOTSET. The default is set to WARNING. Only events of this level and above are tracked. If you have this: logging. warning('homer')logging. info('bart')It will ONLY print: WARNING:root:homer. Because the default is set to WARNING and events below this level such as INFO are ignored. For this reason I am using this to track events at DEBUG level and above: logger. setLevel(logging. DEBUG)logger. debug(): Logs a message with level debug. debug(msg, *args, **kwargs)Use the string formatting operator to concatenate msg and args. It has 3 keword arguments:  exc_info stack_info extraIf exc_info=True then exception information is added to the logger. Other logging level methods:  logger. info() logger. error()logger. addHandler(): Adds a specific handler to this logger: logger. addHandler(fh)Handlers: send the log records to an output: The class is logging. Handler. Not instantiated directly. fh. setLevel(logging. DEBUG)fh. setFormatter(formatter)Filters: specify which log records to output: TBA. I haven’t used this before. Please comment with an example. Formatters: specify layout of log records: Such as: formatter = logging. Formatter('%(asctime)s  - %(name)s - %(levelname)s - %(message)s')When to use logging: As seen on the official docs basic tutorial here. Use print() to display console output for ordinary usage. print('Welcome to my awesome program')Use logging. info() to report normal operation of the program. Use logging. warning() to issue a warning about a runtime event. Use logging. error() for error handling. Report supression of an error without raising an exception. Logging with many modules: Inside main: import loggingimport my_modulelogger = logging. getLogger()logger. setLevel(logging. DEBUG)fh = logging. FileHandler('output. log')fh. setLevel(logging. DEBUG)formatter = logging. Formatter('%(asctime)s  - %(name)s - %(levelname)s - %(message)s')fh. setFormatter(formatter)logger. addHandler(fh)Then use: logger. info() or logger. error() or the level that applies to the situtation. Inside my_module: import logginglogger = logging. getLogger(__name__)Used in the same way logger. info…etc. Logging and Classes: Similar way as shown above: Inside main: import loggingfrom my_module import Awesomedef main()  iam = Awesome()  iam. nerd()if __name__ ==  __main__ :  logger = logging. getLogger()  logger. setLevel(logging. DEBUG)  fh = logging. FileHandler('output. log')  fh. setLevel(logging. DEBUG)  formatter = logging. Formatter('%(asctime)s    - %(name)s - %(levelname)s - %(message)s')  fh. setFormatter(formatter)  logger. addHandler(fh)  main()Inside my_module: import loggingclass Awesome():  def __init__(self):    self. logger = logging. getLogger(__name__)  def nerd(self):    self. logger. info('I am awesome')Logging to STDOUT: If you wish to also log to STDOUT. Then create a new handler. Modifying the main file above. It should look like this: import sysif __name__ ==  __main__ :  logger = logging. getLogger()  logger. setLevel(logging. DEBUG)  formatter = logging. Formatter('%(asctime)s    - %(name)s - %(levelname)s - %(message)s')  fh = logging. FileHandler('output. log')  fh. setLevel(logging. DEBUG)  fh. setFormatter(formatter)  logger. addHandler(fh)  sh = logging. StreamHandler(sys. stdout)  sh. setLevel(logging. ERROR)  sh. setFormatter(formatter)  logger. addHandler(sh)You can add a handler called StreamHandler and send this to sys. stdout. And set the level to ERROR. You also have to import sys. Closing handlers: At the end of your program. You can close the handlers like this: for handler in logger. handlers:  handler. close()  logger. removeHandler(handler)logger. handlers contains a list of handler objects. Config files: This answer on StackOverflow says that it’s recommended to use config files for logging. "
    }, {
    "id": 185,
    "url": "https://www.tomordonez.com/reload-tmuxinator/",
    "title": "How to Reload Tmuxinator",
    "body": "2018/08/29 - This is how I reload Tmuxinator if I want to change my dashboard without having to exit all the panes. Edit the yml Tmuxinator config project file: $ tmuxinator open your-projectSave the config file and close. Stop the project session: $ tmuxinator stop your-projectThen start the project again: $ tmuxinator your-projectNow the session is reloaded with the new config changes. "
    }, {
    "id": 186,
    "url": "https://www.tomordonez.com/install-sphinx-and-readthedocs/",
    "title": "How to Install Sphinx and ReadTheDocs",
    "body": "2018/08/29 - How to install Sphinx and deploy to ReadTheDocs. Sphinx is used to write technical documentation, books or a structured document.  In a previous article I wrote about moving from Markdown to Sphinx reStructuredText. Go to Readthedocs to see a preview of what I wanted to achieve. It has a table of contents on the sidebar and the content on the right. It’s also mobile friendly.  This blog uses Markdown but I have been looking to use something else to keep my diary of my studying. I have been taking online classes for the last year and I take notes using Markdown. I started school this semester and wanted to be more organized and have a way to deploy my notes somewhere. I followed this video to learn how to install Sphinx. Which is used to write Python documentation. But you can use it to write any type of technical documentation. The output formats for Sphinx are:  HTML LaTeX (for printable PDF) ePubThe official Sphinx site has more details here. How to Install Sphinx: Create a github repo for your project. The repo won’t have any code. You can use the README file as a placeholder and will just have a link to the readthedocs site. I like to create an outer directory called docs-repo that contains these:  docs directory env README. md . gitignoreCreate the outer directory: $ mkdir docs-repo$ cd docs-repoCreate a docs directory: $ mkdir docsCreate and activate a virtual environment $ virtualenv -p /usr/bin/python3 env$ source env/bin/activateCreate a . gitignore and add: env/Initialize: $ git init$ git remote add origin your-awesome-repo. gitInstall Sphinx: $ pwd. . /docs-repo/$ pip install sphinx sphinx-autobuildGo inside docs and start sphinx: $ cd docs$ sphinx-quickstartThis will start the config file and it will ask some questions. The first question was related to selecting the root path. Because I was already inside the docs directory. The first option was set to selected root path: . . Then I selected these options:  Separate source and build directories: y Name prefix for templates and static dir: n Project name: (enter the project name you want) Author name: (your name) Project release: (up to you) Project language[en]: (hit enter for default) Source file suffix [. rst]: (hit enter for default) Name your master document (without suffix) [index]: (hit enter for default) Do you want to use the epub builder: n autodoc: automatically insert docstrings from modules: yFor the rest I hit enter for the defaults. Then these:  Create Makefile: y Create windows command file: nFiles Created: After the questions it created these files: . /source/conf. py. /source/index. rst. /MakefileList the directories: $ lsbuild Makefile sourceOpen the conf. py file: $ vim source/conf. pyThis file shows all the options we configured. Build the Documentation: $ make htmlRunning Sphinx v1. 7. 7making output directory…loading pickled environment… not yet createdbuilding [mo]: targets for 0 po files that are out of datebuilding [html]: targets for 1 source files that are out of dateupdating environment: 1 added, 0 changed, 0 removedreading sources… [100%] indexlooking for now-outdated files… none foundpickling environment… donechecking consistency… donepreparing documents… donewriting output… [100%] indexgenerating indices… genindexwriting additional pages… searchcopying static files… donecopying extra files… donedumping search index in English (code: en) … donedumping object inventory… donebuild succeeded. index. html: $ ls build/html/index. htmlOpen this file to review on the browser. I am on Linux. If you are on something else. Use your specific command or just browse to the file and open it. $ xdg-open build/html/index. htmlindex. rst: $ ls source/index. rstOpen this file to review: $ vim source/index. rstMake changes, then build again: $ make htmlReload browser. Here is the source code of index. rst for readthedocs. I built my own index. rst based on this file. See my file at the end. Github: Push to Github $ pwd. . /docs-repo/docs/$ cd . . $ lsdocs env README. md$ git add . $ git commit -m  first commit for docs $ git push -u origin masterDeploy Sphinx to Readthedocs: Go to your Readthedocs dashboard and go to “Import a Project”.  If you are not connected to Github it will ask to do so.  When you refresh, it will show a list of your repos.  Select the repo.  Edit Project Details.  It should take some moment to build the docs. You can also set your repo and your readthedocs to private. My index. rst file: It took me a while to get this right. I wanted to have this structure: Course Name  intro Notes Book Udacity Reading Research SoftwareCourse Name  same …If you are not familiar with reStructuredText there are a lot of resources online. But here are a few tips: Adding Links: The format I am following is adding a link on the paragraph and adding the reference right after the paragraph. Readthedocs follows the same process. More `about me`_ and `my blog`_. Blablamore blabla. . . _about me: https://the-url-here. . _my blog: https://another-url-hereFor my index I wanted the list of courses in bullet points and have this built into my sidebar Table of Contents.  Course 1 Course 2 …Follow this syntax to build bullet points: :ref:`Course 1`:ref:`Course 1`Follow this syntax to build the Table of Contents: . . _Course 1:. . toctree::  :maxdepth: 2  :caption: Course 1  intro &lt;course1/index&gt;  course1/notes  course1/book/index  course1/udacity  course1/reading  course1/research  course1/softwareThe code follows the same tabbed whitespace as writing in Python. To be more organized I wanted each Course in its own directory. That’s why you see this course1/notes. The Table of Contents is built with the first headline found under course1/notes. In the case of that syntax. For course1/book/index. It takes the first headline inside that index file. This one has a subdirectory for book to organize it into chapters. For intro &lt;course1/index&gt;. intro is used as the name element in the table of contents. To test if you built your index. rst file correctly. Just run: $ make htmlIt will read the index. rst file in your root path and it will show errors in the output. Then open the build/html/index. html file to preview. Troubleshooting: Menu options showing statics instead of caption names: When you add the toctree to your index. rst. Such as: . . toctree::maxdepth: 2:caption: Course 1intro &lt;course1/index&gt;course1/notesIt seems to have a glitch when you click on a menu option. It will show the name of the file instead of the caption. In this example: intro &lt;course1/index&gt;It should have the URL pointing to course1/index. rst with the caption intro. I saw that upon initial configuration and creating the root index. rst. The menu options show up with the right caption. But when clicking on an option on the menu. The caption names would change to the file name. This is pretty confusing in case you have secondary indices: Course1 subindex1 subindex2Course2 subindex1 subindex2I don’t have a clear resolution for this but what I did was making sure that each subindex had: A title=======A text. Followed by other types of text* such as bullet pointI initially had this: A title=======* Bullet pointsI added a text before the bullet points then make html. Then it displayed the correct captions for the URL make html: When you run this command, it might show errors. Just read the output to correct those errors and then run again. "
    }, {
    "id": 187,
    "url": "https://www.tomordonez.com/from-markdown-to-sphinx-restructuredtext/",
    "title": "From Markdown to Sphinx reStructuredText",
    "body": "2018/08/23 - For the last year I have been taking online courses in Coursera and reading books in programming and computer science. What I usually do to keep track of learning is to create a directory for a topic and put all the materials there. For instance when I took Python in Coursera I did this:  Created a directory Added the course book Created a venv Added the code Added a README file with my daily learningREADME: I used the README file as my diary. The good thing about online learning is that you can watch the video material as many times as you want or pause it and play it. I used books to support my learning. And used the README file in the same way that you would use a notebook to write my lesson. I believe that if you write things down you learn more and you can go back to this material as a reminder. Why I love markdown: I love Markdown because it is simple. Just as I love Vim. For Vim I have a simple setup and I know a few simple commands. I know how to exit a Vim file :) This website uses markdown and I am writing in markdown right now. But I never go back to the markdown source code to read my blog posts. All this content is pushed online. I consult all my blog posts online. Learning README and Markdown: This has been a problem for learning and keeping a diary. All those topics I have been studying are in README files in Markdown. They turn out to be very long files and they are hard to read in Vim. I have been looking for something better. Readthedocs: I went to Pycon last year and met the founder of Readthedocs. I honestly had no idea what that was until the conference was over, did some homework and saw how much technical documentation was used for this. That moment when you realize “I met the founder of X and I asked him what his (awesome_fill_in_the_blanks_technology) was about”. I saw that many Python projects use Readthedocs for documentation and for the following months I thought of reviewing how to learn this. But then I didn’t. Georgia Tech: I recently started at Georgia Tech and I am taking Computer Networks. I think a key to success is to be very organized. I am using TaskWarrior for task management. Following the same way to organize the files. I have a directory of courses and a README file. For this class I have two. One for my diary and another one for the book. Sphinx: I went back to readthedocs and found this blog post about why you shouldn’t use Markdown. And realized “hey that’s the guy I met”. I watched this screencast on how to get started with Sphinx and reStructuredText. The video shows how to install Sphinx, a bit about reStructuredText and how to push to readthedocs. One thing I have to be careful is that for my README file I have been adding a lot of code. For my class. I cannot have any homework code out there in public. I thought of having a private repo. But then I thought that my diary could help someone new taking the class. I am in the process of doing this:  Install Sphinx Add lessons learned and reference code only to the docs Keep homework code outside of the docs Push to Github/readthedocs diary/reference code onlyWith reference code I mean code that is from a libary official doc or public code and that is not related to a project or a specific solution for the class. "
    }, {
    "id": 188,
    "url": "https://www.tomordonez.com/taskwarrior-task-management/",
    "title": "TaskWarrior Task Management",
    "body": "2018/08/21 - I have been using a lot of task management over the years, including a lot of project management software. I wanted something simple that I could open from the Terminal. I am a big fan of CLI tools and found TaskWarrior. Which I have been using for the last few years. Taskwarrior looks like this: You can install Taskwarrior on Windows, Mac or Linux. For Ubuntu: $ sudo apt install taskwarriorIn Linux Fedora: $ dnf install taskwarriorOn Mac: $ brew install task$ brew install taskd$ brew install taskshFor Windows. The easiest is to install first Cygwin, a Linux-like interface to simulate the Terminal. In one of the install steps it will ask you to choose which modules to install. Just select the one that says task. Once installed, it is very easy to use if you know the commands. Using TaskWarrior: To add a task I use this: $ task add project:python Write awesome lesson learned due:todayTo update a task I use this: $ task 1 modify due:mondayTo get a list of tasks: $ task listTo complete a task: $ task 1 doneThere are all sorts of combinations in the official doc here.  Syntax Best practices Examples Searching Reports Filters TagsBash Script: I saw that often I had to do this: $ clear$ task listI wanted to see the task list dashboard with just one word. Created a bin directory in my home folder: $ mkdir ~/bin/Created a file called work $ cd ~/bin$ touch work$ chmod +x workInside the file I just added this: #!/usr/bin/bashclear &amp;&amp; task list"
    }, {
    "id": 189,
    "url": "https://www.tomordonez.com/wget-download-files/",
    "title": "Wget to download files",
    "body": "2018/07/15 - This is how you use wget to download files from the shell. For example. Download pdf files from a site: wget -r -l1 -A pdf --random-wait -e robots=off -U mozilla -nd -np -nc URLThis is what this means:  -r: recursive -l1: level 1. Only the current directory -A pdf: Only pdf files --random-wait -e robots=off -U mozilla: A good practice to appear human.  -nd: No directories -np: No parent directories -nc: “no clobber”. This means only download files if they are newer or you didn’t download them yet. "
    }, {
    "id": 190,
    "url": "https://www.tomordonez.com/r-tutorial-rstudio-data-analysis/",
    "title": "R Tutorial with Rstudio and Data Analysis",
    "body": "2018/07/15 - This is an R tutorial with Rstudio and the data analysis from an ozone dataset. R tutorial with Rstudio: Rstudio is an open source software for R, a programming language for statistical computing. The Environmental Protection Agency (EPA) website has data sets about ozone levels. I downloaded 2 data sets from their website:  Hourly ozone levels in the US from 2016. The zip file was 69MB. Uncompressed, the csv file is 2. 1GB.  Daily AQI by county from 2016. The zip file was 1. 6MB and the csv file was 26. 4MB. AQI stands for Air Quality Index. A measure developed by EPA to explain pollution levels to the general public. Data Visualization: This is the data visualization that we are getting from this R tutorial using Rstudio. The best time to run outside in San Francisco.  A bit more context about Ozone: To do exploratory data analysis from a data set you need to have a general understanding what the data is about.  Ozone is a pale blue gas with a bad smell.  Ozone odour is sharp, smells like chlorine and is measured in parts per billion (ppb).  Concentrations of 100ppb and above damages respiratory tissues.  Ozone layer is a portion of the stratosphere. About 20 miles above the ground. At about 100,000 feet.  The ozone layer prevents UV light from reaching the surface. Which is actually good. But ozone at surface level. Not good.  Bad ozone is due to fossil fuel burning. Loading the dataset into Rstudio:  File &gt; New Project New Directory Empty Project Enter name and browse to subdirectoryInstall readr: If you don’t have the package readr you need to install it. It will take about 5 minutes: &gt; install. packages( readr )The library readr is needed to read rectangular data such as csv. Then load the library: &gt; library(readr)Load the dataset CSV file with read_csv: Load the CSV file in Rstudio and create an object. The function read_csv converts the csv file into a data frame. If you don’t specify the types for columns, strange things happen. For example the data set has a column called Time. Local, which by default is of type integer. If you run a plot using Time. Local you will get the time in seconds instead of hours, such as: 3600, representing 1am7200, representing 2amand so on. . . The column type has to be set to c character so that Time. Local can take values like this: 00:0001:0002:00and so on. . . Setting the column types depend on the columns. But how do you set them up if you haven’t loaded the data? The data source should have a description of the data. In this case there is more information about this data set on the EPA website. Here is the line to load the CSV into the object ozone. &gt; ozone &lt;- read_csv( ozone_data_2016. csv , col_types =  ccccinnccccccncnnccccccc )Get the names of the columns: If you call this function: names(ozone) you get this result: &gt; names(ozone) [1]  State Code     County Code       Site Num       [4]  Parameter Code   POC           Latitude       [7]  Longitude      Datum          Parameter Name    [10]  Date Local     Time Local       Date GMT       [13]  Time GMT      Sample Measurement   Units of Measure   [16]  MDL         Uncertainty       Qualifier      [19]  Method Type     Method Code       Method Name     [22]  State Name     County Name       Date of Last Change By definition names is used to get or set the name of an object. If the object is a dataframe it gets the names of the columns. Normalize the column names: If you call this function: make. names(names(ozone)) you get this result: &gt; make. names(names(ozone)) [1]  State. Code     County. Code       Site. Num       [4]  Parameter. Code   POC           Latitude       [7]  Longitude      Datum          Parameter. Name    [10]  Date. Local     Time. Local       Date. GMT       [13]  Time. GMT      Sample. Measurement   Units. of. Measure   [16]  MDL         Uncertainty       Qualifier      [19]  Method. Type     Method. Code       Method. Name     [22]  State. Name     County. Name       Date. of. Last. Change By definition make. names is used to make valid names out of character vectors. The help page explains that syntactically valid names consist of letters, numbers and dot or underline and start with a letter or the dot but not followed by a number. For instance . 2file is not a valid name. To normalize the column names, and replace spaces with periods, use: make. names(names(ozone)). Replace the names of columns: Finally, this is used to set this conversion back to the names object: &gt; names(ozone) &lt;- make. names(names(ozone))Now, when you call names(ozone) the spaces are replaced by dots. &gt; names(ozone) [1]  State. Code     County. Code       Site. Num       [4]  Parameter. Code   POC           Latitude       [7]  Longitude      Datum          Parameter. Name    [10]  Date. Local     Time. Local       Date. GMT       [13]  Time. GMT      Sample. Measurement   Units. of. Measure   [16]  MDL         Uncertainty       Qualifier      [19]  Method. Type     Method. Code       Method. Name     [22]  State. Name     County. Name       Date. of. Last. Change Explore the data in Rstudio: Check the number of rows: &gt; nrow(ozone)[1] 9124268Check the number of columns: &gt; ncol(ozone)[1] 24Check the top of the file: &gt; head(ozone[, c(6:7, 10)])# A tibble: 6 x 3Latitude Longitude Date. Local   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   1   30. 5   -87. 9 2016-03-012   30. 5   -87. 9 2016-03-013   30. 5   -87. 9 2016-03-014   30. 5   -87. 9 2016-03-015   30. 5   -87. 9 2016-03-016   30. 5   -87. 9 2016-03-01Check the bottom of the file: &gt; tail(ozone[, c(6:7, 10)])# A tibble: 6 x 3Latitude Longitude Date. Local  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   1   18. 2   -65. 9 2016-12-312   18. 2   -65. 9 2016-12-313   18. 2   -65. 9 2016-12-314   18. 2   -65. 9 2016-12-315   18. 2   -65. 9 2016-12-316   18. 2   -65. 9 2016-12-31Check the number of records for each hour: Using the table function in Rstudio. This function “uses the cross classifying factors to build a contingency table of counts at each combination of factor levels”. In this case I know that the ozone levels are measured every hour from midnight to noon to 11pm. I would like to know the number of records for each hour. table(ozone$Time. Local)This results in: 00:00 01:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 369489 373144 359722 358576 363421 385829 386222 384176 381303 09:00 10:00 11:00 12:00 13:00 14:00 15:00 16:00379689 379424 380814 381202 383439 385152 386580 38756517:00 18:00 19:00 20:00 21:00 22:00 23:00 388252 388576 388656 388650 387492 377278 379617What does this data mean?: I selected and filtered parts of the data to understand more about it. Was the data grouped by state? Or was the data grouped by hour? Install dplyr for data frames in Rstudio: The next step requires to use the library dplyr. Which is the next iteration of plyr, a set of tools for splitting, applying and combining data. dplyr “provides a flexible grammar of data manipulation…focused on tools for working with data frames”. &gt; install. packages( dplyr )This might ask to restart the R session before installing. Then try again. Now load the library. &gt; library(dplyr)US States in the data set: I wanted to see which states were included in the data set I loaded. &gt; select(ozone, State. Name) %&gt;% unique# A tibble: 52 x 1State. Name       &lt;chr&gt;        1 Alabama       2 Alaska        3 Arizona       4 Arkansas       5 California      6 Colorado       7 Connecticut     8 Delaware       9 District Of Columbia10 Florida       # . . . with 42 more rowsOzone levels in Florida: I filtered the data set to see the ozone levels in Florida on July 9, 2016 at 6am for all counties &gt; filter(ozone, State. Name ==  Florida , Date. Local ==  2016-07-09 , Time. Local ==  06:00 ) %&gt;% select(County. Name,   Sample. Measurement)The result was: # A tibble: 58 x 2  County. Name Sample. Measurement     &lt;chr&gt;       &lt;dbl&gt; 1   Alachua       0. 002 2    Baker       0. 001 3     Bay       0. 017 4   Brevard       0. 014 5   Brevard       0. 016 6   Broward       0. 005 7   Broward       0. 004 8   Broward       0. 004 9   Broward       0. 00810   Collier       0. 004# . . . with 48 more rowsConvert PPM (parts per million) to PPM (per billion): The sample measurement has units of ppm (parts per million). To convert to ppb (parts per billion) you just move the dot three times to the right. For instance, the first row that says 0. 002 ppm converts to 2 ppb. I learned that the EPA has an AQI calculator to convert from ppb to AQI (air quality index). In the AQI calculator you have a few choices as shown here:  Select a pollutant: O3 - Ozone (8hr avg) or (1hr avg) Units required: ppb Enter the concentration CalculateThe way to choose between 8hr and 1hr is: AQI values of 301 or greater are calculated with 1-hr ozone concentrations. AQI Categories: The EPA has a great website to learn more about the Air Quality Index (AQI). The AQI is categorized like this:  0-50. Good. Green 51-100. Moderate. Yellow 101-150. Unhealthy for sensitive groups. Orange 151-200. Unhealthy. Red 201-300. Very unhealthy. Purple 301-500. Hazardous. Maroon. This information was very helpful to understand the data Very Unhealthy Jefferson county, Alabama: I filtered the data to find ozone measurements above 0. 100 ppm (aka 100ppb). Which calculates to an AQI of 187 (Unhealthy). &gt; filter(ozone, Sample. Measurement &gt; 0. 1) %&gt;% select(State. Name, County. Name, Date. Local,   Time. Local, Sample. Measurement)# A tibble: 1,407 x 5State. Name  County. Name Date. Local Time. Local Sample. Measurement.   &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;   &lt;time&gt;     &lt;dbl&gt;1  Alabama  Jefferson  2016-02-23  12:00:00  0. 3462  Alabama  Jefferson  2016-02-23  13:00:00  0. 2023  Alabama  Russell   2016-04-18  14:00:00  0. 1014  Alaska  Matanuska  2016-04-08  02:00:00  0. 1025  Arizona  Gila    2016-11-26  17:00:00  0. 1456  Arkansas Crittenden 2016-06-10  13:00:00  0. 1077  Arkansas Crittenden 2016-06-10  14:00:00  0. 1138  Arkansas Crittenden 2016-06-10  15:00:00  0. 1049 California Alameda   2016-06-03  15:00:00  0. 10210 California Alameda   2016-07-26  15:00:00  0. 102# . . . with 1,575 more rowsThis shows that on February 23, 2016 at noon, there was a measurement of 346ppb in Jefferson county, Alabama. Converted to AQI results in 276 aka “Very Unhealthy” and very close to “Hazardous”, which is described as “Health warnings of emergency conditions. The entire population is more likely to be affected. ” After a quick Google search I found that Jefferson county is amongst the top most polluted counties in the US.  Exploring the data visualization plots in Rstudio: I know the names of some state counties in Florida. For example, for Miami, I know that the county is called “Miami Dade” but I wasn’t sure how they added this value into the data set. If they put “Dade” or “Miami” or what combination of Miami and Dade. For each state I created vectors with the names of counties. I visualized the vectors into a table and then created a filter for the specific county. Finally, I plotted Time vs Sample Measurement. I created a vector with Florida counties: &gt; florida. counties &lt;- filter(ozone,   State. Name ==  Florida ) %&gt;%   select(County. Name) %&gt;% uniqueThen created a view: &gt; View(florida. counties) On the view I saw they had it as Miami-Dade. &gt; ozone. miami. 2016 &lt;- filter(ozone,   County. Name ==  Miami-Dade ) %&gt;%   select(Date. Local, Time. Local,     Sample. Measurement)I filtered by county name and then plotted Time. Local vs Sample. Measurement. &gt; plot(ozone. miami. 2016$Time. Local,   ozone. miami. 2016$Sample. Measurement)The output was an error: Error in plot. window(. . . ) : need finite 'xlim' valuesIn addition: Warning messages:1: In xy. coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion2: In min(x) : no non-missing arguments to min; returning Inf3: In max(x) : no non-missing arguments to max; returning -InfThe error says that one of the coordinates is a vector with NAs. I looked up the types for each variable: &gt; class(ozone. miami. 2016$Time. Local)[1]  character &gt; class(ozone. miami. 2016$Sample. Measurement)[1]  numeric Time. Local is of type character and Sample. Measurement is of type numeric. &gt; head(ozone$Time. Local)[1]  15:00   16:00   17:00   18:00   19:00   20:00 &gt; head(ozone$Sample. Measurement)[1] 0. 041 0. 041 0. 042 0. 041 0. 038 0. 038You cannot plot character vs numeric. To prove that Time. Local was a vector with NAs I used the function strptime. The function strptime is used to convert between character representations and objects of classes “POSIXlt” and “POSIXct” representing calendar dates and times. I read more about POSIXt and it seems to be a complicated topic as described here. The strptime function is used like this: strptime(x, format, tz =   ) x: An object to be converted: a character vectorfor strptime, an object which can be converted to POSIXlt  for strftime. format: A character string. The default forthe format methods is  %Y-%m-%d %H:%M:%S . tz: A character string specifying thetime zone to be used for the conversionI ran the strptime function like this: &gt; strptime(ozone. miami. 2016$Time. Local, '%H:%M:%S')The result was something like this: [1] NA NA NA NA NA NA NANA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NAAs far as the error saying: In xy. coords(x, y, xlabel, ylabel, log) : NAs introduced by coercionI assume that plot is trying to make Time. Local which is a character element into numeric data. And when it fails to do this, it is returning NAs. Normalizing data in Rstudio: There is a way to plot Time. Local as a factor. &gt; plot(factor(ozone. miami. 2016$Time. Local), ozone. miami. 2016$Sample. Measurement, main =  Best Time to Run in Miami , xlab =  Hour , ylab =  Ozone level in ppm )I suffered a bit with lower case and upper case variables. To get the list of State. Name I did: &gt; unique(ozone$State. Name) [1]  Alabama     Alaska     Arizona        [4]  Arkansas     California   Colorado        [7]  Connecticut   Delaware    District Of Columbia [10]  Florida     Georgia To get the County. Name for District of Columbia I did this but didn’t get any results: &gt; filter(ozone, State. Name ==  District of Columbia ) %&gt;% select(County. Name) %&gt;% unique# A tibble: 0 x 1# . . . with 1 variables: County. Name &lt;chr&gt;I realized that I didn’t use the exact case. Instead of Of, I used of. &gt; filter(ozone, State. Name ==  District Of Columbia ) %&gt;% select(County. Name) %&gt;% unique# A tibble: 1 x 1      County. Name         &lt;chr&gt;1 District of ColumbiaI figured that normalizing the data is a pain in the butt. I followed the same process to plot Miami, Washington DC, San Francisco and Los Angeles. Creating Data Visualization in Rstudio: Get a list of States: unique(ozone$State. Name)Find the County Name. This will give you a table of County names: florida. counties &lt;- filter(ozone, State. Name ==  Florida ) %&gt;% select(County. Name) %&gt;% uniqueView(florida. counties)In this case I found the county for Miami Dade to be Miami-Dade. Filter the data and create an object: ozone. miami. 2016 &lt;- filter(ozone, County. Name ==  Miami-Dade ) %&gt;% select(Date. Local, Time. Local, Sample. Measurement)Create a plot with this object and add titles: plot(factor(ozone. miami. 2016$Time. Local), ozone. miami. 2016$Sample. Measurement, main =  Best Time to Run in Miami , xlab =  Hour , ylab =  Ozone level in ppm )Creating the plot for San Francisco: ca. counties &lt;- filter(ozone, State. Name ==    California ) %&gt;% select(County. Name)   %&gt;% uniqueView(ca. counties)ozone. sf. 2016 &lt;- filter(ozone, County. Name ==    San Francisco ) %&gt;% select(Date. Local,     Time. Local, Sample. Measurement)plot(factor(ozone. sf. 2016$Time. Local), ozone. sf. 2016$Sample. Measurement, main =  Best Time to Run in San Francisco , xlab =  Hour , ylab =  Ozone level in ppm ) &gt; summary(ozone. sf. 2016$Sample. Measurement)  Min. 1st Qu.  Median  Mean 3rd Qu.   Max. 0. 00000 0. 01600 0. 02300 0. 02243 0. 03100 0. 07000Ozone levels in San Francisco:  The lowest ozone levels in San Francisco are between 5am and 7am.  The highest ozone levels are between 12pm and 3pm. "
    }, {
    "id": 191,
    "url": "https://www.tomordonez.com/install-plugin-vim/",
    "title": "Install a Plugin in Vim",
    "body": "2018/07/14 - A short one on how to install a plugin in Vim. Install Vundle. More details here. git clone https://github. com/VundleVim/Vundle. vim. git ~/. vim/bundle/Vundle. vimEdit the . vimrc file and add these lines: set nocompatiblefiletype off  set the runtime path to include Vundle and initializeset rtp+=~/. vim/bundle/Vundle. vimcall vundle#begin()  let Vundle manage Vundle, requiredPlugin 'VundleVim/Vundle. vim'  All of your Plugins must be added before the following linecall vundle#end()        requiredfiletype plugin indent on    requiredThis is how I install Vim plugins:  Go to a plugin that says you can install using Vundle.  For instance. This plugin auto saves when you are editing a file: auto-save.  The Github project should have a directory called plugin.  Add the user/project from Github to your . vimrc file. In the auto-save example. The user is 907th and the project is vim-auto-save. I will add this line to my . vimrc file: Plugin 907th/vim-auto-save before a specific line as noted above.  Save vimrc. Source it with :so %.  List all Vundle plugins. Still inside Vim: :PluginList. Make sure the Plugin is listed. You can do :q to exit this view.  Inside Vim: :PluginInstall. This will install recently added Plugins. This particular Plugin for auto-save had a few more lines that had to be added. Depending on the Plugin readme page, please follow those specific instructions. For auto-save. You need to add these lines to the . vimrc file: let g:auto_save = 1let g:auto_save_in_insert_mode = 0Here is a list of Vim plugins: http://vim-scripts. org/vim/scripts. html"
    }, {
    "id": 192,
    "url": "https://www.tomordonez.com/linkedin-recruiter-bookmarklet/",
    "title": "Linkedin Recruiter Bookmarklet",
    "body": "2018/07/13 - This is a sourcing trick that helps you in Linkedin recruiter. This script comes from Getting Started in Custom Programming for Sourcing Purposes” from the 1st programmers track session ever at SourceCon in Austin in Sept 2017. This is a presentation by my friends Glenn and Kameron. Where they explain how to use Javascript bookmarklets to improve the Linkedin recruiter experience.  Chrome Bookmark manager Add boomark For name enter the title For URL enter the codeLinkedin JS Project List View: Go to a Linkedin recruiter project. It shows pagination with 10 results per page. The original URL might have this format, followed by a number id: https://www. linkedin. com/recruiter/projects/12345678To convert to a longer list. Use this bookmarklet. javascript:(function(){var loc=location. href;loc=loc. replace('linkedin. com/recruiter/projects/','linkedin. com/cap/project/savedProfiles/');loc=loc+'?max=500';location. replace(loc)})()The output URL will change to this: https://www. linkedin. com/cap/project/savedProfiles/12345678?max=500The Javascript bookmarklet will find and replace a piece of the URL and add a number of results. It will find linkedin. com/recruiter/projects/ and replace with linkedin. com/cap/project/savedProfiles/. Then at the end add ?max=500. You can modify the number of results if you have a project with more than 500 profiles. For instance if you have a project with 567 profiles. You can modify to ?max=567 Troubleshooting the bookmarklet: What if this doesn’t work? It’s possible your “input URL” is different. For the bookmarklet above we are assuming the input URL is: linkedin. com/recruiter/projects/. But what if the URL is something like: https://linkedin. com/awesome/recruiter/projects/12345678Or what if it has parameters at the end: . . . . awesome/recruiter/projects/12345678?trk=homepage_v2Then the bookmarklet won’t work and you need to modify it. If it is just a different URL you can make the change here: loc. replace('YOUR INPUT URL','linkedin. com/cap/project/savedProfiles/')If the Input URL has a question mark at the end with parameters: . . . awesome/recruiter/projects/12345678?trk=homepage_v2Then you need to use a regular expression to remove that part of the URL: replace(/\?. +/g,'')You don’t have to understand this too much. But this means: find a question mark, then at least 1 or more (any) character after. And replace with nothing. Here is a modified bookmarklet if your input URL has ? question mark parameters. javascript:(function(){var%20loc=location. href;loc=loc. replace(/\?. +/g,'');loc=loc. replace('linkedin. com/recruiter/projects/','linkedin. com/cap/project/savedProfiles/');loc=loc+'?max=500';location. replace(loc)})()Linkedin JS Results 250: This is a bookmarklet that might not work anymore. I remember the “list view” having a restriction of showing only up to 250 results and then paginating more results. I am leaving this here for reference: javascript: (function(){var loc = location. href;loc = loc. replace('&amp;page=1&amp;start=0&amp;count=25', '&amp;page=1&amp;start=0');loc = loc + '&amp;count=250';location. replace(loc) })()"
    }, {
    "id": 193,
    "url": "https://www.tomordonez.com/conference-mobile-apps/",
    "title": "Conference Mobile Apps",
    "body": "2018/07/12 - I am making a list of conference mobile apps. These are the apps they use at conferences :)  Whova Bizzabo CrowdCompass Gartner Events Eventbase Technology, Inc"
    }, {
    "id": 194,
    "url": "https://www.tomordonez.com/cpp-indent-vim/",
    "title": "Cpp Indent Vim",
    "body": "2018/06/24 - Given . vimrc: filetype plugin indent onTo indent C++ files in Vim. Create this directory: ~/. vim/after/ftplugin/Inside that directory create the file: ~/. vim/after/ftplugin/cpp. vimAdd the lines: set expandtabset shiftwidth=2set softtabstop=2That’s using 2 spaces for indentation like they do it at Google. When you create a file: vim awesome. cppIt will indent to 2 spaces: #include &lt;iostream&gt;using namespace std;int main(){ cout &lt;&lt;  2 spaces  &lt;&lt; endl; return 0;}"
    }, {
    "id": 195,
    "url": "https://www.tomordonez.com/dev-mapper-fedora-root-full/",
    "title": "Dev Mapper Fedora Root is Full",
    "body": "2018/06/04 - On Fedora Linux: /dev/mapper/fedora-root is Full $ df -h/dev/mapper/fedora-root  50G  46G 971M 98% /As seen on StackExchange. Check disk usage on /var as described in Fedora Forum $ sudo du -hs /var36G   /var$ sudo du -m /var | sort -nr | head -3036061  /var31152  /var/cache30768  /var/cache/PackageKit“Delete the cached data in this directory as described in Fedora 23 – Can I safely delete files in /var/cache/PackageKit/metadata/updates/packages?” $ sudo pkcon refresh force -c -1About pkcon:  PackageKit console client pkcon refresh [force]: Refresh the cached information about available updates.    -c, –cache-age AGE: Set the maximum acceptable age for cached metadata, in seconds. Use -1 for ‘never’.   Refreshing cache Loading cache Downloading repository information Loading cache Downloading repository information Loading cache Downloading repository information Finished  Check again: $ df -h/dev/mapper/fedora-root  50G  25G  23G 53% /"
    }, {
    "id": 196,
    "url": "https://www.tomordonez.com/python-socket-syntax/",
    "title": "Python Socket Syntax",
    "body": "2018/05/20 - As seen on late night TV…  Python3 LinuxThou shall: import socketsux = socket. socket(socket. AF_INET, socket. SOCK_STREAM)Did I dream this? I could be sure that I wrote this tutorial before but I cannot find it. To read the official docs about Python sockets go here. First you start with importing the module: import socketThen you need to create a socket object and pass some arguments:  socket. AF_INET: This means address family internet protocol v4 socket. SOCK_STREAM: This means a TCP socket. Other arguments: These are the default for IPv4 and TCP:  socket. AF_INET socket. SOCK_STREAMIf you want IPv6:  socket. AF_INET6If you want UDP:  socket. DGRAMDifferent socket families use different number of arguments:  AF_INET: A pair-tuple (host, port).  AF_INET6: A four-tuple (host, port, flowinfo, scopeid). You can also use bluetooth:  AF_BLUETOOTHClosing the socket: If you do this: import socketsux = socket. socket(socket. AF_INET, socket. SOCK_STREAM)sux. connect( (host, port) )Then you have to close it like this: sux. close()Keep in mind that the argument for IPv4 (AF_INET) is a pair-tuple: sux. connect( (host, port) )You don’t need the whitespace but it helps me remember that it needs a tuple. This won’t work: sux. connect(host, port)You can also open and close the socket like this: with socket. socket(socket. AF_INET, socket. SOCK_STREAM) as sux:  sux. connect( (host, port) )Which has a similar syntax as the open method. with open('simpsons. txt', 'r') as fhandle:Summary: The default:  AF_INET: IPv4 SOCK_STREAM: TCPUsing with. . . as: import socketwith socket. socket(socket. AF_INET, socket. SOCK_STREAM) as sux:  sux. connect( (host, port) )  . . .   something_awesome_here  . . . "
    }, {
    "id": 197,
    "url": "https://www.tomordonez.com/get-schema-sqlite-python/",
    "title": "Get Schema in SQLite with Python",
    "body": "2018/05/09 - This is how to get the schema in SQLite with Python. You can check if you already have SQLite installed: $ sqlite3If the prompt doesn’t change to sqlite&gt; then try the download options from the official docs here. Create a SQLite database: Let’s setup this tutorial by creating a SQLite database. I created a simple table and added one record: $ sqlite3sqlite&gt; CREATE TABLE Users(id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT);sqlite&gt; INSERT INTO Users(name, email) VALUES('homer', 'homer@simpson. com');sqlite&gt; SELECT * FROM Users;1|homer|homer@simpson. comYou can also create a SQLite database and table using a Python script: import sqlite3conn = sqlite3. connect('users. sqlite')cur = conn. cursor()cur. execute('CREATE TABLE Users(id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT')cur. execute('INSERT INTO Users(name, email) VALUES(?, ?)', ('homer', 'homer@simpson. com'))conn. commit()cur. close()Get the SQLite schema with PRAGMA: Given that the table name is Users: PRAGMA table_info('Users')PRAGMA is a SQL extension for SQLite.  A PRAGMA can take 0 or 1 argument.  The argument could be in parenthesis () or with an equal = The argument could be boolean: 1 yes true on, 0 no false off.  The argument could be a string literal.  There could be an optional schema-name before the PRAGMA name.  The schema-name is the name of the attached database or main or temp.  If the name is omitted, the defaul is main.  In some pragmas the schema-name is ignored. More details about PRAGMA on the official SQLite docs here SQLite schema syntax: The syntax is: PRAGMA schema. table_info(table-name);This one returns one row for each colum in the table. Each row includes:  column name data type whether or not the column can be NULL the default value for the column the primary key in the result set is 0 for columns that are not part of the primary key otherwise the primary key in the result set is the index of the columnGet Schema in SQLite with Python: Get the SQLite schema: PRAGMA table_info('Users')Let’s add it here: $ sqlite3sqlite&gt; CREATE TABLE Users(id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT);sqlite&gt; INSERT INTO Users(name, email) VALUES('homer', 'homer@simpson. com');sqlite&gt; SELECT * FROM Users;1|homer|homer@simpson. comsqlite&gt; PRAGMA table_info('Users')0|id|INTEGER|0||11|name|TEXT|0||02|email|TEXT|0||0Get the SQLite schema with Python: cur. execute( PRAGMA table_info('Courses') ). fetchall()Using the Python shell: &gt;&gt;&gt; import sqlite3&gt;&gt;&gt; conn = sqlite3. connect('users. sqlite')&gt;&gt;&gt; cur = conn. cursor()&gt;&gt;&gt; cur. execute('CREATE TABLE Users(id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT)')&gt;&gt;&gt; cur. execute('INSERT INTO Users(name, email) VALUES(?, ?)', ('homer', 'homer@simpson. com'))&gt;&gt;&gt; cur. execute( PRAGMA table_info('Users') )&lt;sqlite3. Cursor object at 0x7f590a00cea0&gt;&gt;&gt;&gt; cur. execute( PRAGMA table_info('Users') ). fetchall()[(0, 'id', 'INTEGER', 0, None, 1), (1, 'name', 'TEXT', 0, None, 0), (2, 'email', 'TEXT', 0, None, 0)]&gt;&gt;&gt; for row in cur. execute( PRAGMA table_info('Users') ). fetchall():. . .   print(row). . . (0, 'id', 'INTEGER', 0, None, 1)(1, 'name', 'TEXT', 0, None, 0)(2, 'email', 'TEXT', 0, None, 0)SQLite cheat sheet: SQLite cheat sheet for Python More SQLite resources:  Aggregate functions in SQLite Date in SQLite SQLite browser SQLitestudio SQLite manager"
    }, {
    "id": 198,
    "url": "https://www.tomordonez.com/sqlite3-cheatsheet/",
    "title": "SQLite3 CheatSheet for Python",
    "body": "2018/05/07 - This is an ongoing SQLite3 cheatsheet for Python. SQLite3 comes with Python and you can launch it with: $ sqlite3If you launch it without an argument, it will say Connected to a transient in-memory database. The prompt changes to: sqlite&gt; The . help is a good idea. sqlite&gt; . helpTo exit do . exit. sqlite&gt; . exitCreate a SQLite database: I find it better to launch SQLite passing the name of the database as an argument. $ sqlite3 sql1Now it says SQLite version. . . Enter . help for usage hints. Create a SQLite table: This is just your standard SQL. You can use uppercase or lowercase. But I prefer to follow the standard. sqlite&gt; CREATE TABLE Users (  . . . &gt; name VARCHAR(128),  . . . &gt; email VARCHAR(128)  . . . &gt; );I often forget the semicolon ;. It will give you an error if you forget. Check that a TABLE was created: sqlite&gt; . tablesUsersYou can also check the schema: sqlite&gt; . schemaCREATE TABLE Users (name VARCHAR(128),email VARCHAR(128));Add values to the SQLite table: Add some data to the table: sqlite&gt; INSERT INTO Users VALUES('homer', 'homer@simpson. com');sqlite&gt; INSERT INTO Users VALUES('marge', 'marge@simpson. com');sqlite&gt; INSERT INTO Users VALUES('lisa', 'lisa@simpson. com');sqlite&gt; INSERT INTO Users VALUES('bart', 'bart@simpson. com');sqlite&gt; INSERT INTO Users VALUES('maggie', 'maggie@simpson. com');Read data from SQLite table: This doesn’t work: sqlite&gt; Users;I wish that SQL was more user friendly with a variety of human-friendly options: Show me usersGive me usersWhat's in UsersInstead do: sqlite&gt; SELECT * FROM Users;homer|homer@simpson. commarge|marge@simpson. comlisa|lisa@simpson. combart|bart@simpson. commaggie|maggie@simpson. comsqlite&gt;Exit: sqlite&gt; . exitOpen again: $ sqlite3Connected to a transient in-memory database. Ooops forgot to open the right database. sqlite&gt; . open sql1sqlite&gt;sqlite&gt; . schemaCREATE TABLE Users(name VARCHAR(128),email VARCHAR(128));Looks like the right place. Export SQLite database table to CSV: I can’t always remember how to do this: sqlite&gt; . mode listsqlite&gt; . separator  , sqlite&gt; . output test_sql1. csvsqlite&gt; SELECT * FROM Users;sqlite&gt; . exitAlthough a good way to remember is to understand the workflow:  Convert to a list Separate with a comma Output to file name SQL statement   Exit sqlite3   $ cat test_sql1. csv homer,homer@simpson. com marge,marge@simpson. com lisa,lisa@simpson. com bart,bart@simpson. com maggie,maggie@simpson. com  Import CSV to SQLite database table: Let’s say we have a CSV called springfield. csv with some name, email values: apu,apu@springfield. commrburns,mrburns@springfield. commilhouse,milhouse@springfield. comned,ned@springfield. commoe,moe@springfield. comOpen SQLite and import: $ sqlite3 sql1sqlite&gt; . tablesUsersWorkflow:  mode CSV   import file into table   sqlite&gt; . mode csv sqlite&gt; . import springfield. csv Users sqlite&gt; SELECT * FROM Users; homer,homer@simpson. com marge,marge@simpson. com lisa,lisa@simpson. com bart,bart@simpson. com maggie,maggie@simpson. com apu,apu@springfield. com mrburns,mrburns@springfield. com milhouse,milhouse@springfield. com ned,ned@springfield. com moe,moe@springfield. com  Export SQLite Table to CSV…another way: sqlite&gt; . headers onsqlite&gt; . mode csvsqlite&gt; . once test2_sql1. csvsqlite&gt; SELECT * FROM Users;Workflow:  Add the header Set mode to CSV Output to static SQL commandIf you look at the . help you fill find what . once means: “Output for the next SQL command only to FILENAME”. Used like this: . once FILENAMEOpen the CSV without closing SQLite: If you are in Linux or maybe Mac(not sure): sqlite&gt; . system xdg-open test2_sql1. csvThis will open the CSV file in the default editor for CSV. Insert rows into SQLite table: Open the database: $ sqlite3 sql1sqlite&gt; . tablesUserssqlite&gt; . schemaCREATE TABLE Users (name VARCHAR(128),email VARCHAR(128));I find it useful to see what the tables and schema are. This is the same as before: sqlite&gt; INSERT INTO Users VALUES('bob', 'bob@springfield. com');Although this syntax is optional: sqlite&gt; INSERT INTO Users(name, email) VALUES('bob', 'bob@springfield. com');That one shows a comma-separated list of columns. Perhaps is a reminder of the data that you are inserting. I think is a good idea. Delete data from a SQLite table: In our example database we have name and email. Let’s delete the last row that has bob. sqlite&gt; DELETE FROM Users WHERE email='bob@springfield. com';Update a field from a SQLite table: Let’s update moe and change his email to moe@moestavern. com. sqlite&gt; UPDATE Users SET email='moe@moestavern. com' WHERE email='moe@springfield. com';Get records from a SQLite table: We already tried this too many times: sqlite&gt; SELECT * FROM Users;What about this one: sqlite&gt; SELECT * FROM Users WHERE name='bart';bart|bart@simpson. comGet records and sorting from a SQLite table: Use ORDER BY: sqlite&gt; SELECT * FROM Users ORDER BY name;sqlite&gt; SELECT * FROM Users ORDER BY email;Create a SQLite connection in Python: import sqlite3conn = sqlite3. connect('phantom. sqlite')cur = conn. cursor(). . . do something herecur. close()SQLite in Python: Let’s write a simple program called contacts. py that asks for names and emails: import sqlite3conn = sqlite3. connect('contacts. sqlite')cur = conn. cursor()cur. execute('DROP TABLE IF EXISTS Users')cur. execute('CREATE TABLE Users(name TEXT, email TEXT)')while True:  name = input('Enter name: ')  email = input('Enter email: ')  cur. execute('INSERT INTO Users(name, email) VALUES(?, ?)', (name, email))  conn. commit()  try:    more_values = input('Add more? (y/n): ')    if more_values == 'y':      continue    elif more_values == 'n':      break  except (KeyboardInterrupt, SystemExit):    raisecur. close()This line DROP TABLE IF EXISTS Users is used to drop table everytime we run the program. Running the program: $ python3 contacts. pyEnter name: homerEnter email: homer@simpson. comAdd more? y/n: yEnter name: bartEnter email: bart@simpson. comAdd more? y/n: n$ lscontacts. py contacts. sqliteLet’s open the contacts. sqlite: $ sqlite3 contacts. sqlitesqlite&gt; . tablesUserssqlite&gt; SELECT * FROM Users;homer|homer@simpson. combart|bart@simpson. comLet’s export to CSV: sqlite&gt; . headers onsqlite&gt; . mode csvsqlite&gt; . once contacts. csvsqlite&gt; SELECT * FROM Users;sqlite&gt; . system xdg-open contacts. csvLaunches my text editor Sublime Text: name,emailhomer,homer@simpson. combart,bart@simpson. com"
    }, {
    "id": 199,
    "url": "https://www.tomordonez.com/best-time-to-run-according-to-science/",
    "title": "The Best Time To Run According To Science",
    "body": "2018/04/25 - What is the best time to run? According to science it depends on the ozone levels where you live. (I published this story last year in another website. I updated some code and plots) The best time to run in San Francisco is at 6am. : The best time to run in Miami is at 6am or 7am but not at noon. : I studied ozone levels from a data set that showed hourly measurements of ozone levels for different cities in the US for 2016. Ozone who?: Wikipedia says that ozone is a pale blue gas with a bad smell.  Ozone odour is sharp, smells like chlorine and is measured in parts per billion (ppb).  Concentrations of 100ppb and above damages respiratory tissues. Which makes ozone a respiratory hazard near ground level. You might also know ozone as in “ozone layer”. Which is a portion of the stratosphere. About 20 miles above the ground. Just to give you a better reference. Perhaps you remember the pilot saying “We are at 35,000 feet now you can watch a movie and eat peanuts”. The ozone layer is at about 100,000 feet. The ozone layer prevents UV light from reaching the surface. Which is actually good. But ozone at surface level. Not good. Bad ozone is due to…drum roll…fossil fuel burning. Example of fossil fuel burning: Factories burning fossil fuel: Cars in traffic burning fossil fuel: Data About Ozone Levels: Now that you know how bad ozone is created let’s look at some data. The Environmental Protection Agency (EPA) website has data sets about ozone levels in the US since 1980. I downloaded 2 data sets: Hourly ozone levels in the US from 2016. The zip file was 69MB. Uncompressed, the csv file is 2. 1GB. The other data set was daily AQI by county from 2016. The zip file was 1. 6MB and the csv file was 26. 4MB. AQI stands for Air Quality Index. A measure developed by EPA to explain pollution levels to the general public. I used Rstudio to open and analyze the data. Rstudio is an open source software for R, a programming language for statistical computing. Loading the data into Rstudio: This process comes from the Coursera class “Managing Data Analysis”. Setup the project:  File New Project New Directory Empty Project Enter name and browse to subdirectoryInstall readr: If you don’t have the package readr you need to install it. It will take about 5 minutes: &gt; install. packages( readr )The library readr is needed to read rectangular data such as csv. Then load the library: &gt; library(readr)Load the CSV file with read_csv: Load the CSV file and create an object. The function read_csv converts the csv file into a data frame. If you don’t specify the types for columns, strange things happen. For example the data set has a column called Time. Local, which by default is of type integer. If you run a plot using Time. Local you will get the time in seconds instead of hours, such as: 3600, representing 1am7200, representing 2amand so on. . . The column type has to be set to c character so that Time. Local can take values like this: 00:0001:0002:00and so on. . . Setting the column types depend on the columns. But how do you set them up if you haven’t loaded the data? The data source should have a description of the data. In this case there is more information about this data set on the EPA website.   Here is the line to load the CSV into the object ozone. &gt; ozone &lt;- read_csv( ozone_data_2016. csv , col_types =  ccccinnccccccncnnccccccc )Get the names of the columns: If you call this function: names(ozone) you get this result: &gt; names(ozone) [1]  State Code     County Code       Site Num       [4]  Parameter Code   POC           Latitude       [7]  Longitude      Datum          Parameter Name    [10]  Date Local     Time Local       Date GMT       [13]  Time GMT      Sample Measurement   Units of Measure   [16]  MDL         Uncertainty       Qualifier      [19]  Method Type     Method Code       Method Name     [22]  State Name     County Name       Date of Last Change By definition names is used to get or set the name of an object. If the object is a dataframe it gets the names of the columns. Normalize the column names: If you call this function: make. names(names(ozone)) you get this result: &gt; make. names(names(ozone)) [1]  State. Code     County. Code       Site. Num       [4]  Parameter. Code   POC           Latitude       [7]  Longitude      Datum          Parameter. Name    [10]  Date. Local     Time. Local       Date. GMT       [13]  Time. GMT      Sample. Measurement   Units. of. Measure   [16]  MDL         Uncertainty       Qualifier      [19]  Method. Type     Method. Code       Method. Name     [22]  State. Name     County. Name       Date. of. Last. Change By definition make. names is used to make syntactically valid names out of character vectors. The help page explains that syntactically valid names consist of letters, numbers and dot or underline and start with a letter or the dot but not followed by a number. For instance . 2file is not a valid name. I am thinking that one of the reasons for this conversion is the problem of spaces in character strings. For instance, when using files and directories in Linux. You need to escape the space with \ . Otherwise you should not use spaces when naming files or directories. To normalize the column names, and replace spaces with periods, use: make. names(names(ozone)). Replace the names of columns: Finally, this is used to set this conversion back to the names object: &gt; names(ozone) &lt;- make. names(names(ozone))Now, when you call names(ozone) the spaces are replaced by dots. &gt; names(ozone) [1]  State. Code     County. Code       Site. Num       [4]  Parameter. Code   POC           Latitude       [7]  Longitude      Datum          Parameter. Name    [10]  Date. Local     Time. Local       Date. GMT       [13]  Time. GMT      Sample. Measurement   Units. of. Measure   [16]  MDL         Uncertainty       Qualifier      [19]  Method. Type     Method. Code       Method. Name     [22]  State. Name     County. Name       Date. of. Last. Change Reviewing the top and bottom: Check the number of rows: &gt; nrow(ozone)[1] 9124268Check the number of columns: &gt; ncol(ozone)[1] 24Then check the top of the file: &gt; head(ozone[, c(6:7, 10)])# A tibble: 6 x 3Latitude Longitude Date. Local   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   1   30. 5   -87. 9 2016-03-012   30. 5   -87. 9 2016-03-013   30. 5   -87. 9 2016-03-014   30. 5   -87. 9 2016-03-015   30. 5   -87. 9 2016-03-016   30. 5   -87. 9 2016-03-01Check the bottom of the file: &gt; tail(ozone[, c(6:7, 10)])# A tibble: 6 x 3Latitude Longitude Date. Local  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   1   18. 2   -65. 9 2016-12-312   18. 2   -65. 9 2016-12-313   18. 2   -65. 9 2016-12-314   18. 2   -65. 9 2016-12-315   18. 2   -65. 9 2016-12-316   18. 2   -65. 9 2016-12-31Check the number of records for each hour: Using the table function. This function “uses the cross classifying factors to build a contingency table of counts at each combination of factor levels”. In this case I know that the ozone levels are measured every hour from midnight to noon to 11pm. I would like to know the number of records for each hour. table(ozone$Time. Local)This results in: 00:00 01:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 369489 373144 359722 358576 363421 385829 386222 384176 381303 09:00 10:00 11:00 12:00 13:00 14:00 15:00 16:00379689 379424 380814 381202 383439 385152 386580 38756517:00 18:00 19:00 20:00 21:00 22:00 23:00 388252 388576 388656 388650 387492 377278 379617What does this data mean?: I selected and filtered parts of the data to understand more about it. Was the data grouped by state? Or was the data grouped by hour? I learned that there are monitors all around the US that can measure ozone levels. Install dplyr: The next step requires to use the library dplyr. Which is the next iteration of plyr, a set of tools for splitting, applying and combining data. dplyr “provides a flexible grammar of data manipulation…focused on tools for working with data frames”. &gt; install. packages( dplyr )This might ask to restart the R session before installing. Then try again. Now load the library. &gt; library(dplyr)US States in the data set: I wanted to see which states were included in the data set I loaded. &gt; select(ozone, State. Name) %&gt;% unique# A tibble: 52 x 1State. Name       &lt;chr&gt;        1 Alabama       2 Alaska        3 Arizona       4 Arkansas       5 California      6 Colorado       7 Connecticut     8 Delaware       9 District Of Columbia10 Florida       # . . . with 42 more rowsOzone levels in Florida: I filtered the data set to see the ozone levels in Florida on July 9, 2016 at 6am for all counties &gt; filter(ozone, State. Name ==  Florida , Date. Local ==  2016-07-09 , Time. Local ==  06:00 ) %&gt;% select(County. Name,   Sample. Measurement)The result was: # A tibble: 58 x 2  County. Name Sample. Measurement     &lt;chr&gt;       &lt;dbl&gt; 1   Alachua       0. 002 2    Baker       0. 001 3     Bay       0. 017 4   Brevard       0. 014 5   Brevard       0. 016 6   Broward       0. 005 7   Broward       0. 004 8   Broward       0. 004 9   Broward       0. 00810   Collier       0. 004# . . . with 48 more rowsConvert PPM (parts per million) to PPM (per billion): The sample measurement has units of ppm (parts per million). To convert to ppb (parts per billion) you just move the dot three times to the right. For instance, the first row that says 0. 002 ppm converts to 2 ppb. I learned that the EPA has an AQI calculator to convert from ppb to AQI (air quality index). In the AQI calculator you have a few choices as shown here:  Select a pollutant: O3 - Ozone (8hr avg) or (1hr avg) Units required: ppb Enter the concentration CalculateThe way to choose between 8hr and 1hr is: AQI values of 301 or greater are calculated with 1-hr ozone concentrations. AQI Categories: The EPA has a great website to learn more about the Air Quality Index (AQI). The AQI is categorized like this:  0-50. Good. Green 51-100. Moderate. Yellow 101-150. Unhealthy for sensitive groups. Orange 151-200. Unhealthy. Red 201-300. Very unhealthy. Purple 301-500. Hazardous. Maroon. This information was very helpful to understand the data Very Unhealthy Jefferson county, Alabama: I filtered the data to find ozone measurements above 0. 100 ppm (aka 100ppb). Which calculates to an AQI of 187 (Unhealthy). &gt; filter(ozone, Sample. Measurement &gt; 0. 1) %&gt;% select(State. Name, County. Name, Date. Local,   Time. Local, Sample. Measurement)# A tibble: 1,407 x 5State. Name  County. Name Date. Local Time. Local Sample. Measurement.   &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;   &lt;time&gt;     &lt;dbl&gt;1  Alabama  Jefferson  2016-02-23  12:00:00  0. 3462  Alabama  Jefferson  2016-02-23  13:00:00  0. 2023  Alabama  Russell   2016-04-18  14:00:00  0. 1014  Alaska  Matanuska  2016-04-08  02:00:00  0. 1025  Arizona  Gila    2016-11-26  17:00:00  0. 1456  Arkansas Crittenden 2016-06-10  13:00:00  0. 1077  Arkansas Crittenden 2016-06-10  14:00:00  0. 1138  Arkansas Crittenden 2016-06-10  15:00:00  0. 1049 California Alameda   2016-06-03  15:00:00  0. 10210 California Alameda   2016-07-26  15:00:00  0. 102# . . . with 1,575 more rowsThis shows that on February 23, 2016 at noon, there was a measurement of 346ppb in Jefferson county, Alabama. Converted to AQI results in 276 aka “Very Unhealthy” and very close to “Hazardous”, which is described as “Health warnings of emergency conditions. The entire population is more likely to be affected. ” After a quick Google search I found that Jefferson county is amongst the top most polluted counties in the US. Yikes! I read on Wikipedia that before Detroit. This county was the largest bankruptcy in the US with corruption being a big cause. Not sure if there is a correlation between corruption and air quality level. It is worth exploring this data into more detail to find some correlation and try to answer a lot of interesting questions. Such as the correlation of corruption and air quality level. For now let’s just look at hourly data from some cities to see when is the best time to run. Plotting the best time to run: I know the names of some state counties in Florida. For example, for Miami, I know that the county is called “Miami Dade” but I wasn’t sure how they added this value into the data set. If they put “Dade” or “Miami” or what combination of Miami and Dade. For each state I created vectors with the names of counties. I visualized the vectors into a table and then created a filter for the specific county. Finally, I plotted Time vs Sample Measurement. I suffered a little bit making these plots, as noted below. Plotting Workflow: For example, look at the plotting workflow for Miami. I created a vector with Florida counties: &gt; florida. counties &lt;- filter(ozone,   State. Name ==  Florida ) %&gt;%   select(County. Name) %&gt;% uniqueThen created a view: &gt; View(florida. counties) On the view I saw they had it as Miami-Dade. &gt; ozone. miami. 2016 &lt;- filter(ozone,   County. Name ==  Miami-Dade ) %&gt;%   select(Date. Local, Time. Local,     Sample. Measurement)Troubleshooting drawing the Plot: I filtered by county name and then plotted Time. Local vs Sample. Measurement. &gt; plot(ozone. miami. 2016$Time. Local,   ozone. miami. 2016$Sample. Measurement)The output was an error: Error in plot. window(. . . ) : need finite 'xlim' valuesIn addition: Warning messages:1: In xy. coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion2: In min(x) : no non-missing arguments to min; returning Inf3: In max(x) : no non-missing arguments to max; returning -InfThe error says that one of the coordinates is a vector with NAs. I looked up the types for each variable: &gt; class(ozone. miami. 2016$Time. Local)[1]  character &gt; class(ozone. miami. 2016$Sample. Measurement)[1]  numeric Time. Local is of type character and Sample. Measurement is of type numeric. &gt; head(ozone$Time. Local)[1]  15:00   16:00   17:00   18:00   19:00   20:00 &gt; head(ozone$Sample. Measurement)[1] 0. 041 0. 041 0. 042 0. 041 0. 038 0. 038You cannot plot character vs numeric. To prove that Time. Local was a vector with NAs I used the function strptime. The function strptime is used to convert between character representations and objects of classes “POSIXlt” and “POSIXct” representing calendar dates and times. I read more about POSIXt and it seems to be a complicated topic as described here. I don’t want to get into much detail about this now. The strptime function is used like this: strptime(x, format, tz =   ) x: An object to be converted: a character vectorfor strptime, an object which can be converted to POSIXlt  for strftime. format: A character string. The default forthe format methods is  %Y-%m-%d %H:%M:%S . tz: A character string specifying thetime zone to be used for the conversionI ran the strptime function like this: &gt; strptime(ozone. miami. 2016$Time. Local, '%H:%M:%S')The result was something like this: [1] NA NA NA NA NA NA NANA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NAAs far as the error saying: In xy. coords(x, y, xlabel, ylabel, log) : NAs introduced by coercionI assume that plot is trying to make Time. Local which is a character element into numeric data. And when it fails to do this, it is returning NAs. (although I might be wrong about this). Plotting Best Time to Run: Instead I thought that there is a way to plot Time. Local as a factor. &gt; plot(factor(ozone. miami. 2016$Time. Local), ozone. miami. 2016$Sample. Measurement, main =  Best Time to Run in Miami , xlab =  Hour , ylab =  Ozone level in ppm )Troubleshooting lower case and UPPER case variables: I also suffered a little bit with lower case and upper case variables. To get the list of State. Name I did: &gt; unique(ozone$State. Name) [1]  Alabama     Alaska     Arizona        [4]  Arkansas     California   Colorado        [7]  Connecticut   Delaware    District Of Columbia [10]  Florida     Georgia To get the County. Name for District of Columbia I did this but didn’t get any results: &gt; filter(ozone, State. Name ==  District of Columbia ) %&gt;% select(County. Name) %&gt;% unique# A tibble: 0 x 1# . . . with 1 variables: County. Name &lt;chr&gt;I realized that I didn’t use the exact case. Instead of Of, I used of. &gt; filter(ozone, State. Name ==  District Of Columbia ) %&gt;% select(County. Name) %&gt;% unique# A tibble: 1 x 1      County. Name         &lt;chr&gt;1 District of ColumbiaI figured that normalizing the data is a pain in the butt. I followed the same process to plot Miami, Washington DC, San Francisco and Los Angeles. Summary how to Plot in R: Get a list of States: unique(ozone$State. Name)Find the County Name. This will give you a table of County names: florida. counties &lt;- filter(ozone, State. Name ==  Florida ) %&gt;% select(County. Name) %&gt;% uniqueView(florida. counties)In this case I found the county for Miami Dade to be Miami-Dade. Filter the data and create an object: ozone. miami. 2016 &lt;- filter(ozone, County. Name ==  Miami-Dade ) %&gt;% select(Date. Local, Time. Local, Sample. Measurement)Create a plot with this object and add titles: plot(factor(ozone. miami. 2016$Time. Local), ozone. miami. 2016$Sample. Measurement, main =  Best Time to Run in Miami , xlab =  Hour , ylab =  Ozone level in ppm )Best Time To Run in Miami: &gt; summary(ozone. miami. 2016$Sample. Measurement)  Min. 1st Qu.  Median  Mean 3rd Qu.   Max. 0. 00000 0. 02000 0. 02900 0. 02985 0. 04000 0. 09200Best time to run in Miami Looking at the ozone levels in Miami:  The lowest ozone levels in Miami are at 6am and 7am.  The highest ozone levels in Miami are between 12pm and 5pm.  At night the ozone levels are lower but not as low as in early morning. The best time to run in Miami seems to be between 6am to 8am. Best Time to Run in Washington DC: dc. counties &lt;- filter(ozone, State. Name ==    District Of Columbia ) %&gt;% select(County. Name)   %&gt;% uniqueView(dc. counties)ozone. dc. 2016 &lt;- filter(ozone, County. Name ==    District of Columbia ) %&gt;% select(Date. Local,     Time. Local, Sample. Measurement)plot(factor(ozone. dc. 2016$Time. Local), ozone. dc. 2016$Sample. Measurement, main =  Best Time to Run in DC , xlab =  Hour , ylab =  Ozone level in ppm ) &gt; summary(ozone. dc. 2016$Sample. Measurement)  Min. 1st Qu.  Median  Mean 3rd Qu.   Max. 0. 00000 0. 01500 0. 02500 0. 02575 0. 03500 0. 09000Ozone levels in Washington DC:  The lowest ozone levels in Washington DC are between 5am to 7am.  The highest ozone levels are from 9am until 7pm. With the highest around noon.  At night there are lower ozone levels after 8pm. The best time to run in Washington DC in the morning is between 5am and 7am. Then at night, after 8pm. Best Time to Run in San Francisco: ca. counties &lt;- filter(ozone, State. Name ==    California ) %&gt;% select(County. Name)   %&gt;% uniqueView(ca. counties)ozone. sf. 2016 &lt;- filter(ozone, County. Name ==    San Francisco ) %&gt;% select(Date. Local,     Time. Local, Sample. Measurement)plot(factor(ozone. sf. 2016$Time. Local), ozone. sf. 2016$Sample. Measurement, main =  Best Time to Run in San Francisco , xlab =  Hour , ylab =  Ozone level in ppm ) &gt; summary(ozone. sf. 2016$Sample. Measurement)  Min. 1st Qu.  Median  Mean 3rd Qu.   Max. 0. 00000 0. 01600 0. 02300 0. 02243 0. 03100 0. 07000Ozone levels in San Francisco:  The lowest ozone levels in San Francisco are between 5am and 7am.  The highest ozone levels are between 12pm and 3pm. The best time to run in San Francisco is between 5am and 7am. Then at night, after 7pm. Best Time to Run in Los Angeles: &gt; summary(ozone. la. 2016$Sample. Measurement)  Min.  1st Qu.  Median   Mean 3rd Qu.   Max. -0. 00300 0. 01300 0. 02800 0. 02864 0. 04100 0. 14800Los Angeles has a very interesting plot. There are a lot of points away of the median, within the following ranges:  51-100. Moderate. Yellow 101-150. Unhealthy for sensitive groups. Orange 151-200. Unhealthy. RedThere is a maximum ozone measurement of 0. 148. Which is very close to the Red range, aka “Everyone may begin to experience health effects; members of sensitive groups may experience more serious health effects. ” Ozone levels in Los Angeles:  The lowest ozone levels in Los Angeles are between 4am and 6am.  The highest ozone levels are between 11am and about 7pm.  Then it drops around 9pm. The best time to run in Los Angeles is between 4am and 6am. Don’t go out for a run between 11am and 7pm. Then you can try around 9pm. Improving this analysis:  I wanted to plot a comparison of the ozone levels at multiple counties for a specific time. I wasn’t sure how to do this.  I wonder if there is a positive correlation between corruption and air quality level. "
    }, {
    "id": 200,
    "url": "https://www.tomordonez.com/python-lambda-beautifulsoup/",
    "title": "Python Lambda and BeautifulSoup",
    "body": "2018/04/24 - This Python Lambda is a very weird concept. I almost grok it. I was trying to parse HTML comments using BeautifulSoup. After a quick Google search I found this solution: commments = soup. find_all(text=lambda text:isinstance(text, Comment))Say what? You lost me at lambda. Here is an easier lambda example. : def sum(x, y):  return x + yThis is easy right? Just a simple function that takes x and y and returns the sum. What if one day you say “I bet there is a one-liner for this”. And I am not talking about this: def sum(x, y): return x + yI am talking about some mutant Python code skill that when you write it everybody in the room just faints. Python Lambda: sum = lambda x,y: x + yDid you see that? From this: def sum(x, y):  return x + yTo this: sum = lambda x,y: x + yI read it like this: “Invoke the powers of Lambda and take the parameters x and y. Add them together and return the value. Assign the value to sum”. Python Lambda Details: A Lambda function is an anonymous function. A function defined without a name. Your usual function is defined with def. While anonymous functions are defined with lambda. The syntax of the lambda function is: lambda arguments: expression Any number of arguments One expression to rule them all The expression needs to return a valueCalling Lambda Helloooo Lambda: This works: &gt;&gt;&gt; lambda1 = lambda x: x**2&gt;&gt;&gt; lambda1(3)9Lambda, BeautifulSoup and HTML Comments: This is where I was tripping a bit. Given a soup object. And an HTML such as: &lt;!-- Python is awesome --&gt;&lt;!-- Lambda is confusing --&gt;&lt;!-- name= Homer Simpson  --&gt;&lt;title&gt;I am grook&lt;/title&gt;&lt;h1&gt;Homer groks Lambda&lt;/h1&gt;&lt;p&gt;Lolcats&lt;/p&gt;&lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;&lt;a href= https://twitter. com/tomordonez &gt;I grok Twitter&lt;/a&gt;I wanted to extract the text from the HTML comments. BeautifulSoup has a module called Comment that is used for this. from bs4 import CommentThe solution from StackOverflow says that to extract the comments to a list. You need lambda and the isinstance function. comments = soup. find_all(text=lambda text: isinstance(text, Comment))BeautifulSoup and Lambda: Keep as reference the short HTML example above. The “find all HTML comments code” starts with find_all. Some people keep using findAll too. But the new syntax is find_all to comply with PEP8. Using underscores and not camelCase. In BeautifulSoup, the find_all method, searches for all tags in the soup object. Using find_all(): &gt;&gt;&gt; soup. find_all('a')[&lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;,&lt;a href= https://twitter. com/tomordonez &gt;I grok Twitter&lt;/a&gt;]Will search for all a anchor tags and return a list of tag objects. Using find: &gt;&gt;&gt; soup. find('a')&lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;Will only search for one a anchor tag and return a tag object. &lt;class 'bs4. element. Tag'&gt;The default search method of a soup object is find_all so you can also do this: &gt;&gt;&gt; soup('a')[&lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;,&lt;a href= https://twitter. com/tomordonez &gt;I grok Twitter&lt;/a&gt;]This will also return a list of a tags. I guess the difference of using find_all and find is about returning a list of results or returning just one result. In an HTML document there is only one title tag. Why would you use find_all? You wouldn’t. &gt;&gt;&gt; soup. find('title')&lt;title&gt;I am grook&lt;/title&gt;If you use this: &gt;&gt;&gt; soup('title')It will default to using find_all. Anchor Tags and Attributes: In BeautifulSoup the attributes of an anchor tag can be accessed as a dictionary. Using find will only return the 1st value it finds. : &gt;&gt;&gt; anchor = soup. find('a')&gt;&gt;&gt; anchor&lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;To get the string from href: &gt;&gt;&gt; anchor['href']https://www. tomordonez. comThe attributes of the anchor tag are defined as a dictionary such as: {'href': 'https://www. tomordonez. com'}Which means that you can find a specific tag that has attributes such as: soup. find('tag_name', attrs={'key': 'value'})Anchor Tags and Strings: By default find_all and find are looking for HTML tags. What about strings such as: I grok Lambda tooWhich is inside of: &lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;You can use the string argument. In a previous version of BeautifulSoup it was called text. They changed it to string. You could do this: &gt;&gt;&gt; soup. find(string='I grok Lambda too') I grok Lambda too For this example: &lt;title&gt;I am grook&lt;/title&gt;You can do this. Which uses find to search for the tag title: &gt;&gt;&gt; soup. find('title'). string I am grook Or this also works. Which uses the argument string to search for strings instead of tags: &gt;&gt;&gt; soup. find(string= I am grook ) I am grook Weird Lambda code: Let’s review that lambda code again to find HTML comments: comments = soup. find_all(text=lambda text: isinstance(text, Comment))First of all text= is not used anymore. You should use string=. Let’s change that: comments = soup. find_all(string=lambda text: isinstance(text, Comment))Now, text is just a variable. Which in this case is not a really good name. We are not looking for any text, we are looking for HTML comments. Let’s change that: comments = soup. find_all(string=lambda html_comment: isinstance(html_comment, Comment))What is Comment?: BeautifulSoup has a module called Comment that helps you find HTML comments. from bs4 import CommentWhat is isinstance?: isinstance is a Python built-in function. The syntax is: isinstance(object, classinfo)It returns True if the object argument is an instance of the classinfo argument. And you could test it like this: &gt;&gt;&gt; isinstance('Homer', str)True&gt;&gt;&gt; isinstance('Homer', int)FalseWhich means that this: isinstance(html_comment, Comment)Returns True or False… “Is html_comment an instance of the Comment object?” The result will be either True or False. Lambda function: lambda html_comment: isinstance(html_comment, Comment)This is the same as doing this: def comments(html_comment):  isinstance(html_comment, Comment)Putting this together: soup. find_all(string=lambda html_comment: isinstance(html_comment, Comment))This reads as:  find_all strings Pass the string as an argument of the lambda function.  The string uses the argument html_comment.  isinstance says “Is html_comment an instance of Comment?” If it is then return that string. Finding HTML Comments in BeautifulSoup: To summarize. Given this: from bs4 import BeautifulSoup, Commenthtml = '''&lt;!-- Python is awesome --&gt;&lt;!-- Lambda is confusing --&gt;&lt;!-- name= Homer Simpson  --&gt;&lt;title&gt;I am grook&lt;/title&gt;&lt;h1&gt;Homer groks Lambda&lt;/h1&gt;&lt;p&gt;Lolcats&lt;/p&gt;&lt;a href= https://www. tomordonez. com &gt;I grok Lambda too&lt;/a&gt;&lt;a href= https://twitter. com/tomordonez &gt;I grok Twitter&lt;/a&gt;'''We create a soup object: soup = BeautifulSoup(html, 'html. parser')Using find_all, which is the default search method for soup: &gt;&gt;&gt; soup. find_all('title')[&lt;title&gt;I am grook&lt;/title&gt;]Keep in mind that find_all will always return a list of tag objects. And it’s the same as using this: &gt;&gt;&gt; soup('title')[&lt;title&gt;I am grook&lt;/title&gt;]Getting the string out of title. Remember that right now we have a list with one element. If you do this. It won’t work: &gt;&gt;&gt; soup('title'). stringTraceback. . . . . . Did you call find_all() when youmeant to call find()?This works: &gt;&gt;&gt; soup('title')[0]. string'I am grook'But it’s weird right? If there is only one title on the HTML then just use find: &gt;&gt;&gt; soup. find('title')&lt;title&gt;I am grook&lt;/title&gt;This doesn’t return a list. But only searches for the 1st result it finds. This is the same as using this: &gt;&gt;&gt; soup. title&lt;title&gt;I am grook&lt;/title&gt;To get the string. These two do the same: &gt;&gt;&gt; soup. find('title'). string&gt;&gt;&gt; soup. title. stringIt will give you: 'I am grook'To find or to find_all. That’s the question: The shorthand of find_all is: &gt;&gt;&gt; soup('title')The shorthand of find is: &gt;&gt;&gt; soup. titleIt’s easy to get confused by this optimization. It’s better to just use the name of the method. Finding attributes: The methods find_all and find search for tags. Attributes can be accessed as dictionaries. Given: &lt;a href= https://twitter. com/tomordonez &gt;I grok Twitter&lt;/a&gt;&lt;a href= https://www. tomordonez. com  name= Awesome &gt;I grok Lambda too&lt;/a&gt;For the second a anchor, the attributes dictionary is: {'href': 'https://www. tomordonez. com', 'name': 'Awesome'}Find an anchor tag with a specific attribute: &gt;&gt;&gt; soup. find('a', attrs={'name': 'Awesome'})&lt;a href= https://www. tomordonez. com  name= Awesome &gt;I grok Lambda too&lt;/a&gt;Then to access the href you could do this: &gt;&gt;&gt; anchor_tag = soup. find('a', attrs={'name': 'Awesome'})&gt;&gt;&gt; anchor_tag&lt;a href= https://www. tomordonez. com  name= Awesome &gt;I grok Lambda too&lt;/a&gt;&gt;&gt;&gt; anchor['href']'https://www. tomordonez. com'Or all in one: &gt;&gt;&gt; soup. find('a', attrs={'name': 'Awesome'})['href']Finding Strings: Use the string argument: &gt;&gt;&gt; soup. find(string= I grok Lambda )'I grok Lambda'If you do this. It won’t work: &gt;&gt;&gt; soup. find(string= I  )But you can use a regular expression: &gt;&gt;&gt; soup. find(string = re. compile(r'^I'))'I grok Lambda'You can also pass a function to the string argument Passing a Lambda function to the string argument: Given: &lt;!-- Python is awesome --&gt;&lt;!-- Lambda is confusing --&gt;&lt;!-- name= Homer Simpson  --&gt;To get only the 1st comment you can use find: &gt;&gt;&gt; soup. find(string = lambda html_comment: isinstance(html_comment, Comment))' Python is awesome 'To get a list of comments then use find_all: &gt;&gt;&gt; soup. find_all(string = lambda html_comment: isinstance(html_comment, Comment))[' Python is awesome ', ' Lambda is confusing ', ' name= Homer Simpson ']But keep in mind in this case the HTML comments have leading and trailing whitespace. You can just use the strip() method. "
    }, {
    "id": 201,
    "url": "https://www.tomordonez.com/python-self/",
    "title": "Python Self",
    "body": "2018/04/19 - Understanding Python self. This answer from StackOverflow helped me understand this concept a bit better. The Simpsons, Self and Python: If you have a class called list with a method called append. &gt;&gt;&gt; simpsons = list()&gt;&gt;&gt; simpsons. append('homer')&gt;&gt;&gt; simpsons['homer']The method is defined as this: def append(self, arg1, arg2):  # do somethingThe simpsons object is an instance of the class list. As the solution says, but using my example: When simpsons. append('homer') is called, Python internally converts this to: list. append(simpsons, 'homer')If I run this again with another argument: &gt;&gt;&gt; list. append(simpsons, 'bart')&gt;&gt;&gt; simpsons['homer', 'bart']Which means that these 2 do the same: &gt;&gt;&gt; simpsons. append('bart')&gt;&gt;&gt; list. append(simpsons, 'bart')Looking at this again: def append(self, arg1)The self variable refers to the object. Rock bands, Self and Python: This is another good answer about self and Python in Quora. The first few paragraphs create a good context for understanding self.  A class has methods A class can have multiple objectsHere is an interesting question. “When an object calls a method of the class, how would the method know which object has called it?” class myBand:  def __init__(self):    self. instruments = []    self. instruments. append('drums')  def append(self):    awesome append codemuse = myBand()muse. append('bass') # prints ['drums', 'bass']radiohead = myBand()radiohead. append('moog') # prints ['drums', 'moog']Following the same example as the Simpsons &gt;&gt;&gt; muse. append('bass')&gt;&gt;&gt; myBand. append(muse, 'bass')&gt;&gt;&gt; myBand. append(self, arg1)"
    }, {
    "id": 202,
    "url": "https://www.tomordonez.com/url-encoding-python/",
    "title": "URL Encoding and Python",
    "body": "2018/04/16 - This is not a full reference guide. It’s just a quick reminder for URL encoding. I am studying OAuth and I am reviewing the signature base string for a signed request. The base string has:  Method of request: POST URL: https://api. twitter. com/statuses/update. json Parameters joined by &amp;Parameters:  include_entities = true oauth_consumer_key = weirdnumberhere oauth_signature_method = HMAC-SHA1 oauth_token = otherweirdnumber oauth_version = 1. 0 status = The Chemical BrothersBase string: POST&amp;https%3A%2F%2Fapi. twitter. com%%2Fstatuses%2Fupdate. json&amp;include_entities%3Dtrue%26oauth_consumer_key%3Dweirdnumberhere%26oauth_signature_method%3DHMAC-SHA1%26oauth_token%3Dotherweirdnumber%26oauth_version%3D1. 0%26status%3DThe%20Chemical%20BrothersHex codes:  %3A %2F %3D %26 %20Converting character to hex in Python: The first part of the string is: https://You can get the character encoding to hex using hex(ord()). &gt;&gt;&gt; hex(ord(':'))'0x3a'&gt;&gt;&gt; hex(ord('/'))'0x2f'This is the conversion: https%3A%2F%2FThese are the conversions: &gt;&gt;&gt; hex(ord('='))'0x3d'&gt;&gt;&gt; hex(ord('&amp;'))'0x26'&gt;&gt;&gt; hex(ord(' '))'0x20'"
    }, {
    "id": 203,
    "url": "https://www.tomordonez.com/python-socket-save-data-to-file/",
    "title": "Python Socket Save Data To File",
    "body": "2018/04/06 - Send and receive data can be tested using 3 methods:  telnet Chrome developer tools Python socket and save data to fileUsing Telnet: $ telnet data. pr4e. org 80The output should be something like: Trying 192. 241. 136. 170. . . Connected to data. pr4e. org. Escape character is '^]'Then enter the command: GET / HTTP/1. 0The output should be a response: HTTP/1. 1 200 OKAnd then the header of that page: Date: Fri, 06 Apr 2018 23:07:21 GMTServer: Apache/2. 4. 7 (Ubuntu)Last-Modified: Thu, 12 Nov 2015 19:12:19 GMTETag:  2cf6-5245cb8c635cb Accept-Ranges: bytesContent-Length: 11510Vary: Accept-EncodingConnection: closeContent-Type: text/htmlThen the output is the HTML page. Using Chrome developer tools: Open the page data. pr4e. org. Go to Developer tools. Network. Reload the page. Under Name. Click on data. pr4e. org. The Headers tab shows similar info: Request URL: http://data. pr4e. org/Request Method: GETStatus Code: 200 OKRemote Address: 192. 241. 136. 170:80Content-Length: 625Content-Type: text/html;charset=UTF-8Date: Fri, 06 Apr 2018 23:18:23 GMTConnection: keep-aliveUsing Python socket to save data to file: import socketimport rehost = input('Enter the host: ')port = int(input('Enter the port number: '))mysock = socket. socket(socket. AF_INET, socket. SOCK_STREAM)cmd = 'GET / HTTP/1. 0\r\n\r\n'. encode()try:  mysock. connect((host, port))except:  print('Could not connect')print('Connected')try:  mysock. send(cmd)except:  print('Could not send data')print('Data sent')with open('data_stream. txt', 'w') as fhandle:  while True:    data = mysock. recv(512)    if (len(data) &lt; 1):      break    fhandle. write(data. decode())mysock. close()with open('data_stream. txt', 'r') as fhandle:  for line in fhandle:    match = re. match(r'Last-Modified: ([\w,: ]+)', line)    if match:      last_modified = match. group()      print(last_modified)    match2 = re. match(r'ETag: ([ \w-]+)', line)      etag = match. group()      print(etag)"
    }, {
    "id": 204,
    "url": "https://www.tomordonez.com/install-chromedriver-linux/",
    "title": "Install Chromedriver in Linux",
    "body": "2018/03/27 - Based on this gist about installing Chromedriver in Linux Fedora. And setting up the correct file location in your Python scripts. $ wget https://chromedriver. storage. googleapis. com/2. 37/chromedriver_linux64. zip$ unzip chromedriver_linux64_2. 3. zip$ sudo cp chromedriver /usr/bin/chromedriver$ sudo chown root /usr/bin/chromedriver$ sudo chmod +x /usr/bin/chromedriver$ sudo chmod 755 /usr/bin/chromedriverThen setup Chromedriver using the right location. driver = webdriver. Chrome('/usr/bin/chromedriver')"
    }, {
    "id": 205,
    "url": "https://www.tomordonez.com/file-iterations-python/",
    "title": "File Iterations in Python",
    "body": "2018/03/12 - This is a short explanation on how file iterations work in Python. Given a file input. txt that has content such as: Content-Type: text/plain; charset=UTF-8X-DSPAM-Result: InnocentX-DSPAM-Processed: Fri Jan 4 14:50:18 2017X-DSPAM-Confidence: 0. 7556X-DSPAM-Probability: 0. 0000I wanted to check in the Python shell if a substring exists in a string like this: &gt;&gt;&gt; fhandle = open('input. txt', 'r')&gt;&gt;&gt; for line in fhandle:&gt;&gt;&gt;  if 'DSPAM' in line:&gt;&gt;&gt;    print(line)The output is: X-DSPAM-Result: InnocentX-DSPAM-Processed: Fri Jan 4 14:50:18 2017X-DSPAM-Confidence: 0. 7556X-DSPAM-Probability: 0. 0000If I run the for loop again in the same shell session. It doesn’t return any output. It just goes back to the prompt &gt;&gt;&gt; I wanted to know why this happened. My question on StackOverflow, has an answer that says:  File objects can only be iterated once unless you seek back to the beginning. It was also marked as a duplicate question to this one. File iterations in Python: Here is an answer from the duplicate question:  The first time you read to the end of the file. You can’t read it anymore unless you reset it. To “reset it” you can do:  Exit and start the shell again.  Use fhandle. seek(0) to reposition to the start of the file.  Close and open the file again.  Use with open() asNone of these solutions have a real benefit in the Python shell. The best thing to do is just to: &gt;&gt;&gt; fhandle. close()These are still good solutions to “reset” the iteration on the file: Use fhandle. seek(0): Although this doesn’t seem much different than fhandle. close(). Not sure which one uses less memory. with open as syntax: This syntax opens and closes the file for you: with open('input. txt', 'r') as fhandle:  for line in fhandle:    if 'SPAM' in line:      print(line)"
    }, {
    "id": 206,
    "url": "https://www.tomordonez.com/install-react-native-mac/",
    "title": "Install React Native on Mac",
    "body": "2017/12/17 - Follow this simple tutorial to install React Native on Mac. Install Node: Download the Node installer from here. When I published this tutorial 12/17/17. The current Node version was 8. 9. 3 LTS. This package will install::  Node. js v8. 9. 3 to /usr/local/bin/node npm v5. 5. 1 to /usr/local/bin/npmMake sure that /usr/local/bin is in your $PATH: $ echo $PATH/usr/local/binCheck node version: $ node -vv8. 9. 3$ npm -v5. 5. 1Fixing npm permissions: Fixing npm permissions as seen here. $ npm config get prefix/usr/local$ sudo chown -R $(whoami) $(npm config get prefix)/{lib/node_modules,bin,share}Install create-react-native-app: Go to the official react native docs here. “Create React Native App does not currently work with npm v5. We strongly recommend using npm v3, v4, or a recent version of Yarn”. Install Yarn:: Follow the official Yarn doc here $ brew install yarn --without-node==&gt; Downloading https://yarnpkg. com/downloads/1. 3. 2/yarn-v1. 3. 2. tar. gz==&gt; Downloading from https://github. com/yarnpkg/yarn/releases/download/v1. 3. 2/yarn-v1. 3. 2. tar. gz/usr/local/Cellar/yarn/1. 3. 2: 14 files, 3. 9MB, built in 8 secondsInstall create-react-native-app using Yarn: $ yarn global add create-react-native-appyarn global v1. 3. 2[1/4] Resolving packages. . . [2/4] Fetching packages. . . [3/4] Linking dependencies. . . [4/4] Building fresh packages. . . success Installed  create-react-native-app@1. 0. 0  with binaries: - create-react-native-appCreate a react native app: $ create-react-native-app AwesomeProjectCreating a new React Native app in /Users/. . /AwesomeProject. Start the app: $ cd AwesomeProject$ yarn startyarn run v1. 3. 2Output Error Unable to start server: $ react-native-scripts start23:15:49: Unable to start serverSee https://git. io/v5vcn for more information, either install watchman or run the following snippet:sudo sysctl -w kern. maxfiles=5242880sudo sysctl -w kern. maxfilesperproc=524288error Command failed with exit code 1. info Visit https://yarnpkg. com/en/docs/cli/run for documentation about this command. This page had 2 solutions:  Run the sudo sysctl commands or Install Watchman. Here is more info about Watchman: Watchman is a file watching service open sourced by Facebook. “Watchman exists to watch files and record when they change. It can also trigger actions (such as rebuilding assets) when matching files change. ” The solution with the most thumbs up is this one: $ sudo sysctl -w kern. maxfiles=5242880kern. maxfiles: 12288 -&gt; 5242880$ sudo sysctl -w kern. maxfilesperproc=524288kern. maxfilesperproc: 10240 -&gt; 524288Here is more info about this command: sysctl is used to get or set kernel state. Here is more info about “tuning kernel limits”: It says that “The kern. maxfiles sysctl(8) variable can be raised or lowered based upon system requirements. This variable indicates the maximum number of file descriptors on the system” Here is some more info about this: “By default, the maximum number of files that Mac OS X can open is set to 12,288 and the maximum number of files a given process can open is 10,240. ” It also says that if you reboot, that it will go back to the original values. Although there is a workaround as seen on that post. Start development server: $ yarn startIt will show this output: yarn run v1. 3. 2$ react-native-scripts start12:11:00: Starting packager. . . Packager started!To view your app with live reloading, point the Expo app to this QR code. You'll find the QR scanner on the Projects tab of the app. GIANT QR CODE HERE. . . Or enter this address in the Expo app's search bar:exp://10. 0. 0. 10:19000Your phone will need to be on the same local network as this computer. For links to install the Expo app, please visit https://expo. io. Logs from serving your app will appear here. Press Ctrl+C at any time to stop. › Press a to open Android device or emulator, or i to open iOS emulator. › Press q to display QR code. › Press r to restart packager, or R to restart packager and clear cache. › Press d to toggle development mode. (current mode: development)Install the Expo client app: As seen in the Expo doc here: Download the app on your phone. Then scan the QR code on your terminal. Back in the Terminal now says: 12:17:51: Finished building JavaScript bundle in 69956ms12:17:58: Running app on Tom O in development modeTo stop use Ctrl+C. 12:36:05: Stopping packager. . . 12:36:06: Packager stopped. "
    }, {
    "id": 207,
    "url": "https://www.tomordonez.com/regex-remove-lines-not-containing-character/",
    "title": "Regex Remove Lines Not Containing a Character",
    "body": "2017/12/03 - Use this regex pattern to remove lines not containing a character. I want to remove lines that don’t contain a space. ^(?!. * . *). +$Or remove lines that don’t contain numbers. ^(?!. *[0-9]. *). +$Or remove lines that don’t contain UPPER CASE. ^(?!. *[A-Z]. *). +$Or remove lines that don’t contain letters ^(?!. *[A-Za-z]. *). +$Or remove lines that don’t contain an email ^(?!. *@. *). +$"
    }, {
    "id": 208,
    "url": "https://www.tomordonez.com/aws-cli-help-page/",
    "title": "AWS CLI Help Pages",
    "body": "2017/12/03 - How to open the AWS man page or AWS help page through CLI When you SSH to an AWS EC2 and you try this: man awsman ec2It shows No manual entry for aws. As seen here. The way to open the man page or the help page is with this: aws helpaws ec2 helpaws ec2 run-instances help "
    }, {
    "id": 209,
    "url": "https://www.tomordonez.com/install-dropbox-linux/",
    "title": "Installing Dropbox on Linux",
    "body": "2017/12/01 - Updated on Jan 11, 2020 Follow this short tutorial for installing Dropbox on Linux Go here and choose an installer. There are 3 install options. Choose the one that applies to you:  Ubuntu Fedora Compile from sourceThen you have to install the dropbox deamon. As seen on the source above. For example, for 64-bit you need to run this: $ cd ~ &amp;&amp; wget -O -  https://www. dropbox. com/download?plat=lnx. x86_64  | tar xzf -Then run it with: $ ~/. dropbox-dist/dropboxdToo Many Dropbox Directories: If you have too many directories. More than 10,000. Dropbox will stop running. Stop dropbox: $ dropbox stopRun this: $ echo fs. inotify. max_user_watches=100000 | sudo tee -a /etc/sysctl. conf; sudo sysctl -pRestart Dropbox: $ dropbox startInstall Dropbox Tray Icon: The Dropbox icon might not show up on the tray. You need to install a Gnome extension. First stop dropbox: $ dropbox stopOn Chrome. Install GNOME shell extensions If you don’t have it installed. When you open this page. There will be a notification towards the top that says: To control GNOME Shell extensions using this site you must installGNOME Shell integration that consists of two parts:browser extension and native host messaging application. Click here to install browser extension. Go to Click here to install browser extension. A popup opens asking Add GNOME shell integration?. Hit Add extension. A new notification (warning) message now shows: Although GNOME Shell integration extension is running,native host connector is not detected. Install chrome-gnome-shell. Applicable to Ubuntu or Fedora. Mine is Fedora: $ sudo dnf install chrome-gnome-shellInstall the GNOME Shell extension TopIcons Fix: In a previous Fedora version, this extension used to work: TopIcons Plus. Link here I am currently on Fedora 31 with GNOME 3. 34. 2. I removed TopIcons Plus and installed this TopIcons Fix. Link here Restart dropbox: $ dropbox startSync specific folder: Now the Dropbox icon should be on the taskbar. Go to:  Dropbox icon Preferences Sync Selective SyncThen choose the folders that you want to sync. Dropbox CLI command line: On the command line run this: $ dropbox helpOutput: commands:Note: use dropbox help &lt;command&gt; to view usage for a specific command. autostart  automatically start Dropbox at login exclude   ignores/excludes a directory from syncing filestatus  get current sync status of one or more files help     provide help lansync   enables or disables LAN sync ls      list directory contents with current sync status proxy    set proxy settings for Dropbox puburl    get public url of a file in your Dropbox's public folder running   return whether Dropbox is running sharelink  get a shared link for a file in your Dropbox start    start dropboxd status    get current status of the dropboxd stop     stop dropboxd throttle   set bandwidth limits for Dropbox update    download latest version of Dropbox version   print version information for DropboxUpdate dropbox with this: $ dropbox updateversion information for DropboxUpdate dropbox with this: $ dropbox update"
    }, {
    "id": 210,
    "url": "https://www.tomordonez.com/install-tmux-linux/",
    "title": "How To Install Tmux on Linux",
    "body": "2017/11/22 - Follow this tutorial to install Tmux on Linux. $ sudo apt-get install tmuxOr for Fedora use dnf. Now type tmux and it should open the Tmux interface. If instead you get this [exited] Open your tmux configuration (see below about Configuring Tmux): $ vim ~/. tmux. confIf you have this line. Then either remove it or add it as a comment for future reference. It was: set-option -g default-command  reattach-to-user-namespace -l $SHELL Now it is: # set-option -g default-command  reattach-to-user-namespace -l $SHELL To learn more about Tmux. You should get a book called “Tmux Productive Mouse Free Development”. Configuring Tmux: Tmux has the config file . tmux. conf. If after installing is not in your home directory. Then create this file. The following configuration is from the Tmux book mentioned above: set -g default-terminal  xterm set -g prefix C-aunbind C-bset -s escape-time 1set -g base-index 1setw -g pane-base-index 1bind r source-file ~/. tmux. conf \; display  Reloaded! bind C-a send-prefixbind | split-window -hbind - split-window -vbind h select-pane -Lbind j select-pane -Dbind k select-pane -Ubind l select-pane -Rbind -r C-h select-window -t :-bind -r C-l select-window -t :+bind -r H resize-pane -L 5bind -r J resize-pane -D 5bind -r K resize-pane -U 5bind -r L resize-pane -R 6set -g default-terminal  screen-256color set-option -g status-bg colour235 #base02set-option -g status-fg colour136 #yellowset-option -g status-attr defaultset-window-option -g window-status-fg colour244 #base0set-window-option -g window-status-bg defaultset-window-option -g window-status-current-fg colour166 #orangeset-window-option -g window-status-current-bg defaultset-option -g pane-border-fg colour235 #base02set-option -g pane-active-border-fg colour240 #base01 set-option -g message-bg colour235 #base02set-option -g message-fg colour166 #orangeset-option -g display-panes-active-colour colour33 #blueset-option -g display-panes-colour colour166 #orangeset-window-option -g clock-mode-colour colour64 #greenset -g status-left-length 40set -g status-left  #[fg=green]SomeNameHere: #S #[fg=yellow]#I #[fg=cyan]#P set -g status-right  #[fg=cyan]%d %b %R set -g status-justify centreset -g monitor-activity onset -g visual-activity onsetw -g mode-keys viRemap the Caps Lock with Ctrl: Since Tmux uses shortcuts to navigate different panes and windows. You need to remap your Caps Lock key so that it behaves as a Ctrl key…you don’t have to but it’s easier to attach and detach tmux sessions using Caps Lock + a. Than doing Ctrl + a. $ setxkbmap -option caps:ctrl_modifierTo change back to the original settings: $ setxkbmap -optionA stuck Caps Lock might happen when you remap the caps lock when it was enabled. To solve this, change back to the original settings and remap when the caps lock is not enabled. Install Tmuxinator to manage Tmux sessions: This requires that you have Ruby installed. Follow this to install Ruby on Ubuntu. For Fedora, the tutorial is almost the same. $ gem install tmuxinatorOpen a configuration file for a Tmuxinator session $ tmuxinator open awesome-projectEdit the settings as you want or go here to learn more about configuring Tmuxinator. Open a tmuxinator session like this: $ tmuxinator awesome-projectTo dettach from the session use the remapped key shortcut CapsLock+a then d. To get a list of all tmuxinator sessions do: $ tmuxinator listGet the book “Tmux Productive Mouse Free Development” to understand in more detail how Tmux works. "
    }, {
    "id": 211,
    "url": "https://www.tomordonez.com/open-source-crm-python/",
    "title": "Open Source CRM Python Tutorial",
    "body": "2017/11/19 - In this tutorial I will show you how to install an open source CRM that uses Python.  Install PostgreSQL on Fedora: $ sudo dnf install -y postgresql-serverInstalling: postgresql-server (9. 6. 5-1. fc26)Installing dependencies: postgresqlInstall PostgreSQL on Ubuntu: Similar process but in Ubuntu. $ sudo apt-get -y install postgresqlSee docs for PostgreSQL on Ubuntu here and PostgreSQL on Windows here. Initialize PostgreSQL: $ sudo postgresql-setup initdbWARNING: using obsoleted argument syntax, try --helpWARNING: arguments transformed to: postgresql-setup --initdb --unit postgresql* Initializing database in '/var/lib/pgsql/data'* Initialized, logs are in /var/lib/pgsql/initdb_postgresql. logOpen source CRM preview: Start PostgreSQL: $ sudo systemctl enable postgresqlCreated symlink /etc/systemd/system/multi-user. target. wants/postgresql. service → /usr/lib/systemd/system/postgresql. service. $ sudo systemctl start postgresqlNo outputInstall yum-utils: $ sudo dnf install yum-utilsLast metadata expiration check: 0:55:00 ago on Sun 19 Nov 2017 05:55:48 PM EST. Package yum-utils-1. 1. 31-512. fc26. noarch is already installed, skipping. Dependencies resolved. Nothing to do. Complete!Add the repository of the open source CRM: $ sudo dnf config-manager --add-repo=https://nightly. odoo. com/11. 0/nightly/rpm/odoo. repoAdding repo from: https://nightly. odoo. com/11. 0/nightly/rpm/odoo. repoLook at this preview: Install Python CRM: $ sudo dnf install -y odoo  Click to see Output        Installing:   odoo  11. 0. post20171119-1odoo-nightly 96 M  Installing dependencies:   babel  2. 3. 4-5. fc26   graphviz  2. 40. 1-4. fc26   gts  0. 7. 6-30. 20121130. fc26   lasi  1. 1. 2-7. fc26   libxslt-python  1. 1. 29-1. fc26   netpbm  10. 80. 00-2. fc26   nodejs-less  2. 7. 2-2. fc26   pychartnoarch     1. 39-22. fc26   pyparsing  2. 1. 10-3. fc26   python-libxml2  2. 9. 4-2. fc26   python2-pyparsing  2. 1. 10-3. fc26   python3-PyPDF2  1. 25. 1-15. fc26   python3-PyYAML  3. 12-3. fc26   python3-babel  2. 3. 4-5. fc26   python3-dateutil  1:2. 6. 0-3. fc26   python3-docutils  0. 13. 1-4. fc26   python3-feedparser  5. 2. 1-1. fc26   python3-funcsigs  1. 0. 2-5. fc26   python3-gevent  1. 1. 2-3. fc26   python3-greenlet  0. 4. 11-2. fc26   python3-html2text  2016. 9. 19-2. fc26   python3-jinja2  2. 9. 6-1. fc26   python3-mock  2. 0. 0-4. fc26   python3-num2words  0. 5. 4-2. fc26   python3-ofxparse  0. 16-2. fc26   python3-passlib  1. 7. 0-4. fc26  python3-pbr  1. 10. 0-4. fc26   python3-psutil  5. 0. 1-2. fc26   python3-psycopg2  2. 6. 2-4. fc26   python3-pyasn1-modules  0. 2. 3-1. fc26   python3-pydot  1. 0. 28-15. fc26   python3-pyldap  2. 4. 35. 1-2. fc26   python3-pyserial  3. 1. 1-3. fc26   python3-pyusb  1. 0. 0-4. fc26   python3-qrcode  5. 1-6. fc26   python3-qrcode-core  5. 1-6. fc26   python3-reportlab  3. 3. 0-4. fc26   python3-stdnum  1. 3-4. fc26   python3-suds  0. 7-0. 4. 94664ddd46a6. fc26   python3-vatnumber  1. 2-5. fc26   python3-vobject  0. 9. 4. 1-2. fc26   python3-werkzeug  0. 11. 10-5. fc26   python3-xlrd  1. 0. 0-6. fc26   python3-xlwt  1. 1. 2-2. fc26   xorg-x11-fonts-ISO8859-1-100dpi 7. 5-17. fc26    Install 46 Packages    Total download size: 113 M  Installed size: 477 M  Running transaction    Preparing:  Installing: python3-suds-0. 7-0. 4. 94664ddd46a6. fc26. noarch  Installing: python3-greenlet-0. 4. 11-2. fc26. x86_64  Installing: python3-dateutil-1:2. 6. 0-3. fc26. noarch  Installing: python3-babel-2. 3. 4-5. fc26. noarch  Installing: babel-2. 3. 4-5. fc26. noarch  Installing: python3-vobject-0. 9. 4. 1-2. fc26. noarch  Installing: python3-gevent-1. 1. 2-3. fc26. x86_64  Installing: python3-vatnumber-1. 2-5. fc26. noarch  Installing: netpbm-10. 80. 00-2. fc26. x86_64  Running scriptlet: netpbm-10. 80. 00-2. fc26. x86_64  Installing: gts-0. 7. 6-30. 20121130. fc26. x86_64  Running scriptlet: gts-0. 7. 6-30. 20121130. fc26. x86_64  Installing: python3-xlrd-1. 0. 0-6. fc26. noarch  Installing: xorg-x11-fonts-ISO8859-1-100dpi-7. 5-17. fc26. noarch  Running scriptlet: xorg-x11-fonts-ISO8859-1-100dpi-7. 5-17. fc26. noarch  Installing: lasi-1. 1. 2-7. fc26. x86_64  Running scriptlet: lasi-1. 1. 2-7. fc26. x86_64  Installing: graphviz-2. 40. 1-4. fc26. x86_64  Running scriptlet: graphviz-2. 40. 1-4. fc26. x86_64  Installing: python3-pydot-1. 0. 28-15. fc26. noarch  Installing: python3-qrcode-core-5. 1-6. fc26. noarch  Installing: python3-qrcode-5. 1-6. fc26. noarch  Installing: python3-pyasn1-modules-0. 2. 3-1. fc26. noarch  Installing: python3-pyldap-2. 4. 35. 1-2. fc26. x86_64  Installing: python3-pbr-1. 10. 0-4. fc26. noarch  Installing: python3-funcsigs-1. 0. 2-5. fc26. noarch  Installing: python3-mock-2. 0. 0-4. fc26. noarch  Installing: python2-pyparsing-2. 1. 10-3. fc26. noarch  Installing: python2-pyparsing-2. 1. 10-3. fc26. noarch  Installing: pyparsing-2. 1. 10-3. fc26. noarch  Installing: python-libxml2-2. 9. 4-2. fc26. x86_64  Installing: libxslt-python-1. 1. 29-1. fc26. x86_64  Installing: python3-xlwt-1. 1. 2-2. fc26. noarch  Installing: python3-werkzeug-0. 11. 10-5. fc26. noarch  Installing: python3-stdnum-1. 3-4. fc26. noarch  Installing: python3-reportlab-3. 3. 0-4. fc26. x86_64  Installing: python3-pyusb-1. 0. 0-4. fc26. noarch  Installing: python3-pyserial-3. 1. 1-3. fc26. noarch  Installing: python3-psycopg2-2. 6. 2-4. fc26. x86_64  Installing: python3-psutil-5. 0. 1-2. fc26. x86_64  Installing: python3-passlib-1. 7. 0-4. fc26. noarch  Installing: python3-ofxparse-0. 16-2. fc26. noarch  Installing: python3-num2words-0. 5. 4-2. fc26. noarch  Installing: python3-jinja2-2. 9. 6-1. fc26. noarch  Installing: python3-html2text-2016. 9. 19-2. fc26. noarch  Installing: python3-feedparser-5. 2. 1-1. fc26. noarch  Installing: python3-docutils-0. 13. 1-4. fc26. noarch  Installing: python3-PyYAML-3. 12-3. fc26. x86_64  Installing: python3-PyPDF2-1. 25. 1-15. fc26. noarch  Installing: pychart-1. 39-22. fc26. noarch  Installing: nodejs-less-2. 7. 2-2. fc26. noarch  Installing: odoo-11. 0. post20171119-1. noarch  Running scriptlet: odoo-11. 0. post20171119-1. noarch    Installed:    odoo. noarch 11. 0. post20171119-1  babel. noarch 2. 3. 4-5. fc26  graphviz. x86_64 2. 40. 1-4. fc26  gts. x86_64 0. 7. 6-30. 20121130. fc26  lasi. x86_64 1. 1. 2-7. fc26  libxslt-python. x86_64 1. 1. 29-1. fc26  netpbm. x86_64 10. 80. 00-2. fc26  nodejs-less. noarch 2. 7. 2-2. fc26  pychart. noarch 1. 39-22. fc26  pyparsing. noarch 2. 1. 10-3. fc26  python-libxml2. x86_64 2. 9. 4-2. fc26  python2-pyparsing. noarch 2. 1. 10-3. fc26  python3-PyPDF2. noarch 1. 25. 1-15. fc26  python3-PyYAML. x86_64 3. 12-3. fc26  python3-babel. noarch 2. 3. 4-5. fc26  python3-dateutil. noarch 1:2. 6. 0-3. fc26  python3-docutils. noarch 0. 13. 1-4. fc26  python3-feedparser. noarch 5. 2. 1-1. fc26  python3-funcsigs. noarch 1. 0. 2-5. fc26  python3-gevent. x86_64 1. 1. 2-3. fc26  python3-greenlet. x86_64 0. 4. 11-2. fc26  python3-html2text. noarch 2016. 9. 19-2. fc26  python3-jinja2. noarch 2. 9. 6-1. fc26  python3-mock. noarch 2. 0. 0-4. fc26  python3-num2words. noarch 0. 5. 4-2. fc26  python3-ofxparse. noarch 0. 16-2. fc26  python3-passlib. noarch 1. 7. 0-4. fc26  python3-pbr. noarch 1. 10. 0-4. fc26  python3-psutil. x86_64 5. 0. 1-2. fc26  python3-psycopg2. x86_64 2. 6. 2-4. fc26  python3-pyasn1-modules. noarch 0. 2. 3-1. fc26  python3-pydot. noarch 1. 0. 28-15. fc26  python3-pyldap. x86_64 2. 4. 35. 1-2. fc26  python3-pyserial. noarch 3. 1. 1-3. fc26  python3-pyusb. noarch 1. 0. 0-4. fc26  python3-qrcode. noarch 5. 1-6. fc26  python3-qrcode-core. noarch 5. 1-6. fc26  python3-reportlab. x86_64 3. 3. 0-4. fc26  python3-stdnum. noarch 1. 3-4. fc26  python3-suds. noarch 0. 7-0. 4. 94664ddd46a6. fc26  python3-vatnumber. noarch 1. 2-5. fc26  python3-vobject. noarch 0. 9. 4. 1-2. fc26  python3-werkzeug. noarch 0. 11. 10-5. fc26  python3-xlrd. noarch 1. 0. 0-6. fc26  python3-xlwt. noarch 1. 1. 2-2. fc26  xorg-x11-fonts-ISO8859-1-100dpi. noarch 7. 5-17. fc26  Start the open source CRM: $ sudo systemctl enable odooCreated symlink /etc/systemd/system/multi-user. target. wants/odoo. service → /usr/lib/systemd/system/odoo. service. $ sudo systemctl start odooRunning the CRM: Browse to: localhost:8069The first screen you will see has this URL: localhost:8069/web/database/selectorIt will show you this screen to setup the database: You will have the option to Load demonstration data. Open source CRM demo data: If you choose the option to load demonstration data, it will send you to the Apps screen.  Go to CRM and click install. The next screen will be the Inbox.  From here you can navigate the top menu:  Discuss Calendar Contacts CRM Apps SettingsOnce you review all demo data, you might be inclined to delete all demo data and start with your own data. Create a new database: You can create a new database. Use this URL to select databases http://localhost:8069/web/database/selectorUse this URL to manage databases: http://localhost:8069/web/database/manager In my example. When I created the database with the demo data, I named it crm. If you hit delete. You will get this screen: When you confirm deletion. You will go back to this screen: Once the database is created. It will go back to Apps where you can install the CRM module. You will now have a clean install of the CRM.  Open source CRM documentation: Follow the official documentation for Odoo open source CRM here Other open source CRM:  Django-CRM React Redux CRM FatFreeCRM Ruby openCRX Java CiviCRM Spark"
    }, {
    "id": 212,
    "url": "https://www.tomordonez.com/additional-second-hard-drive-ubuntu/",
    "title": "Additional Second Hard Drive in Ubuntu",
    "body": "2017/11/10 - I got a refurbished Thinkpad T430 for very cheap. It came with 250GB SSD but wanted to upgrade it to 500GB. When I got the computer and opened the hard drive panel, I noticed that it was empty. I thought that maybe I got scammed. After much head banging I learned that the hard drive was stored on the RAM panel. I first thought was a mini wireless card. But reading the label it said 256 GB. Uh? I learned it was an mSATA SSD. I couldn’t fit the new SSD in the hard drive panel cause it was missing the tray. I installed Ubuntu on the current drive. Then I got a Caddy adapter that replaces the DVD drive with a way to insert another hard drive. That was pretty easy to install. Now you need to configure the hard drive. I followed this from the Ubuntu docs. Although it has a section that is outdated. Find the logical name of the new drive: $ sudo lshw -C diskThis means: lshw List hardware. -C with Class of type disk. I identified the new disk because it said product: Crucial. Which is the brand I got. The size was 525GB. And it didn’t say partitioned, while the other one said capabilities: partitioned. The logical drive of the new SSD was /dev/sda Partition the disk using GParted: I started the process using the Terminal instructions. But got confused half way. I closed that and I decided to use the GUI. Then it took 5 seconds. The tool should be under Applications/System Tools/Administration In my case it wasn’t there so I installed it with: $ sudo apt install gpartedAfter installing. I opened the tool and it asked for the root password. When the program opens. There is a drop down to select the drive you want. I chose the new drive. In my case it was /dev/sda Create a partition table: Go to Device/Create Partition Table Select msdos. Then I clicked on the green check mark to apply. Create a partition: Then I right clicked on the white rectangle that said my drive name.  Selected: New Chose: Primary Partition.  Filesystem: ext4 Add.  Green check mark to apply. Change the label of the drive: When I completed the process I realized that under Places my drive was listed as 525 GB. . . . I right clicked on the white rectangle again and hit Label File System. I entered an awesome name that I could remember. Create a mount point: $ sudo mkdir /media/name_of_new_driveFor the name_of_new_drive I used the same name as the label name. Now you need to find the UUID of the new drive as mentioned here. UUID means Universal Unique Identifier. $ sudo blkidIt says that blkid is used to locate device attributes. The result was something like this: /dev/sdb1: UUID= bunch of numbers with letters here  TYPE= ext4  PARTUUID= some other number here /dev/sdb5: UUID= another bunch of numbers  TYPE= swap  PARTUUID= other number here /dev/sda1: LABEL= awesome name I chose  UUID= number I need  TYPE= ext4  PARTUUID= number here Now you need to edit the file fstab like this: sudo nano -Bw /etc/fstabThe -B is used to create a backup of the file under /etc/fstab~. The w is used to disable wrap of long lines. Then I added this line to the end of the file: UUID= number I needed from above  /media/name_of_new_drive ext4 defaults 0 2For UUID above you don’t need to put the quotes. Then quit the file with Ctrl X. It will ask if you want to save. Enter Yes. Mount all disks: The new disk should be mounted but just in case: sudo mount -aRestart and Update the BIOS: I restarted and realized that it wouldn’t boot. I flipped out. I had enabled the Virtualization settings so I thought that probably I broke something. It was booting from the 2nd hard drive. Change the BIOS and make sure the sequence order starts with your main drive. Otherwise it won’t boot. Last note: Inside the fstab I noticed the following on my main SSD: errors=remount-roNot sure what that is. "
    }, {
    "id": 213,
    "url": "https://www.tomordonez.com/virtualbox-resize-linux-guest-storage-vdi/",
    "title": "VirtualBox Resize Linux Guest Storage VDI",
    "body": "2017/11/05 - This tutorial applies to Virtualbox when Windows is the host and Linux is the VM guest. Setup:  Windows 10 host Virtualbox Ubuntu Linux guest or Fedora Linux guestYou also need to download the ISO file from Gparted. You can download it from here. This file is about 200MB. It should take 1-10m depending on your Internet speed. The file might say gparted live i686. iso. Video Tutorial: 	Backup the VDI file: Open Virtualbox. The Virtual Machine should NOT be running. If it’s running just shut it down. Right click the Virtual Machine. Then Settings. Go to Storage. Under Controller: SATA. Click on the . vdi file. On the right there is a field that says Location. Copy/paste this somewhere (Sublime) In my case this location is: C:\Users\neo\VirtualBox VMs\Ubuntu\Ubuntu. vdiOpen that folder. Copy paste the vdi file to another location. Perhaps Downloads. You can use this as a backup in case you mess something up :) Resize the VDI file: Close VirtualBox We are going to resize the current vdi file. NOT the backup. Open the command prompt. If you go to the Windows search and type command you should find it. Go to this directory: cd C:\Program Files\Oracle\VirtualBoxYou have to resize the file in MBs. Get your calculator. Let’s say that your current file is 10GBs. You want to resize it to 20GBs? Calculate 1024 * 20. The result is 20480. You are going to need this number and the location of your vdi file. Go to the command prompt and follow this formula: VBoxManage modifyhd  Location  --resize RESULTIn my case:  Location: C:\Users\neo\VirtualBox VMs\Ubuntu\Ubuntu. vdi Result: 20480The command should be: VBoxManage modifyhd  C:\Users\neo\VirtualBox VMs\Ubuntu\Ubuntu. vdi  --resize 20480It should show an output such as: 0%. . . 10%. . . 20%. . . 30%. . . Then go back to the prompt. You can close the prompt for now. Open VirtualBox. Select the VM but don’t start it. On the Preview specs on the right side. Look at Storage. Under Controller: SATA it should have changed the size of the storage drive. In my case it would say 20GB. (It used to say 10GB). Load Gparted: Go back to VirtualBox. Right click on the VM. Then Settings. Go to Storage. Click on Controller: IDE. If this is not listed. You could create one. There is a little Plus icon towards the bottom of this window. Actually there are 2 icons. The one closest to the right is a tilted square. Click on this. There is an option to Add IDE Controller. Once you click on Controller: IDE. Click on the Plus icon that is closest to the left. Click the option Add Optical Drive. On the popup box, click on Choose disk. Find the Gparted iso file that you downloaded previously. Then click Open. Click on this drive. On the right side choose these options:  Optical Drive: IDE Primary Master Live CD: Checked. Click OK to save. And close the settings window. Resize the Partition with Gparted: Start the VM. It should load Gparted. There are a few options when it starts. Just hit Enter to everything until you login to what it looks like a Linux VM. By default it starts Gparted automatically. The screen should show 3 partitions:  /dev/sda1: root /dev/sda2: swap unallocatedIn my example my original storage drive was 10GB and I resized it to 20GB. You will see that:  /dev/sda1 size is 6GB /dev/sda2 size is 4GBIf you setup the VM with 4GB of RAM. Well this used by a swap partition. In this case /dev/sda2. All your files should be in the root partition. But you will see that the unallocated partition is on the opposite side of root with swap in the middle. In other words. You need to resize root but swap is on the way. We need to delete swap. Resize root to the unallocated side. Type in the swap size again. And build a new swap partition. Follow these steps. Thanks to this source:  Right click the swap partition Click on Swapoff. (if this option is not there, continue) Right click the swap partition and Click on Delete On the top menu. Click the Apply.  Right click on the Extended file system that had the swap partition and delete it.  Right click on the root partition /dev/sda1.  Use the mouse to drag and resize to take all the unallocated space.  Inside the field Free space following enter the size of the swap partition. In my example this is 4GB. So type 4096.  Hit tab to see how it auto-resizes the total. Click Save or OK. Let’s create the swap partition again.  Right click unallocated.  Select extended partition. Click OK Right click on this new partition.  In File system. Select linux-swap. Click OK On the top menu select Apply.  Right click on the swap partition and hit swapon. Quit Gparted. File, exit. Then on the desktop, hit the red turn off button. When shutting down, it might say to remove the CD and hit Enter. Do that. It might take 5m for this to turn off. Change Gparted settings: Right click on the VM, then Settings. Go to Storage. Under Controller: IDE select gparted. Set the Optical Drive to IDE Secondary Master or IDE Secondary Slave. Uncheck Live CD. Click OK. Start the VM: We should be good now. Start the VM and it should start as usual. Verify that all files remain there. "
    }, {
    "id": 214,
    "url": "https://www.tomordonez.com/linkedin-xray-search-googler/",
    "title": "Linkedin X-Ray Search with Googler",
    "body": "2017/11/04 - Linkedin XRay search with Googler. Or Google search anything in a programmatic way. Installation for Mac: Requires Python 3. 3 or later $ git clone https://github. com/jarun/googler/$ sudo make installInstallation for Linux: $ git clone https://github. com/jarun/googler/$ cd googlerVideo Tutorial:   Running googler: It runs as a standalone program: $ . /googlerIf you run it with no options you will get this: Please initiate a query. googler (? for help)To exit hit Enter twice. Options: To show the help documentation: $ . /googler -h &nbsp; &nbsp; This will show an output with all the options. Some are explained here… Show N results: Use the option -n to get a number of results like this: $ -n 50Show results from the News section: Many options are case sensitive. If you want to get results from the news section use this: $ -NShow country specific search: Use this option to get the results that are country specific using the top level domain (TLD). For example if you want results from Canada: $ -c caOr results from Brazil: $ -c brThese are a few popular TLDs:  Worldwide,com,google. com Brazil,br,google. com. br Canada,ca,google. ca Chile,cl,google. cl Colombia,co,google. com. co France,fr,google. fr Mexico,mx,google. com. mx Netherlands,nl,google. nl United Kingdom,uk,google. co. uk United States,us,google. usShow results in a specific language:: Use this option to show results in a language. For example, Spanish: $ -l esTo get the correct syntax search “List of ISO 639-1 codes”  English: en Spanish: es Portugues: ptDisable automatic spelling correction: Use this option: $ -xDisable color output: This will be helpful to save the results to a file. You don’t have to understand what this is but just be aware that it’s important. $ -CSearch with a time limit: Use this option: $ -tUsing this syntax:  h5 = 5 hours d5 = 5 days w5 = 5 weeks m5 = 5 months y5 = 5 yearsSearch a specific site: Use the option: $ -wOr also this one: $ --siteFor example: $ . /googler --site linkedin. com/in 'data scientist' &nbsp; &nbsp; The default number of results is 10. Let’s change that to 20: $ . /googler --site linkedin. com/in -n 20 'data scientist'Disable user agent: This is important. The program uses the user agent to simulate Firefox on Ubuntu: USER_AGENT = ('Mozilla/5. 0 (X11; Ubuntu; Linux x86_64; rv:56. 0) Gecko/20100101 Firefox/56. 0')If you want to disable it for any reason use this option: --nouaTo get output in JSON format: Use this option for JSON format --jsonSearch and Exit: This is important when saving the output to a file. It will search and exit. It will not show the prompt. -npTo search with filetype: This is the same as searching using Google: $. /googler 'data science' filetype:pdfSaving the results to a file: To understand how saving works, you need to know some basic Linux commands. Read my tutorial from zero to hero in Linux. Save the results to a text file: $. /googler -C 'data science' filetype:pdf &gt; dm. txtWhen it’s done, the prompt will move down twice. Hit Enter twice to exit. The option -C disables color output. Otherwise you will get a mess of data. You don’t have to understand what this means. Just use it :) Linkedin Xray Search: Ok Padawan. Now let’s do a few more examples: $ . /googler -C -w linkedin. com/in -n 50 'machine learning' &gt; ml_li. csvOpen the file with Sublime Text. You can download Sublime from here. You could convert this “raw file” into a CSV file using a Regex macro. Read my tutorial to create a regex macro in Sublime. You can use this macro to convert the Linkedin Xray search result to a CSV file.  &nbsp; &nbsp; For the Rules-User: {   format :  3. 0 ,   replacements : {    // replace comma with semicolon     linkedin_url_replace_comma : {       find :  , ,       replace :  ; ,       greedy : true    },    // add 3 empty lines between profiles     linkedin_url_3_lines : {       find :  ^\\n ,       replace :  \\n\\n\\n ,       greedy : true    },    // join url, city and description     linkedin_url_join_url_city : {       find :  ^(http. *)\\n(. *)\\n(. *) ,       replace :  \\1,\\2,\\3 ,       greedy : true    },    // join name with url     linkedin_url_join_name_url : {       find :   \\| Professional. *\\n ,       replace :  , ,       greedy : true    },    // remove leading number     linkedin_url_leading_number : {       find :  ^ [0-9]{1,3}  ,       replace :   ,       greedy : true    },    // remove empty lines     linkedin_url_remove_empty_lines : {       find :  ^\\n ,       replace :   ,       greedy : true    },    // remove last googler line     linkedin_url_remove_googler_line : {       find :  ^googler. * ,       replace :   ,       greedy : true    }  }}For the Commands-User [  {     caption :  Reg Replace: Googler Linkedin CSV ,     command :  reg_replace ,     args : { replacements : [ linkedin_url_replace_comma ,     linkedin_url_3_lines , linkedin_url_join_url_city ,     linkedin_url_join_name_url , linkedin_url_leading_number ,     linkedin_url_remove_empty_lines ,     linkedin_url_remove_googler_line ]}  }] &nbsp; &nbsp; Activate the macro with the shortcut Ctrl+Shift+P and typing Reg Replace: Googler Linkedin CSV. Then open the CSV file in Excel. "
    }, {
    "id": 215,
    "url": "https://www.tomordonez.com/create-a-regex-macro-in-sublime/",
    "title": "Create a Regex Macro in Sublime",
    "body": "2017/11/03 - Follow this tutorial to create a regex macro in Sublime. Install Sublime: You need to have Sublime right? Download from here. Learn some JSON: You need to be familiar with JSON. Install Sublime Package Control: You need to have Sublime Package Control installed. On the top menu Preferences/Package Control. (It will install it by clicking on it the first time). Install RegReplace: You need to have RegReplace installed. Use the shortcut Ctrl+Shift+p. This opens a sort of popup menu. Then type Package Control: Install Package and hit Enter. Type RegReplace. Then select it. It will open a new file that says: # RegReplace Welcome to RegReplace! For a quick start guide, please go to`Preferences-&gt;Package Settings-&gt;RegReplace-&gt;Quick Start Guide`. Learn Python re regex syntax: To build the Macros you must learn the Python re syntax. Which is slightly different from the default regex engine on Sublime. This is the documentation for Python re. Learn which files to edit in Sublime: You need to edit 2 files:  The JSON file with the Find and Replace pairs The JSON file with the Macro commandJSON file with Find and Replace pairs: Go to the top menu Preferences/Package Settings/RegReplace/Rules-User. Follow this syntax to see what type of string and values you can create. You can also read this file in Sublime. To see examples. Go to Preferences/Package Settings/RegReplace/Rules-Example. This is an example I did, when I copy/paste a piece of HTML code into Sublime. It breaks lines at an specific element, removes the first line and replaces the “and” character. {   format :  3. 0 ,   replacements : {    // break into lines     break_lines : {       find :  (&lt;li class=\ card card-user) ,       replace :  \\n\\1 ,       greedy : true    },    // remove first ul line     remove_first_line : {       find :  ^&lt;ul class. *\\n ,       replace :   ,       greedy : true    },    // replace &amp;amp; with and     replace_amp : {       find :  &amp;amp; ,       replace :  and ,       greedy : true    }  }}Each find and replace rule has this: // break into lines break_lines : {  find :  (&lt;li class=\ card card-user) ,  replace :  \\n\\1 ,  greedy : true}, A // comment.  A command name  break_lines .  A pair for find.  A pair for replace. You can add more string/value pairs as seen in the official documentation. Each rule is enclosed with open and close curly braces { rule }. Rules are separated by a comma. The last rule doesn’t have a comma. Save the file. JSON file with the Macro command: Go to the top menu. Preferences/Package Settings/RegReplace/Commands-User This JSON file looks like this: [ { 	 caption :  Reg Replace: Clean HTML into something nice ,   command :  reg_replace ,   args : { replacements : [ break_lines ,  remove_first_line ,  replace_amp ]} }]Following the previous example. The caption is how you can find the Macro. Do Ctrl+Shift+p. Start typing the “caption” name and you will see it listed. For example. I would type: Reg Replace: Clean HTML. . . Running the Macro: My workflow is like this:  Open the raw HTML file.  Do Ctrl+Shift+p.  Find the Macro name.  Click on it and it runs. Building the Macro: There are 2 processes. One is testing the regex in Sublime. Edit the JSON. Run the Macro. Test the next regex. Edit JSON. Run Macro and so on. While this workflow works. It is kind of tedious having to go back and forth and run the whole Macro from the beginning every time you add something new to the JSON file. The official documentation recommends another process here. It allows you to edit the regex rules in a Python panel with highlighted syntax. Github example: You don’t have to understand what this does exactly but just follow the regex examples to see how you can apply them to your own macro. Inside the Rules User: {   format :  3. 0 ,   replacements : {     github_get_contributor_url_remove_comma : {       find :  , ,       replace :   ,       greedy : true    },     github_get_contributor_url_join_lines : {       find :  (^#. *)\\n(. *) ,       replace :  \\1,\\2 ,       greedy : true    },     github_get_contributor_url_remove_lines : {       find :  ^(?!. *#. *). +$\\n ,       replace :   ,       greedy : true    },     github_get_contributor_url_remove_pound_sign : {       find :  ^# ,       replace :   ,       greedy : true    },     github_get_contributor_url_remove_commits_word : {       find :  commits / ,       replace :  , ,       greedy : true    },     github_get_contributor_url_remove_plus_sign : {       find :  \\+\\+ / ,       replace :  , ,       greedy : true    },     github_get_contributor_url_remove_minus_sign : {       find :  -- ,       replace :   ,       greedy : true    },     github_get_contributor_url_insert_github_url : {       find :  ^ ,       replace :  https://github. com/ ,       greedy : true    },     github_get_contributor_url_remove_dotcom_number : {       find :  (\\. com/)\\d{1,3} ,       replace :  \\1 ,       greedy : true    },     github_get_contributor_url_remove_spaces : {       find :    ,       replace :   ,       greedy : true    },     github_get_contributor_url_add_first_row_header : {       find :  ^https://github. com/$ ,       replace :  user_url,commits,lines_added,lines_removed ,       greedy : true    },     github_get_contributor_url_remove_last_line : {       find :  https://github. com/&amp;copy;. * ,       replace :   ,       greedy : true    }  }}Go to the top menu Preferences/Package Settings/RegReplace/Commands-User Add this to JSON file [ {   caption :  Reg Replace: Github Get Contributor URL ,   command :  reg_replace ,   args : { replacements : [ github_get_contributor_url_remove_comma ,  github_get_contributor_url_join_lines ,  github_get_contributor_url_remove_lines ,  github_get_contributor_url_remove_pound_sign ,  github_get_contributor_url_remove_commits_word ,  github_get_contributor_url_remove_plus_sign ,  github_get_contributor_url_remove_minus_sign ,  github_get_contributor_url_insert_github_url ,  github_get_contributor_url_remove_dotcom_number ,  github_get_contributor_url_remove_spaces ,  github_get_contributor_url_add_first_row_header ,  github_get_contributor_url_remove_last_line ]} }]Run Macro by doing Ctrl+Shift+P and type “Github Get Contributor URL”. "
    }, {
    "id": 216,
    "url": "https://www.tomordonez.com/terminate-ssh-connection/",
    "title": "Terminate SSH Connection",
    "body": "2017/11/01 - I keep forgetting how to terminate an SSH connection when it gets stuck. I always end up in this page. Type Enter, ~ and then . period. This also works if you vim or nano a file and you come back to the shell and seems to be stuck. Typing this combination will exit. "
    }, {
    "id": 217,
    "url": "https://www.tomordonez.com/top-5-python-scripts-sourcing/",
    "title": "The Top 5 Python Scripts for Sourcing",
    "body": "2017/10/25 - These are my top 5 Python scripts I use for researching and sourcing data. 0. Virtualenv: Before installing any script I recommend setting up a virtual environment for each project. virtualenv is a tool to create isolated Python environments. You can get all the details here. To install virtualenv do: $ pip install virtualenvTest the installation: $ virtualenv --versionFind the directory for your python version. On Linux this is my output: $ which python3/usr/bin/python3$ which python/usr/bin/pythonGo to the script directory. For example: $ cd awesome-directory/Install virtualenv with the python version recommended for that script. For example for python3: $ virtualenv -p /usr/bin/python3 envFor python2. 7: $ virtualenv -p /usr/bin/python envThis will create a virtual environment with the selected python version and create a directory called env. To activate the environment do: $ source env/bin/activateThis will change the prompt to this: (env) $To exit virtualenv: (env) $ deactivateIt will change the prompt back to just $. 1a. ResumeParser with Anaconda: ResumeParser is an awesome Python scripts to convert PDF resumes to a CSV file. The first step is to install Anaconda. Go to Anaconda for Linux and download the 64 bit x86 file for Python3. The file is about 500MB. It might take around 5 minutes to download. Let’s say you downloaded the file to ~/Downloads Open the terminal and verify data integrity. Use the name of the file you downloaded, in this case I am calling it anaconda. sh: $ sha256sum ~/Downloads/anaconda. shThis will produce a string that you need to match with the corresponding hash string here. Then install: $ bash ~/Downloads/anaconda. shIf prompted about the license agreement. Hit “Enter” to scroll down until you see a prompt where you have to enter “Yes”. Choose the default installation. Prepend install location to PATH. “Yes”. When completed. Enter the command: $ source ~/. bashrcTest that it was installed: $ anaconda-navigatorIf the Navigator opens then it’s installed correctly. To learn more about the Anaconda Navigator go here. Git clone ResumeParser: $ git clone https://github. com/bjherger/ResumeParser. gitThis will create a directory ResumeParser. $ cd ResumeParserThis one follows a different way to configure the virtual environment, because many of the required packages are handled by Anaconda: $ conda env create -f environment. ymlIf this command stops with errors. I recommend to follow the next headline “ResumeParser Legacy” Then activate the environment: $ source activate resumeNow the magic potion is ready. Add resumes to this input directory. Although this directory comes with some defaults for PDF files for testing purposes. You can remove them: data/input/example_resumesTo run the code do: $ cd bin/$ python main. pyOpen the CSV file in this directory: $ open data/output/resume_summary. csvI recommend that you customize the data that you want extracted to your own needs. 1b. ResumeParser Legacy: There is another way to install ResumeParser but the “legacy” source files are not found on the author’s Github project anymore. Follow the same process to download the project. If you are on Ubuntu and you haven’t installed git: $ sudo apt install gitGit clone ResumeParser: $ git clone https://github. com/bjherger/ResumeParser. gitThis will create a directory ResumeParser. $ cd ResumeParserCreate the virtual environment: $ virtualenv -p /usr/bin/python envTo activate the environment do: $ source env/bin/activateCreate a file called requirements. txt and add this content: html2text==2016. 9. 19                          numpy==1. 11. 2                              pandas==0. 19. 0                             pdfminer==20140328                           python-dateutil==2. 5. 3                         pytz==2016. 7                              six==1. 10. 0Clone the file ResumeChecker. py from here: (env) $ git clone https://gist. github. com/bb222f8b39f246d9add0644192e274e1. gitThis will create a directory called bb222f8b39f246d9add0644192e274e1 and inside there is a file called ResumeChecker. py. You need to move this file into the bin directory. (env) $ mv bb222f8b39f246d9add0644192e274e1/ResumeChecker. py bin/Install the dependencies: (env) $ pip install -r requirements. txtNow the magic potion is ready. Add resumes to this input directory. Although this directory comes with some defaults for PDF files for testing purposes. You can remove them: data/input/example_resumesTo run the code do: $ cd bin/$ python ResumeChecker. py --data_path . . /data/input/example_resumes --output_path . . /data/output/resume_summary. csvOpen the CSV file in this directory: $ open data/output/resume_summary. csvCustomize the code for your needs, extracting specific content that you want. For instance I like to create an attribute dashboard. The process is to add columns on the CSV for the keywords you want to extract. The script will count the number of times this keyword appears on the PDF. Name     Java  Machine Learning   AWSElon Musk   2     10       3Homer Simpson 1      2       1Create a list of the keywords that you want to add. This can be any number of keywords. Add those keywords as columns on the CSV file here: data/output/resume_summary. csvOpen the file ResumeChecker. py. Towards the bottom of the file there is code that looks like this: resume_summary_df[ file_path ] = file_listFind the rows that start with: resume_summary_dfScroll down a little bit more and find a section that starts with: Scrape skill informationFollow the format to add the keywords you want. Adding rows with the exact keywords you added on your CSV. For example, if you added Machine Learning. Then you could add a row like this: resume_summary_df[ ml_count ] = resume_summary_df[ raw_text ]. apply(functools. partial(term_count, term=r machine learning ))Once you make all the changes, test the script with a small batch of PDFs. Perhaps up 10 files. Then look at the output CSV to see the results. When you are done using the script. You can deactivate the virtual environment like this: (env) $ deactivate2. Subbrute: Subbrute is a “DNS meta-query spider that enumerates DNS records and subdomains”. Let’s install it and then I will follow with an example: $ git clone https://github. com/TheRook/subbrute. git$ cd subbruteCreate the virtual environment: $ virtualenv -p /usr/bin/python envTo activate the environment do: $ source env/bin/activateRun this example to see what you get: (env) $ . /subbrute. py google. com --type CNAMEIf I run this I get this output: google. comwww. google. comalt1. aspmx. l. google. comalt4. aspmx. l. google. com_spf. google. comIt keeps on running giving more results. You can stop it using Ctrl+C. When you are done using the script. You can deactivate the virtual environment like this: (env) $ deactivate3. Googler: Googler is a Python script…“Google Search, Google Site Search, Google News from the terminal” $ git clone https://github. com/jarun/googler. git$ cd googlerCreate the virtual environment: $ virtualenv -p /usr/bin/python envTo activate the environment do: $ source env/bin/activateRun a search: (env) $ . /googler --count 3 --exact fenderIt will produce these results: *1 Fender Guitars | Electric, Acoustic &amp; Bass Guitars, Amps, Pro Audio**https://www. fender. com/**Since 1946, Fender's iconic Stratocasters, Telecasters and Precision &amp; Jazz bass guitars have transformed nearly every music genre. **2 Fender (@Fender) · Twitter**https://twitter. com/Fender?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor**3 Fender Musical Instruments Corporation - Wikipedia**https://en. wikipedia. org/wiki/Fender_Musical_Instruments_Corporation**Coordinates: 33°38′46″N 111°53′57″W﻿ / ﻿33. 6460322°N 111. 899058°W﻿ / 33. 6460322; -111. 899058. Fender Musical Instruments Corporation (FMIC), . . . *The bottom of the screen will show: googler (? for help)Type ? It will show help options: omniprompt keys:  n, p         fetch the next or previous set of search results index         open the result corresponding to index in browser f           jump to the first page o           open the current search in browser g keywords      initiate a new Google search for ‘keywords’ with original options q, ^D, double Enter  exit googler ?           show omniprompt help *           any other string initiates a new search with original optionsType: site:github. com pythonIt will show 3 results from Github. Type: filetype:pdf pythonIt will show 3 PDF results. Hit the Enter key twice to exit. When you are done using the script. You can deactivate the virtual environment like this: (env) $ deactivate4. Parsero: Parsero is a Python script that reads Robots. txt and looks for disallow entries. $ git clone https://github. com/behindthefirewalls/Parsero. git$ cd ParseroCreate the virtual environment: $ virtualenv -p /usr/bin/python envTo activate the environment do: $ source env/bin/activateSetup the script: (env) $ sudo setup. py installOpen the help options: (env) $ parsero -husage: parsero. py [-h] [-u URL] [-o] [-sb] optional arguments:  -h, –help show this help message and exit -u URL   Type the URL which will be analyzed -o     Show only the “HTTP 200” status code -sb     Search in Bing indexed Disallows -f FILE   Scan a list of domains from a listRun an example: (env) $ parsero -u google. comWhen you are done using the script. You can deactivate the virtual environment like this: (env) $ deactivate5. The Harvester: The Harvester is a Python script used to gather data from public sources. $ git clone https://github. com/laramies/theHarvester. git$ cd theHarvesterCreate the virtual environment: $ virtualenv -p /usr/bin/python envTo activate the environment do: $ source env/bin/activateSet execute permission for the script: (env) $ chmod +x theHarvester. pyInstall dependencies: (env) $ pip install requestsRun with: (env) $ . /theHarvester. pyThis will show the help page and how to use it. Here is an example: (env) $ . /theHarvester. py -d microsoft. com -l 50 -b google -f example1This will search for data on that website. Find 50 results using google and save the results to an html file called example1. If you open the file example1 in your browser, it looks like this: When you are done using the script. You can deactivate the virtual environment like this: (env) $ deactivate"
    }, {
    "id": 218,
    "url": "https://www.tomordonez.com/how-to-setup-vpn-linux-fedora/",
    "title": "How To Setup a VPN in Linux Fedora",
    "body": "2017/10/21 - This is a short tutorial to setup a VPN in Linux Fedora. Although the same process applies to Ubuntu. Open an account with Hide. me VPN: Get the Plus plan Download the OpenVPN Configuration: Once logged in to the members dashboard. Go to the left menu and click on Servers. &nbsp; &nbsp; It will show a Location and Internet Address. Chose the Location you want and click on More Details. Go to OpenVPN Configuration. Click on Linux. This will download a zip file. &nbsp; &nbsp; Install OpenVPN in the Terminal: sudo dnf -y install openvpnUnzip the OpenVPN Configuration file: Let’s say the file is called Location. zip sudo unzip ~/Downloads/Location. zip -d /etc/openvpn/Save your login credentials: If you want to auto login with your user and password. Create this file: sudo touch /etc/openvpn/credentialsOpen this file with your favorite editor using sudo sudo vim /etc/openvpn/credentialsEnter your hideme login and password in 2 separate lines: your-useryour-passwordSetup openvpn to use this credentials file. Previously unzipping the file Location. zip into /etc/openvp/, created a file called Location. ovpn. Change the Location name with the specific file you downloaded. sudo sed -i 's/auth-user-pass/auth-user-pass \/etc\/openvpn\/credentials/g' /etc/openvpn/Location. ovpnStart the VPN: sudo openvpn --config /etc/openvpn/Location. ovpn Test if VPN is working: Go to https://hide. me/en/check. It should show the IP number of the Location you downloaded. But then google this “what is my IP”. If the IP shows your current location and not the one from the VPN then you have an IPv6 issue. Read below. Close the VPN: Use Ctrl+C. If there is a prompt then type: sudo killall openvpn Alternative Way of Adding the VPN: Both Linux Ubuntu and Fedora have a user interface to add the VPN. That way you don’t need to have the VPN running from the shell. And you can select easier from a list of VPNs if you want to configure many of them. If you go to Settings &gt; Network. There should be an option for VPN.  Add VPN Import from file Find the . ovpn file.  Enter your account user and pwd Disable IPv6 Enable the VPN Test your IPIPv6 Issues: In the US. Comcast has IPv6 enabled and this can be an issue. The best solution is to login to your router. Disable IPv6 and enable IPv4. If this is not possible. You can try to disable IPv6 from the command line. But if your router doesn’t have IPv4 enabled this will not work: To disable IPv6 from terminal: sudo sysctl -w net. ipv6. conf. all. disable_ipv6=1To re-enable IPv6: sudo sysctl -w net. ipv6. conf. all. disable_ipv6=0Or edit the file directly with: sudo vim /etc/sysctl. confYou can also disable IPv6 from your laptop Wifi Settings. If you disable IPv6 and you lose internet. Try restarting your laptop Wifi or Wifi card. Verify your IP: Always check your IP with https://hide. me/en/check to review if IPv6 has been disabled. And check the new IP number. IPv6 alternative solution: Use your phone hotspot connecting via USB. Disable the phone Wifi. Connect to the VPN sudo openvpn --config /etc/openvpn/Location. ovpnGo to https://hide. me/en/check. It should show the IP number of the Location you downloaded. Then google “what is my IP”. It should also show the VPN IP number. "
    }, {
    "id": 219,
    "url": "https://www.tomordonez.com/from-zero-to-hero-linux/",
    "title": "From Zero to Hero in Linux",
    "body": "2017/10/03 - From zero to hero in Linux is a tutorial to learn the command line and basic Linux commands. This is a great tutorial if you are:  Learning a programming language Growing tech teams Interested in LinuxYour computer setup: These commands are applicable if you are on Mac or Linux. If you are on Windows. I recommend that you install Virtualbox and Linux. Follow the setup details on this page if you are on Mac or Windows. Video Tutorial:   Open the Terminal: The Terminal is the interface to talk to the computer. If you are on Mac. Open iterm2. If you are on Linux (Virtualbox on Windows). Search for Terminal. The prompt will always show a dollar sign $ with your username and computer name such as user@computer:$. You can change this prompt to show just a dollar sign $. And doing this customization is helpful once you start typing long commands. Let’s look at some Linux commands. Print working directory:: $ pwdThis command is used to show the full path of the current directory. List contents:: $ lsThis one is used to show the contents of the current directory. To know more about how to use a command put the word man before a commmand such as: $ man lsThis is called the manual page aka man page. To navigate this window you can use the arrows up and down or navigate using the keyboard ala vim style. Vim is a text editor that you can open within the Terminal. It has a big learning curve because you cannot use the mouse and only the keyboard. If you want to navigate a man page using what I call is the vim style. Press the key J to go down and the key K to go up. To quit this window just press the key q. A command can have options denoted by a single dash - or two dashes --. The two dashes are used for words and a single dash is used for single letters or numbers. For example: $ ls -aIs the same as: $ ls --allIf you look at the man page for ls you will see that ls -a is explained as “do not ignore entries starting with . ” (…with a period) Files that start with a period are hidden files. Just like in Windows and Mac, some files are hidden from a folder. To view them you have to change the folder settings. Change directory: $ cdThis one has to be used with a parameter. If you want to move up or down a directory tree structure you have to use a special character. Let’s say that I am in this directory: $ pwd/home/tom/Documents/sandboxIf I type this: $ lspicturesIt shows there is another directory called pictures. To change to that directory I need to do this: $ cd pictures$ pwd/home/tom/Documents/sandbox/picturesTo go to the previous directory: $ cd . . To see where you are: $ pwdAnd the output is: /home/tom/Documents/sandboxTo go to your home directory: $ cdThat is with no paremeters. Check where you are: $pwd/home/tomClear the screen: As you type many commands you want to go back to the top. $ clearOr you can also use Ctrl+L. Create a directory: $ mkdir name-of-directoryLet’s see where we are: $ pwd/home/tom/Documents/sandboxLet’s create a directory called videos $ mkdir videosList contents: $ lspictures videosChange the name of a directory: I want to change of a directory from videos to data. $ mv videos/ dataThe formula is from old name to new name. $ lsdata picturesCreate a file inside data: I want to create a new csv file inside the data directory. $ cd data/ &amp;&amp; touch emails. csvThe characters &amp;&amp; are used like this: Run B only if A works A &amp;&amp; BThe touch command is used to create a file. Let’s see where we are: $ pwd/home/tom/Documents/sandboxList the contents: $ lsdata picturesLet’s run that command: $ cd data/ &amp;&amp; touch emails. csvLet’s see where we are now: $ pwd/home/tom/Documents/sandbox/dataList the contents: $ lsemails. csvList of computer processes: To see all the processes running in your computer you can use the Task Manager. In Mac you can use the Activity Monitor. For Linux you can use the following: $ topThis will show all the processes running in real time. To take a snapshot of the processes use: $ ps auxInput and Output: Whenever you type a command and it gives you a result on the screen, this is called the “standard output” aka stdout. You can also redirect this output to a file. The input is called the “standard input” aka stdin. Let’s see where we are: $ pwd/home/tom/Documents/sandbox/dataThe second line above is displayed on the stdout of the Terminal. I want this result to be sent to a file called working_directory. txt $ pwd &gt; working_directory. txtWhen you hit Enter. It will not show any result. List the contents: $ lsemails. csv working_directory. txtAdd to a file: What happened? Using the greater than sign. Sent the output to a new file called working_directory. txt. The way it works is that if such file doesn’t exist. Then create the file. If you open this file, it will have this content: /home/tom/Documents/sandbox/dataThe greater than sign adds to a file. But if you use it again on the same file it will replace the contents. $ ls &gt; working_directory. txtIf you hit Enter here it will not send the result to stdout. If you open the file again you will see that the content has been replaced: emails. csvworking_directory. txtI used ls to list the contents of the current directory and sent this output to the file working_directory. txt. Append to a file: If you don’t want to overwrite the contents of a file using redirection. You should use two greater than signs such as: $ ls &gt;&gt; working_directory. txtSince the file previously had this: emails. csvworking_directory. txtRunning such command will result in this content: emails. csvworking_directory. txtemails. csvworking_directory. txtIt appended to the end of the file. Redirection with Pipe |: You can also redirect the output of one command to the input of another. Previously we saw how to get a snapshot of processes: $ ps auxBut this shows a long list. To show fewer results you can use the less or more commands. Run this command: $ ps aux | lessIt will show the results in a way that you can navigate up and down either using the arrows or the “vim way” with J and K. The pipe is used to sent the output of ps aux to the input of less. Send the contents of a file to stdout: $ cat working_directory. txtThe cat concatenates a file and prints to stdout. What this means is that it opens the file and sends the content to the standard output. Add contents with echo: Let’s see where we are: $ pwd/home/tom/Documents/sandbox/dataList the contents: $ lsemails. csv working_directory. txtOpen the file emails. csv with Sublime. Add this content: first,last,emailelon,musk,elon@tesla. comtim,cook,tim@apple. comSave and close the file. Then go back to the terminal. Let’s add another row to this file like this: $ echo  homer,simpson,homer@aol. com  &gt;&gt; emails. csvUse cat to show the content of emails. csv in stdout. $ cat emails. csvfirst,last,emailelon,musk,elon@tesla. comtim,cook,tim@apple. comhomer,simpson,homer@aol. comCount the number of lines: Open the man page of the command wc. Using the option -l (dash lowercase L). It prints the number of lines. Let’s use a combination of previous commands: $ cat emails. csv | wc -l4I used cat emails. csv to open the file. Used the | pipe to send the output of that to the input of wc -l which is used to count the lines. In this case four lines. "
    }, {
    "id": 220,
    "url": "https://www.tomordonez.com/touch-typing-z-type/",
    "title": "Learn Touch Typing with Z-Type Game",
    "body": "2017/10/01 - Learn touch typing with the Z-type game and become ultra-focused. This is a great skill to have if you are learning Linux or a programming language. Learn touch typing with 8 fingers to get ultra focused. The left hand is placed on the letters ASDF. The right hand is placed on the letters JKL; In the beginning you have to memorize the keyboard. But you always place your hands on the home row, the middle of the keyboard on the keys mentioned above. Every keyboard in the universe is built with 2 dents. F for the left index and J for the right index. To improve your touch typing skills I recommend playing a game called Z-Type. Which is like space invaders but instead of martians you get words. You have to type the words as fast as you can. In the beginning you have to memorize the keyboard, looking down. But eventually if you practice a lot. Say 1hr every day for a month. You will realize that you never have to look at the keyboard ever again. "
    }, {
    "id": 221,
    "url": "https://www.tomordonez.com/create-csv-from-linkedin-sent-invites/",
    "title": "Create a CSV file from Linkedin Sent Invites",
    "body": "2017/09/22 - Follow this process to create a CSV file from Linkedin sent invites. Objective:  Connect with People Get a list of sent invitations Automate “view profile” on browser1. Connect with People: Invite as Friend 2. Get a list of sent invitations: Go to Linkedin People Invites Scroll down the infinite scroll until the bottom. Inspect HTML and copy element &lt;ul class= list-container invites &gt; Create a CSV list: Paste code into Sublime. Go back to LI and get the name of the first person on top of the list and the last person on the bottom of the list. Go to Sublime and search for these names to make sure you got all the HTML code. Find: (&lt;div class= entityblock &gt;)Replace with: \n$1Find: ,Replace with: ;Remove the first row that starts with &lt;ul class= list-container invites &gt; Find: ^. *value= Replace with: emptyFind:   id= checkbox. *?a href= Replace with: ,Find: &amp;amp;. *&lt;span&gt;(pending to review if this is &amp;amp; or &amp;) Replace with: ,Find: &lt;/span. *subheader &gt;Replace with: ,Find: &lt;/p&gt;. *Replace with: emptyA CSV list is born 3. Automate “view profiles” on browser: This part is going to be a little manual. If you want to prioritize some invites then you need to add the date of the invite in the CSV. Go back to LI sent invites and get the invite dates. Add them manually to your CSV (in Excel). You can convert your CSV into XLS if you want. You could also segment your list by priorities. Automating View Profiles with Bash: Note: This requires some coding skills. This process is only half automated and requires some manual work. Perhaps 30min daily. With LI logged in. The process is to open each URL on the browser and then close them. With bash you can automate opening each URL but you have to close them manually. The number of tabs that you can open in Chrome depends on your RAM. If you have 16GB of RAM you could have up to 80 tabs opened. Perhaps you can have lists of 20 URLs for each file and run the script in batches. The script should be something simple like: #!/usr/local/bin/bashwhile read -r linedo open  $line  sleep 10done &lt;  $1 When you create a text file such as input. txt and add say…20 URLs to the file. Then you can run the script like this: Save and name the script with whatever name you want but ending in . sh Such as: awesome-file. sh Now enable the file to be executable. $ chmod +x awesome-file. shNow you can execute the file like this: $ . /awesome-file. sh input. txtThis will open the URLs on the browser. Once they load, you need to manually close them. Automating View Profiles with Python: The problem with the Bash script is that you have to manually close the profiles. If you want to automate opening and closing you need to use Python and Selenium. You need to install Selenium: $ pip install seleniumYou need to install ChromeDriver from here. This downloads a zip file. Unzip. Take a note of where this is installed. You will need this location below. Note: This script is incomplete This Python script opens and closes the profiles but it’s missing Linkedin authentication…to be continued. import osimport timefrom selenium import webdriverchromedriver =  Path to where you downloaded the Chromedriver os. environ[ webdriver. chrome. driver ] = chromedriverdr = webdriver. Chrome(chromedriver)dr. get('URL of profile here')dr. execute_script( $(window. open('URL of next profile here')) )time. sleep(10)dr. close()dr. switch_to. window(dr. window_handles[-1])dr. close()"
    }, {
    "id": 222,
    "url": "https://www.tomordonez.com/blackbelt-sourcing-github/",
    "title": "Blackbelt Sourcing on Github",
    "body": "2017/09/17 - Are you a sourcer or recruiter looking for talent on Github? What is Github?: Github comes from Git + hub. Git is a version control system for software development. A comparison is Google Docs. If you are familiar with Google Docs you know that you can recover a previous version of a file. If you are not familiar with Google Docs versioning, perhaps you are familiar with Sharing an Excel file. You make changes, someone else makes changes and so on. But what if one day someone makes a change and deletes some of your work? End of the world right? What if you could recover a previous version of that Excel file? That would be like discovering fire right? In software development we have a similar scenario. A lot of developers contribute to a project by writing a piece of code. Git, also known as “version control” allows the project owner to revert to a previous version if necessary. Git was created in 2005 by Linus Torvalds, the creator of Linux. Github is not owned by Linus. Github is an independent company that wanted to create Git as software as a service. Github was launched in 2008 and was built in Ruby. It has around 15M users. The Anatomy of Github: I encourage you to open a Github account. Even if you are not a developer. To get an idea of what the product is about. Although you won’t make much sense of it unless you create your own repository. A repository is a project. There are public and private repositories. Most open source projects have a public repository that you can look at. For example Ruby on Rails. Just click around to see what is inside every menu and you will discover a lot of useful information. Batch sourcing: Let’s say that you go to a project and find a few hundred members. You could scrape the list of members and then the profiles. You could scrape them using Bash or Python. Scraping and Regex: Ideally you are doing a scraping using Python and xpath. So that you can get only the nodes in the DOM that correspond to the data that you want. You can use Bash which is not ideal but somehow does the job. Scraping Contributors: Either scrape the content from the source code. Or select the text and Copy Paste the contents into Sublime and use Regex to create a CSV. Scraping Members: The source code for this page reflects the same content that is loaded. You could use wget to scrape it or a Python script. Scraping Profiles: Use a bash script to get the source code. Then use regular expressions to convert data to a CSV. Emails from Hex to ASCII: When you scrape these profiles you will see that the emails are encoded in Hex. Each hex code has a &amp;# followed by an x and 2 digits. First you need to do a replacement: Find: &amp;#(x[0-9a-z]{2});Replace with: \\$1Now those emails should look like this: \x79\x61\x73Reviewing CSV: At this point it should be a good idea to open the CSV file in Excel. The problem with CSV is that the encoding of characters is different. So for instance, accents or letters like ñ will not show up correctly. Open the CSV in Sublime and also create a new Excel file. Remove every column after the first comma, so that you can see only the content of one of the columns displayed. You can use this Regex: Find: ,. *Replace with: emptyCopy paste that column into excel Go back to Sublime and UNDO. Now remove the column that you copied with this Regex: Find: ^. *?,Replace with: emptyRepeat the process with the rest of the columns. Decoding the emails: If you do this transformation: $ echo -e  \x79\x61\x73  | catYou will be able to transform from hex to ASCII. echo -e supports the following escape sequence: \×HHThe eight-bit character whose value is the hexadecimal value HH (one or two hex digits) If you add this to a script you will be able to batch convert all the emails into human readable text. "
    }, {
    "id": 223,
    "url": "https://www.tomordonez.com/regular-expressions-tutorial-regex/",
    "title": "Regular Expressions Tutorial Regex Heaven",
    "body": "2017/09/16 - This is a regular expressions tutorial…regex heaven. Learning regular expressions will save you hours and hundreds of dollars. Problem 1: Cleaning a Massive Excel File:  You have a massive Excel file with user data.  Maybe a product list or a contacts list.  Perhaps you want to send an email to all your contacts. To maximize your reach you might want to make sure all the email addresses are formatted correctly. I have seen this too many times:: johndoe@gmailcommarysmithyahoo. com(not real emails) These emails will bounce because they have missing characters. Problem 2: Cleaning Up Data From Google Searches: Say that you want a list of all countries. You could copy/paste from Wikipedia into Excel and spend 1-2 hrs cleaning up whatever you pasted. That is if Excel doesn’t crash. Since Excel is not great at pasting things from the web. OR You can learn Regular Expressions. A Regular Expression (aka regex) is “a sequence of characters that define a search pattern”:  A sequence of characters.  That define a search pattern. A regex could find characters that make these patterns Example: marysmithyahoo. com:  Find a sequence of characters: any character up to “yahoo” That make a search pattern: any word before “yahoo” that doesn’t have the “@”. Requirements For Regex:  A (good) text editor (not word or google word) Attention to detail (a lot of attention)The Best Text Editors For Regex:  Sublime Text 2 (for Mac and Windows) Vim (if you dare to)Basics of Regular Expressions: Remember the concept:  A sequence of characters.  That define a search pattern. 1. Regex to match a text: Open sublime and copy/paste this: appleapplicationThis is an appanother appleapplecapptain Type Ctrl+H or CMD+H to open the Find and Replace Enable Regex with the button that has a period and star: . * Type: app It matches the pattern “app” everywhere. Even if you don’t have Regex enable it will do the same. So just wait for the magic… 2. Magic Spells of Regex:  \d Matches any digit . A period matches any character \. Used to literally match a period [A-Za-z0-9] Match from A to Z, a to z, 0 to 9 + Match one or more repetitions \s Match any whitespace ^ Match the start of a line $ Match the end of a line3. Please do not drop out yet: I know this sounds like “code”. Might as well be in Martian. Believe me. Regular expressions will save you hours and hundreds of dollars. 4. Get a list of countries from Wikipedia: Say that you need a list of all countries where Spanish is an official language. Open this website in Google Chrome. I know this is a short list. But you could run into larger files to use Regex.  Right click on the list and hit “Inspect Element” Click on the magnifying glass and click on a row on the table.  On the bottom section that shows html code. Click on an element until you find one that encloses the whole table.  Right click on that element.  Copy/ Paste into Sublime5. Use Regex To Clean Up This Code:  Go to Sublime and open Find and Replace Enable the Regex buttonLooking at this code. We need to remove everything up to where the country is. For example. To get “Mexico”. We need to remove all the code until we get to the word “Mexico” I also see that there are some HTML lines that can be easily removed with Find and Replace  Find short HTML line Copy/Paste into Find Replace With: (leave blank) Replace AllThe Find and Replace panels will close. The best way is to learn the shortcuts. Go again to the top menu to see what the shortcuts are for Find and Replace. 6. Remove all code up to the Country: We now see that every line starts with: td style And right before the country there is a: title=  We could use a Regex to remove all this ^. +title. + &gt;Look how awesome this is It found the first match and it outlines all 21 matches. Here is how it works:  ^ goes to the start of the line . matches any character + one or more times title matches the word title . matches any character after the word title “ matches quotes character &gt; matches greater than characterReplace With: Leave empty Replace All. We removed a lot of code. We are almost done. : Let’s remove everything after the less than sign: &lt; &lt;. + &lt; matches less than character . any character (after the &lt;) + one or more timesNow let’s remove the empty lines manually. : Wrong! Another Regex ^\s ^ goes to the start of the line \s matches any whitespaceReplace With: Leave emptyReplace All The final result is a neat list of countries: Using Regex for other things  Clean up a list of emails Clean up a list of products Extract emails from a list Etc…Multiline Regex on Mac: I want to get all the urls from a file that are followed by a line that has an img tag. For example: a href= some-url. html imgOn the Mac this doesn’t work: $ grep 'a href= . *\n. *img'This regular expression should find a href and any character until the end. Then find a new line and find any character on this line until img. But grep doesn’t work like that. Install pcregrep: By default this command tool is not installed on the Mac. $brew install pcreYou can read the man page by looking up: $man pcregrepTo search multiple lines use the option -M Such as: $pcregrep -MNow you can do something like: $ pcregrep -M 'a href= . *\n. *img'"
    }, {
    "id": 224,
    "url": "https://www.tomordonez.com/sourcing-twitter-api/",
    "title": "Sourcing with the Twitter API",
    "body": "2017/09/15 - Follow this process for sourcing with the Twitter API. Getting data using Ruby and the Twitter API Twitter CLI is a command line interface program that allows you to interact with your Twitter account using the command line. Objectives:  Source on Twitter with the Twitter API.  Convert data into a clean list of users.  Get user information.  Find websites, emails, phone numbers.  Find out what users talk about to create a better outreach. Get massive amounts of data with a simple one-liner “formula”. Why?: Perhaps you only source with Linkedin. Perhaps you know boolean searches. A boolean search is like a baby learning to swim. Learning how to source on Twitter with the API is like being Michael Phelps. Do you want to be Michael Phelps? Continue reading. Requirements: This won’t be easy. If it were easy then anybody could do it. This requires:  A general understanding about how computers work.  You need to have: A Twitter account and a Twitter developer app.  You need to have a Windows with a Linux Virtual Machine or a Mac or a computer with Linux.  Ruby installed and some basic Ruby knowledge.  Knowledge of the command line.  Bash scripting knowledge.  A lot of confidence and attention to detail&nbsp;   You need to have a Twitter account: If you don’t have a Twitter account. Something must be wrong in the Universe. You need to have a Twitter developer app: Go to Twitter Application Management Sign-in using your Twitter account. Click on Create New App. Fill out the form.  Name of the app Description Website, including http://Go to Permissions and set Access to “Read, Write and Access direct messages” You might need to authorize your Twitter account with your mobile phone. Go to: Mobile Twitter settings. Towards the bottom it should have a “Phone” listed. If you are on Windows you need Linux in a Virtual Machine: Your computer should have at least 8GB of RAM for this. A Virtual Machine (VM) is used to install an operating system inside another one. Therefore a “virtual machine”. In this case we need to install Linux inside Windows. Install the VM from Virtualbox. Choose Virtualbox for Windows Download and install the defaults. Now download Linux Ubuntu from here. This downloads a file of type . iso DO NOT double click on this file to open it. We will only open this file with VirtualBox. Open VirtualBox:  Click on the button “New” Name: Ubuntu Memory size: 4096 Choose: Create a virtual hard drive now Hard drive file type: VDI (VirtualBox Disk Image) Storage on physical hard drive: Dynamically allocated Select the size of the virtual hard drive: 10. 00 GBA new VM has been created with name “Ubuntu”. With status “Powered Off”. Click on the arrow “Start”. Choose the Ubuntu “iso” file that you downloaded. The VM will load with Ubuntu. Then follow the instructions to install the defaults. Do You have a Mac?: You are almost all set here. Just need to install 2 programs: The text editor Sublime. Download here. The terminal iTerm. Download here. You need to have Knowledge of the command line. : In Windows there is a program called the “Command Prompt”. (We are not going to use this) In Mac or Linux there is a program called the “Terminal” aka “the shell” aka “the bash shell” aka “the command line” This “command line” helps you communicate with the computer by using code. A few basic “command line” examples are: Open the terminal and type this code to see who is the current user logged in: $ whoamiYou don’t need to type in the $ dollar sign. This is the Terminal “prompt” inviting you to talk to the computer. Whenever you see the dollar sign. It means this is a command line code. Type this code to see a list of directories: $ lsType this code to change directories: $ cd DownloadsType this code to copy the contents of one file to another: $ cp this_file to_this_fileThere are a lot more “commands” that you can use to communicate with the computer. Later, I will explain a few more. You need to install Ruby: Ruby is a “high level programming language” that reads sort of like English. Ruby has “modules” called “Gems” that add more functionality to a program. Ruby on Windows with Linux Virtual Machine: Open VirtualBox and start the Ubuntu virtual machine. Open the terminal. Follow this guideline to Install Ruby on Ubuntu. Ruby on Mac: If you have a Mac. The installation process is similar to Ubuntu. You need to Install Twitter CLI ruby gem: Twitter CLI aka t is a Ruby gem that helps you connect with the Twitter API using the command line. Say that out loud 3 times. In the Terminal type: $ gem install tPreviously you created an app on Twitter. We need to authenticate this app with our (Twitter CLI) ruby gem. $ t authorizeThis will say: Welcome!. . . 1. Sign in to Twitter. . .  create a new app 2. Complete the required fields. . . 3. Go to Permissions tab. . . 4. Go to the Keys and Access Tokens tab. . . Press Enter to open the Twitter Developer siteWhen you hit enter then:  Firefox opens Twitter Application Management The terminal says Enter API KeyOpen the app that you created:  Go to permissions tab Go to Keys and access tokensCopy/paste those into the Terminal. Then that opens a site that says “Authorize App”. Then it gives you a pin number that you have to enter in the Terminal. If it works, it will say: Authorization successfulInput and Output to send data: There is another important computer science concept that you need to learn. Input and Output.  When you type into Word. The input is the keyboard and the output is the Word document.  When you want to print the document. The input is the Word document. The output is the printerYou can redirect data in different ways using simple code. Redirecting data with Add and Append: &gt; A greater than sign means  add . &gt;&gt; Two greater than signs mean  append . When you use a command line such as ls to list the contents of your current directory. The output is sent to the “standard output” aka “stdout”. Which is the Terminal screen. Instead we could list the contents of a current directory and send the output to a file. $ ls &gt; directory-contents. txtIn this example we are using ls to list the contents of the current directory. Then we are using &gt; to “add” the output to the file directory-contents. txt. Redirecting data with Pipe: | This symbol is called a  Pipe . With pipes you can redirect the output of one side to the input of another side. $ ls | wc -lIn this example we are using ls again to ask the computer for the contents in the current directory. wc is another command called  word count  that helps you count words from a text file. If you use it like this: wc -l it will count the number of lines from a text file. We use the pipe | to send the output of ls (the contents of the current directory) to the command wc -l to count the number of lines. How to source on Twitter: Open the Terminal. Get a list of all available commands: $ t helpSend a tweet from the command line: $ t update  I am tweeting from the command line Get details about a Twitter user: $ t whois tomordonezCreate a List of People That Don’t Follow You: First, count the number of people that you follow that don’t follow you back: $ t leaders | wc -lWith leaders you can get a list of people that you follow but don’t follow you. With | you are redirecting the output to the command wc -l to count the number of lines. If there is 1 user for each line. Then you will get the number of users. Now create a Twitter list: $ t list create NameoftheListAdd those people to the list. Keep in mind that it might get stuck up to 500 users. Just wait a few minutes and try it again. $ t leaders | xargs t list add NameoftheListIn this example we are using t leaders again to get a list of people that you follow but don’t follow you back. We are sending that output to xargs. Which reads the output line by line and executes the next command t list add NameoftheList, to add every user to that Twitter list. Download Users That Belong To A Twitter list: Let’s get back to the Ruby developer example. The most famous Ruby conference is called “Ruby Conf” and they often use the Twitter hashtag #RubyConf. You can create a Twitter list and add people that use this hashtag. Go to the Terminal and use this code to get a list of all your Twitter lists: $ t lists your-usernameReplacing “your-username” with your Twitter username. With or without the @. It doesn’t matter. Find the Twitter list you created. Let’s say that the Twitter list is called rubyconf. $ t lists tomordonezThis will display all my lists and it might be hard to find the one I am looking for. $ t lists tomordonez | grep 'ruby'This code is using the pipe | to send the output of t lists tomordonez to the input of grep 'ruby'. grep is another command that “filters” data. In this case is filtering the output of all my Twitter lists and find only those that contain the word ruby It will find a list such as: @tomordonez/rubyconfWith the name of the Twitter list now we can download the list of all users that are members of this list. $ t list members -c @tomordonez/rubyconf &gt; twitter-list-rubyconf-members. csvThis code is using the redirection &gt; which sends the output of t list members @tomordonez/rubyconf to the input of twitter-list-rubyconf-members. txt. This code is pulling the list of members that belong to that list and saving them into that csv file. Create a CSV file from a Sourcecon Twitter list: I see that Sourcecon has 5 lists.  &nbsp; &nbsp; The first list is called “SourceCon 2017 Speakers”. It has 26 members. &nbsp; &nbsp; This is the URL of this list: https://twitter. com/SourceCon/lists/sourcecon-2017-speakers What you need to get is the user name and the list name:  User name: SourceCon List name: sourcecon-2017-speakersNow let’s create a CSV file with all the users that belong to that list. t list members -c SourceCon/sourcecon-2017-speakers &gt; sourcecon-2017-speakers. csv These are some of the columns from the CSV file:  Date joined Date of last tweet Number of tweets, favorites, listed, following, followers User name Name Bio Location URL"
    }, {
    "id": 225,
    "url": "https://www.tomordonez.com/redirect-stdout-stderr-bash/",
    "title": "Redirect stdout and stderr in bash",
    "body": "2017/09/14 - This is how to redirect stdout and stderr in bash. Search for:  IO redirection in Linux input redirection Linux output redirection Linux file descriptors bashAs seen here. Redirect stdout to file “static. ” 1&gt;staticRedirect and append stdout to file “static. ” 1&gt;&gt;staticRedirect stderr to file “static. ” 2&gt;staticRedirect and append stderr to file “static. ” 2&gt;&gt;staticRedirect both stdout and stderr to file “static. ” &amp;&gt;staticRedirect stderr to stdout. 2&gt;&amp;1"
    }, {
    "id": 226,
    "url": "https://www.tomordonez.com/linux-execute-many-commands-one-line/",
    "title": "Linux execute many commands in one line",
    "body": "2017/09/13 - Execute many commands in one line in Linux Run A then B, even if A fails A; BRun B only if A works A &amp;&amp; BRun B only if A fails A || B"
    }, {
    "id": 227,
    "url": "https://www.tomordonez.com/execute-script-different-directory-linux/",
    "title": "Execute a Script in a Different Directory in Linux",
    "body": "2017/09/12 - Follow this to execute a script in a different directory in Linux. I had a script in ~/Documents/scripts/awesome-script. sh and wanted to run it inside a different directory ~/Documents/images/awesome-images/. The Subshell: You can use parentheses to create a subshell. Once the command is completed, the subshell will close, such as… $ (cd ~/Documents/images/awesome-images/ &amp;&amp; ~/Documents/scripts/awesome-script. sh)This is how it works:  Use parentheses to create a subshell cd into ~/Documents/images/awesome-images/ If this command is successful then Execute the script located in ~/Documents/scripts/awesome-script. sh When this command is executed. Close the subshell"
    }, {
    "id": 228,
    "url": "https://www.tomordonez.com/windows-10-to-ubuntu-virtualbox-shared-folder/",
    "title": "Windows 10 to Ubuntu VirtualBox Shared Folder",
    "body": "2017/09/11 - Follow this process to transfer files from Windows 10 to Ubuntu in VirtualBox. This applies when you have Ubuntu installed inside Windows 10. Where Windows is the host and Ubuntu is the guest. This assumes that you already have Virtualbox installed with Ubuntu. Follow this process to transfer files from Windows 10 to Ubuntu in VirtualBox. In Windows 10, open Virtualbox… Install Guest Additions: If you haven’t installed guest additions… On the Virtualbox VM (virtual machine). In one of the menu drop downs there is an option that says “guest additions”. When you click on that, it should open a terminal on Linux. Follow the process that is shown there. When that is completed, shut down the VM. Go to Shared Folder: Go to the VM settings. Then shared folders.  Add Folder Use the drop down to lookup a folder (and create one) Check: Auto-mount OK and OK to update changesPlease note your-folder-name since you will need this soon. Let’s say that you call this folder win10-ubuntu Start the VM. Go to Ubuntu and review the shared folder: Open a folder and you will see on the left side that a shared folder has been mounted. Following the example above. It would say sf-win10-ubuntu. You won’t be able to access the folder using the windows interface, since this directory is own by root. The location of this folder is: /media/sf-win10-ubuntu You can either use sudo to copy files from this directory to your local Linux directory. Such as: $sudo cp local-file. txt /media/sf-win10-ubuntu/If you go back to Windows 10. This file will be on the shared folder you created. Or you can change the ownership and permissions of this directory: $ sudo chown -R youruser:yourgroup /media/sf-win10-ubuntu/On Linux you usually belong to a group that has the same name as your user. For example my user is tom and my group is tom. I would run the previous command like this: $ sudo chown -R tom:tom /media/sf-win10-ubuntu/"
    }, {
    "id": 229,
    "url": "https://www.tomordonez.com/installing-ruby-on-ubuntu/",
    "title": "Installing Ruby on Ubuntu",
    "body": "2017/09/11 - Follow this process to install Ruby on Ubuntu Linux Installing Ruby on Ubuntu: As seen on RVM Go to the terminal in Ubuntu. On the Terminal go to the Menu/Edit/Profile Preferences/Title and Command. Check Run command as a login shell. Close the Terminal and start a new one. Add the public key from RVM $ gpg --keyserver hkp://keys. gnupg. net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3Then run this command to install RVM $ \curl -sSL https://get. rvm. io | bash -s stableIf the result says “GPG signature verification failed”. It says to “try to install GPG v2 and then fetch the public key”: $ gpg2 --recv-keys string-letters-numbersAbove copy/paste the corresponding string that is shown on your terminal. If that still doesn’t work it will say: gpg: keyserver receive failed: No keyserver availableThe next troubleshooting is to enter this: $ command curl -sSL https://rvm. io/mpapis. asc | gpg2 --import -If that works it should say something like: gpg: starting migration. . . gpg: porting secret keysgpg: migration succeededgpg: key. . . public key. . . gpg: Total number processed: 1gpg: . . . . imported: 1Then run the command again to install RVM: $ \curl -sSL https://get. rvm. io | bash -s stableIf it works it should say something like: Installing RVM to /home. . . Adding RVM PATH line to . . . Adding rvm loading line to . . . * To start using RVM you need. . . Close the terminal and open it again. $ source ~/. rvm/scripts/rvm$ type rvm | head -n 1This should now say rvm is a function Install Ruby. As of this writing the stable version was 2. 4. 2 $ rvm install 2. 4. 2$ rvm use 2. 4. 2 --defaultUse Gemsets: To work on different projects create a Gemset. $ rvm use 2. 4. 2@name-of-project --createList gemsets with: $ rvm gemset listSwitch gemsets with: $ rvm gemset use name-of-gemset"
    }, {
    "id": 230,
    "url": "https://www.tomordonez.com/generate-random-number-linux/",
    "title": "Generate a Random Number with Linux and Bash",
    "body": "2017/09/10 - Follow this tutorial to generate a random number with Linux and Bash. This is what I was doing to give you some context. I wanted to automate loading images into a funnel to get them auto-published on a social media channel. Pictures of vintage cars. Each car had 4-5 images. Front, back, side, inside, etc. Source: I didn’t want the images to be published sequentially like this: car1-image1. jpgcar1-image2. jpgcar1-image3. jpgcar2-image1. jpgcar2-image2. jpgcar2-image3. jpgI loaded the images in a different order such as: car2-image2. jpgcar1-image3. jpgBut the tool wouldn’t publish the images like that. It would publish them in alphabetical order. Solution 1: Rename 600 images manually to a random number such as: Rename car1-image1. jpg to image20. jpgRename car1-image2. jpg to image5. jpg …and so on. Estimated time: 4 hours. Solution 2: Creating a bash script that can batch-rename files in Linux. Estimated time: 10 seconds. Batch Rename Files in Linux: This is the script I wrote. I will follow with more details. #!/bin/bashfor file in *. jpg; do number= $((($RANDOM % 10) + 1))  filetype= . jpg  mv  $file   $number$filetype doneThe Random function in Linux: $RANDOM is a shell variable that gives you a random number between 0 and 32768. This is some of the source code as seen here: /* Returns a pseudo-random number between 0 and 32767. */static intbrand (){#if 0 rseed = rseed * 1103515245 + 12345; return ((unsigned int)((rseed &gt;&gt; 16) &amp; 32767)); /* was % 32768 */#else /* From  Random number generators: good ones are hard to find ,   Park and Miller, Communications of the ACM, vol. 31, no. 10,   October 1988, p. 1195. filtered through FreeBSD */ long h, l; if (rseed == 0)  seedrand (); h = rseed / 127773; l = rseed % 127773; rseed = 16807 * l - 2836 * h;#if 0 if (rseed &lt; 0)  rseed += 0x7fffffff;#endif return ((unsigned int)(rseed &amp; 32767)); /* was % 32768 */#endif}Limit The Random Number To A Range: To limit the random number you need to use the modulo operator aka %Such as: number= $((($RANDOM % 10) + 1)) Try this from the inside out so you understand what works and what doesn’t. Run this: echo $RANDOMI got this 29811 $RANDOMThis will give you something like this: 21422: command not found. It will give you a random number but it will complain. If you do this: RANDOMIt will say RANDOM: command not found So we know we cannot use RANDOM without the dollar sign. $RANDOM + 1This will give you something like: 24349: command not found. It looks like is doing the math but it complains. Although the proper way to print something to output is with echo. Doing math on the shell: The proper geek name for this is arithmetic expansion. To do math on the shell you have to enclose the operation in double parenthesis. Such as: echo  $((2+2)) This will result in 4 If you do this: echo 2+2You will get: 2+2 If you do this: echo (2+2)You will get: -bash:syntax error near unexpected token '2+2' If you do this: echo ((2+2))You will get: -bash:syntax error near unexpected token '(' If you do this: echo $((2+2))You will get: 4 But the proper syntax for naming variables is  $ . A dollar sign inside double quotes. Now for sure we know this will work: echo  $((4-2)) Modulo operator: Wikipedia is an expert at explaining things in common terms. Check this out: Given two positive numbers, a (the dividend) and n (the divisor), a modulo n (abbreviated as a mod n) is the remainder of the Euclidean division of a by nGot it right? They make it up to you with 2 simple examples:  “5 mod 2” would evaluate to 1 because 5 divided by 2 leaves a quotient of 2 and a remainder of 1.  “9 mod 3” would evaluate to 0 because the division of 9 by 3 has a quotient of 3 and leaves a remainder of 0. The modulo operator results in the remainder of the operation. Do this: echo  $((4 % 2)) . This will result in 0. 4 divided by 2 results in 2 and a remainder of 0. Now do this: echo  $((2 % 4)) . This will result in 2. 2 divided by 4 results in 0 and a remainder of 2. If the number on the left is smaller than the number on the right. The remainder will always be the number on the left. echo  $((1 % 10)) &lt;/code&gt;. Will result in &lt;code&gt;1, because 1 divided by 10 is 0 with a remainder of 1. Here is a good video about long division because as simple as this math may seem, it can easily get you all confused. Generate a Random number within a range: So… How does this work?  $((($RANDOM % 10) + 1)) This generates a random number from 1 to 10. Let’s take it from the inside out: echo  $(($RANDOM % 10)) If $RANDOM gives you this number: 22064. Then the result is 4. Let’s do some long division. 22064 divided by 10. We know the answer is 2206. 4 and the remainder is 4. But sometimes this computation is not so easy. For instance 22064 divided by 24. Long Division of Random Number 22064 by 10:  _______10 | 22064Does 10 fit into 2? No.    0  _______10 | 22064Does 10 fit into 22? Yes. 10 times 2 is 20. Subtract 20 from 22. Remainder is 2.    02  _______10 | 22064  -20  ---   2Does 10 fit into 2? No. So bring down the 0. Does 10 fit into 20? Yes. 10 times 2 is 20. Subtract 20 from 20. Remainder is 0.    022  _______10 | 22064  -20  ---   20   -20   ---    0Does 10 fit into 0? No. Bring down the 6. Does 10 fit into 6? No. Bring down the 4. Does 10 fit into 64? Yes. 10 times 6 is 60. Subtract 60 from 64.    02206  _______10 | 22064  -20  ---   20   -20   ---    064    -60    ---     4The remainder is 4. This operation will result in 4 echo  $((22064 % 10)) Therefore this operation will result in 64 echo  $((22064 % 100)) We can conclude that to generate a random number from 0 to 10. We have to put a 10 on the right side of the modulo. Or to generate a random number from 0 to 100. We have to put a 100. Going back to this example: echo  $(($RANDOM % 10)) What if we don’t want a 0. Which would be the case if the number ($RANDOM) on the left (dividend) is a multiple of 10 (divisor) If $RANDOM is 10 or 20 or 50 or 100. Then the operation above will result in a remainder of zero. We can just add a one 1 to the result of the operation. Such as:  $((($RANDOM % 10) + 1)) This will result in 1: echo  $(((50 % 10) + 1)) . Why are there 3 parenthesis?: This is the 1st parenthesis: (50 % 10)This is the 2nd parenthesis: ((50 % 10) + 1)The 3rd parenthesis is used for the arithmetic expansion (((50 % 10) + 1))"
    }, {
    "id": 231,
    "url": "https://www.tomordonez.com/crontab-selenium-chrome-driver/",
    "title": "Crontab with Selenium and Chrome Driver",
    "body": "2017/09/09 - This is how to setup a crontab with Selenium and Chrome Driver If you are here I assume you have some knowledge of cron. Otherwise read this cronjob tutorial. Let’s pretend that we have a Python script in this directory: ~/Documents/scripts/awesome. pyJust for reference. You should use this for troubleshooting purposes:: Review the system log file here: /var/log/syslog Send the stderr and stdout to a file like this. Right after the last crontab command: &gt; awesome. log 2&gt;&amp;1Read more about the redirection syntax here. Edit your crontab: Edit your crontab with crontab -e In this example you run a job every day at 7:45am. 45 7 * * * export DISPLAY=:0 &amp;&amp; cd /home/your-username/Documents/scripts/ &amp;&amp; /usr/bin/python awesome. py &gt; awesome. log 2&gt;&amp;1To learn what 45 7 * * * means. Go to this tutorial about automating tasks with crontab. Since cron runs with a limited number of environment variables. You need to set the DISPLAY variable. This explains more about the DISPLAY variable. DISPLAY consists of a keyboard, a mouse and a screen. The display is managed by the X server. If I want to open Sublime in Linux. X server is the framework that helps me display the program into a window. If I move the mouse then it helps me go to an exact pixel in the screen. Lookup more details on Wikipedia as X Window System. “The X server receives input from the keyboard and mouse and displays to a screen. A web browser and a terminal run on the user’s workstation. ” If you do this on your terminal: echo $DISPLAYMost likely you will get: :0Which means that DISPLAY variable is set to :0. If you do this on your terminal: env | grep 'DISPLAY'You will get: DISPLAY:=0Other examples of DISPLAY could be:  DISPLAY=localhost:4 DISPLAY=google. com:0The syntax for DISPLAY is hostname:D. S which means screen S on display D of host hostname. hostname:D. S hostname is the computer name where the X server is running. If there is no hostname then it means the localhost.  D is a sequence number. Usually 0. It can be different if the computer has many displays connected.  S is the screen number. 0 is the default. When you echo $DISPLAY. The hostname is omitted and it shows only the default DISPLAY number: :0. Crontab and DISPLAY: Selenium opens the browser if you are using the Chromedriver. Cron doesn’t have the DISPLAY environment variable set. You need to set the DISPLAY variable inside the crontab such as: export DISPLAY=:0Going back the example where you run a job every day at 7:45am. 45 7 * * * export DISPLAY=:0 &amp;&amp; cd /home/your-username/Documents/scripts/ &amp;&amp; /usr/bin/python awesome. py &gt; awesome. log 2&gt;&amp;1You are setting the DISPLAY variable before executing any command. You are using the full path to where the files are. And you are sending stdout and stderr to a log file for troubleshooting. "
    }, {
    "id": 232,
    "url": "https://www.tomordonez.com/automating-tasks-crontab/",
    "title": "Automating Tasks with Crontab",
    "body": "2017/09/08 - Automating tasks with crontab can save you hours of work. This is a short tutorial about cron jobs. Automating tasks: These are a few tasks that need automating:  Every day I have to launch Chrome and open the exact same 20 URLs.  Every other day I have to empty my Recycle bin.  Every Monday morning I have to send a tweet that says “Happy Monday”.  Some days I have to open a file. Filter the contents and send the result to another file.  I also would like to tweet a random quote from a list of quotes every day at noon.  I have my left Ctrl key remapped to my Caps Lock key. When I reboot, it goes back to the default setting. I need to change this every time. These are repetitive tasks that have a formula:  Start Step 1 Step 2 Step 3 FinishThese tasks can be automated. Automating tasks with Crontab: As seen here. Cron is a system daemon. Not a demon. A daemon is used to execute tasks in the background at specific times. Not in an evil way. Cron is like Wall-E: Cron is like having a robot do your routine tasks. Crontab is a text file: Cron is a daemon. A process that runs tasks. Which tasks? Any task that you add to a text file call the crontab. Each user has a crontab. You don’t have to login as root to run this text file. Edit the Crontab: In a terminal run this: crontab -eThis will open the crontab. The file has a few comments to help you get started: 1 # Edit this file to introduce tasks to be run by cron. 2 # 3 # Each task to run has to be defined through a single line4 # indicating with different fields when the task will be run5 # and what command to run for the task6 # 7 # To define the time you can provide concrete values for8 # minute (m), hour (h), day of month (dom), month (mon),9 # and day of week (dow) or use '*' in these fields (for 'any'). # 10 # Notice that tasks will be started based on the cron's system11 # daemon's notion of time and timezones. 12 # 13 # Output of the crontab jobs (including errors) is sent through14 # email to the user the crontab file belongs to (unless redirected). 15 # 16 # For example, you can run a backup of all your user accounts17 # at 5 a. m every week with:18 # 0 5 * * 1 tar -zcf /var/backups/home. tgz /home/19 # 20 # For more information see the manual pages of crontab(5) and cron(8)21 # 22 # m h dom mon dow  command23 Crontab syntax: Follow this syntax: m h dom mon dow  commandWhere:  m = minute (0-59) h = hour (0-23, 0=midnight) dom = day of month (1-31) mon = month (1-12) dow = day of week (0-6, 0=Sunday)You can use a * for any minute or any hour or any day of month or any month or any day of week such as: * * * * * Run at 7:45 am. any day of month, any month, any day of week:45 7 * * * Run at 7:45 am. The 1st day of month, any month, on Monday:45 7 1 * 1 45 = minutes 7 = hour 1 = day of month * = any month 1 = day of week (Monday)It becomes more interesting… Run at 7:45 am. Any day of the month, any month, on Monday, Wednesday and Friday: 45 7 * * 1,3,5Run at 7:45 am. From the 1st to the 15 of November and December: 45 7 1-15 11,12 *Run every 30 days at 8:30am: 30 8 */30 * *Fun right? Crontab command: I know what you are thinking…automate everything right? Here are some guidelines:  Use full paths to files Environment variables are not loaded Crontab doesn’t behave like bash Leave a new (empty) line at the end of crontab Follow troubleshooting guidelines below Automate E. V. E. R. Y. T. H. I. N. GRemap Ctl key to Capslock at reboot: @reboot setxkbmap -option caps:ctrl_modifierOK so cron is not so greedy…got the joke? …Cron has some special strings:  @reboot: Run once at startup @daily: Run once a day aka 0 0 * * * @hourly: Ronceour…I made up that word. Run once an hour aka 0 * * * * @weekly: Run once a week aka 0 0 * * 0You also have @yearly and @monthly Launch Python Script with Selenium and Chromedriver: Follow this tutorial: crontab with selenium and Chrome Driver for all the geek details. It runs a python script at 7:45am every day and send stdout and stderr to a log file. 45 7 * * * export DISPLAY=:0 &amp;&amp; cd /home/your-username/Documents/scripts/ &amp;&amp; /usr/bin/python awesome. py &gt; awesome. log 2&gt;&amp;1See Troubleshooting guide below to read more about “stdout and stderr to a log file”. Empty the Trash every day at 6pm: In Linux you can empty the trash using the command line. sudo apt install trash-cliTo read the manual page use: man trash-cliTo empty trash use: trash-emptyIn the crontab use: 0 18 * * * trash-emptyEvery Monday morning I have to send a tweet that says “Happy Monday”. : I use a Ruby gem to connect to the Twitter API from the command line. 15 8 * * 1 /usr/local/bin/t update  Happy Monday Troubleshooting Cron: I use 2 steps to troubleshoot the cron jobs:  Review /var/log/syslog Redirect stdout and stderrReview /var/log/syslog: Every time you edit your crontab you will see something like this in the syslog file: Dec 5 16:32:43 your-computer crontab[6475]: (your-user) BEGIN EDIT (your-user)Dec 5 16:33:59 your-computer crontab[6475]: (your-user) END EDIT (your-user)For my trash-empty I will get something like: Dec 5 18:00:01 my-computer CRON[28030]: (my-user) CMD (trash-empty)Inside syslog you can at least see if the cron job is running. Redirect stdout and stderr: It might happen that you schedule the cron job but it doesn’t do anything. If you go to syslog you might see that the task ran but yet it didn’t do anything. Get more details of input/output redirection here. You should know the basics: One greater than sign &gt;:  Redirect stdout to a file If the file doesn’t exist then create it. Otherwise overwrite current fileTwo greater than signs &gt;&gt;  Redirect stdout to a file If the file doesn’t exist then create it. Otherwise append to current fileRedirect stdout to a file: 1&gt;file. txtRedirect stderr to a file: 2&gt;file. txtRedirect stdout and stderr to a file: &amp;&gt;file. txtRedirect stderr to stdout and add to a file: 2&gt;&amp;1Every Monday send a Tweet v2: If I want to troubleshoot this cron job. I need to add redirection of stdout and stderr to a file. Then I can open the file to see if the command generated an error. Every Monday morning send a tweet that says “Happy Monday”. 15 8 * * 1 /usr/local/bin/t update  Happy Monday  &gt;&gt; /home/tom/Documents/crontom. log 2&gt;&amp;1 15 = minutes 8 = hour * = any day of month * = any month 1 = day of week (Monday)I could open the file crontom. log and see if it redirected a correct stdout or if it generated a stderr. Backup your crontab:  Your crontab is located at /var/spool/cron/crontabs/your-username You can create a cron job to backup your crontab. Say that 3 times fast.  You cannot just add this job to your crontab because this directory requires root access. You need to add this job to your root crontab: sudo crontab -eAnd then add a line such as backing up the file at 2:05am. 5 2 * * * cat /var/spool/cron/crontabs/your-username &gt; /home/your-username/Documents/crontab_backup. txt 2&gt;&amp;1If you open the file crontab_backup. txt. You will see that a new line is added at the top that says: # DO NOT EDIT THIS FILE - edit the master and reinstall. If you look at the permissions of this file it will show: -rw-r--r-- 1 root rootWhich means that the file is owned by root and it’s set to read only to your-username How to kill a cron job: Find the PID with: ps auxIn this example I scheduled a cron job for 7:45 am to run Python tom   20979 0. 0 0. 0 43092 15168 ?    S  07:45  0:00 /usr/bin/python awesome. pyIf you already knew the PID you could also do ps -p PID -fWhich would give something like: UID    PID PPID C STIME TTY     TIME CMDtom   20979 20978 0 07:45 ?    00:00:00 /usr/bin/python awesome. pyThen you can kill the cron job with: kill PIDIn my case I would do: kill 20979 "
    }, {
    "id": 233,
    "url": "https://www.tomordonez.com/ubuntu-wifi-network-disconnected-after-sleep/",
    "title": "Ubuntu Wifi Network Disconnected After Sleep",
    "body": "2017/09/07 - Ubuntu Wifi disconnected after sleep. This is how I fixed this problem. I got a Thinkpad and I thought I switched off the Wifi. The right side of the laptop has a switch to turn the Wifi on and off. That didn’t work. I went to the settings and I disabled/enabled the Wifi. That didn’t work. I saw a great solution here. Assemble the minions: Open the terminal and run this: sudo systemctl restart network-manager. serviceLet this witchcraft take effect. Happiness unlocked. If that didn’t work. You can try one of these solutions: Check if this file has the setting managed to false $ cat /etc/NetworkManager/NetworkManager. confThe output could be: [main]plugins=ifupdown,keyfile,ofonodns=dnsmasq[ifupdown]managed=falseSet managed to true $ sudo vim /etc/NetworkManager/NetworkManager. confEdit: managed=trueThen run: sudo service network-manager restartOther solution: Follow this thread for another solution. This thread describes the problem when trying to connect to wifi, it would ask for the password over and over again. It also describes the problem when trying to connect to wifi via an ad-hoc network. The thread goes into several infinite rabbit holes. If you have a definite solution please add a comment below. Ubuntu official documentation: This is the official documentation about Ubuntu - wireless network disconnecting. It shows 4 basic “solutions”. Although I would call them more: riddles for you to figure it out:  Weak wireless signal Network connection not being established properly Unreliable wireless drivers Busy wireless networksNot great solutions but I would say: information to think about why your Ubuntu wireless network keeps disconnecting. There is also the Ubuntu wireless network troubleshooter. And the official documentation to connect to a wireless network in Ubuntu. "
    }, {
    "id": 234,
    "url": "https://www.tomordonez.com/disable-touchscreen-ubuntu/",
    "title": "Disable Touchscreen on Ubuntu",
    "body": "2017/09/05 - Follow this process to disable touchscreen on Ubuntu. Go to the terminal and type man xinput It says that xinput is a utility to configure and test X input devices. It has a lot of options but the one we are interested in is list Then do this: xinput --listIt will show you something like this: Virtual core pointer              id=2  [master pointer (3)]⎜  ↳ Virtual core XTEST pointer        id=4  [slave pointer (2)]⎜  ↳ SynPS/2 Synaptics TouchPad        id=13  [slave pointer (2)]⎜  ↳ Raydium Corporation Raydium Touch System id=15  [slave pointer (2)]⎣ Virtual core keyboard             id=3  [master keyboard (2)]  ↳ Virtual core XTEST keyboard        id=5  [slave keyboard (3)]  ↳ Power Button               id=6  [slave keyboard (3)]  ↳ Video Bus                 id=7  [slave keyboard (3)]  ↳ Video Bus                 id=8  [slave keyboard (3)]  ↳ Sleep Button               id=9  [slave keyboard (3)]  ↳ Integrated Camera             id=11  [slave keyboard (3)]  ↳ AT Translated Set 2 keyboard       id=12  [slave keyboard (3)]Find the touchscreen system id and disable it. I initially got confused between choosing Touchpad and Touch System. I looked up Raydium Corporation and indeed they do touch screens. In my case the id is 15 So I did this: xinput disable 15If you feel like you made a mistake and cannot live without the touchscreen then just do the opposite: xinput enable 15Using obviously the id that corresponds to your config. Other resources to disable touchscreen on Ubuntu:  Disable touchscreen on Ubuntu 18. 04 Disable touchscreen on Dell Inspiron and Ubuntu Disable touchscreen on ASUS and Ubuntu 16. 04"
    }, {
    "id": 235,
    "url": "https://www.tomordonez.com/test-your-site-before-it-goes-live/",
    "title": "Test Your Site Before It Goes Live",
    "body": "2017/09/04 - This is a very interesting tool to preview your site before it goes live. I have been using this tool when migrating sites from one hosting to another. hosts. cx You need to enter:  New server IP address Website name. You will get a URL. When you preview the site some things might not display correctly, such as images, since they are cached. "
    }, {
    "id": 236,
    "url": "https://www.tomordonez.com/bash-script-tutorial/",
    "title": "Bash Script Tutorial",
    "body": "2017/09/03 - In this bash script tutorial you will learn the basics to create bash scripts. Bash Script Environment: In this bash script tutorial I am using Linux. But you can also use a Mac and perhaps Windows. Bash Script Linux: My preference is creating bash scripts in Linux. Although I have the feeling that if you are in Linux you already know some Bash :) If you are not using Bash. Maybe you know some Linux commands? If not no problem. First find where bash is installed in your system. Open the terminal and type: which $BASHIt might say: /bin/bashIf you are on Mac it might say: /usr/local/bin/bashKeep this in mind for later. Vim Text Editor: I love Vim because it has a mouse free environment and helps you become hyperfocused. Although Vim has kind of a steep learning curve. Installing Vim on Ubuntu Linux: sudo apt install vimInstalling Vim on Fedora Linux: sudo dnf install vimInstalling Vim on Mac: brew install vimAlthough keep in mind that for the Mac to use brew you need to install homebrew. Just go to Homebrew and copy paste the command shown on the homepage into the terminal. Sublime Text Editor: Sublime is less geek and it looks just like any program. You can download it from here. Bash Script Windows: What I recommend is to install Linux using Virtualbox. A video will soon be posted here. Bash Script Basics: Create a new file either and save it as awesome-file. sh. Yeah it ends with . sh. In the first line type in the location of bash. For Linux bin bash is often: #!/bin/bashFor Mac bin bash is often: #!/usr/local/bin/bashThe character #! is called a shebang or a hashbang. What follows the shebang is the location of bash binaries. In other words it tells the file what you want to execute the file with Bash. Bash script simple silly example: It’s downhill from here :) Below the bin bash line enter a new line and add this line: echo  Hello With bin bash on Linux: #!/bin/bashecho  Hello Or with bin bash on Mac: #!/usr/local/bin/bashecho  Hello Save and close the file. Change the bash script file to execute: Go back to the terminal and change directories to where the file is. Type this command to make the bash script executable: chmod +x awesome-file. shExecute this simple silly bash script example: On the terminal type this to execute this bash script example: . /awesome-file. shThis means to execute in the current directory. The result should be: HolaThese are other advanced examples: How to declare variables and initialize some to zero. declare -i filenumber=0declare -i sponsornumber=0declare -i counter=0declare -i directory_sizeAssign a string to a variable: filetype= . jpg Calculate the directory size where the files are stored: directory_size= $(ls -l . / | grep 'jpg' | wc -l) Rename files in the directory to a random number: for file in *. jpg; do number= $RANDOM  mv  $file   $number$filetype doneUse a random number within a range: Random number from 1 to 10: number= $(((RANDOM % 10) + 1)) Random number from 1 to 100: number= $(((RANDOM % 100) + 1)) Rename files into a sequential number 1, 2, 3…etc: for file in *. jpg; do filenumber= $(($filenumber + 1))  mv  $file   $filenumber$filetype done"
    }, {
    "id": 237,
    "url": "https://www.tomordonez.com/review-programming-for-everybody-coursera/",
    "title": "Review of Programming for Everybody in Python from Coursera",
    "body": "2017/09/02 - I have been on a data science quest on and off for about a year and a half. About a month ago I decided to accelerate my progress and I signed up to Python for Everybody in Coursera. I implemented some Python scripts in the past and this website uses Python. But I wanted to get some detailed foundations before moving on to using Python for data science. The first course in the Python for Everybody path is called “Programming for Everybody (Getting Started with Python)”. It’s the typical intro to programming. They teach you basic syntax, types, conditionals and iterations. They also teach you handling exceptions, which often is a more advanced topic but I am glad this was taught early on. Programming for Everybody in Python Curriculum: Here is the curriculum for programming for everybody in Python:  Week 1: Why we program Week 2: Installing and using Python Week 3: Why we program (continued) Week 4: Variables and expressions Week 5: Conditional code Week 6: Functions Week 7: Loops and iterationI completed this course in about a week. Spent around 3-4 hours per day for 7 days. If you are familiar with C programming you can do this course in less than a week too. If you know what this is, you will find this course very easy: for (i = 1; i &lt;= 3; i++)If you don’t have previous knowledge of programming I suggest that you take each week and spend time to get the knowledge to sink in correctly in your brain :) Programming for Everybody in Python Example: Here is a Python example related to programming for everybody. Let’s write a silly program that asks for the prices of items and calculate the total, the lowest and highest price. Also print an error when you don’t enter a number and finish when you enter the word “batman”. lowest = Nonehighest = Nonecount = 0total = 0while True:  price = input('Enter price: ')if price == 'batman'  breaktry:  price = float(price) except:  print('Master Bruce please enter a number!')  continue  total = total + pricecount = count + 1if highest is None or price &gt; highest  highest = priceif lowest is None or price &lt; lowest  lowest = priceprint('Total is:', total)print('Number of items:', count)print('Highest price:', highest)print('Lowest price:', lowest)Running the program shows: The example uses all the lessons from the course with the exception of the for loop:  while try and except if conditional input"
    }, {
    "id": 238,
    "url": "https://www.tomordonez.com/ms-data-science-coursera-udemy-udacity/",
    "title": "MS in Data Science with Coursera, Udemy, Udacity",
    "body": "2017/07/23 - I have been studying data science for the last few months. I informally started last year when I signed up to the Machine Learning course by Coursera taught by Andrew Ng. My first impression was that it reminded me of Matlab. It had a lot of math and linear algebra. I figured that it was going to take a lot of motivation to finish this class. When I went to engineering school I spent a lot of time learning advanced math, electronics, computer design, Matlab, machine code, Java and some C. I figured that this Machine Learning class would take a lot of effort and motivation and I put it on hold. Then I read these books::  Predictive Analytics by Eric Siegel Data Analytics by Anil MaheshwariIt wasn’t until January 2017 that I really got officially motivated to learn faster. I read that a group of developers were downloading massive data sets on a race to save climate data. I signed up to “Executive Data Science” on Coursera and took these classes::  A crash course in data science Building a data science team Managing data analysisThe first 2 courses were sort of simple. The last one “Managing data analysis” took a little bit more than the average amount of grey matter. It took me about 2 months to complete this class because I wanted the concepts to really sink in. Managing Data Analysis follows this iteration::  Set expectations Collect data Revise expectationsAnd it is divided into these sections:  State and refine the question Explore the data Build formal models Interpreting results Communicating resultsDuring “Explore the data” there was an example that required R. I knew what R was but I never used it. It took me about 1-2 weeks to get into the R mindset to really understand the concepts that were not explained in the example. names(ozone) &lt;- make. names(names(ozone))This looks like a very simple line. But the example didn’t explain anything about it. Over the years managing engineering projects I learned that copy/paste is your worst enemy. I always go by “never copy paste what you don’t understand”. I used a data set about ozone levels in the US from 2016. I did some exploratory data analysis and created plots for a few cities. This was a great learning experience. During “Build Formal Models” they introduced a lot of new concepts, including associational analysis and prediction analysis. I will add more details about this course in another post. Half through this course I started reading this book:  The Data Science Handbook by Carl Shan and friendsIt has a very interesting story by Clare Corthell, where she explains how she came up with her own “open source” MS in Data Science. For the last few months I have been thinking about a MS in Data Science: I have 2 engineering degrees from FIU and I wasn’t looking to go back to FIU. If I went to get another degree it would be from a top engineering school. I have been thinking about the Masters in Predictive Analytics from Northwestern. But I’ve been also thinking what would be the goal of getting this degree. I know that the future of everything will be based on data science but I am not exactly sure what character I want to play in this future. I have been growing technical teams for the last year. Currently focused on growing data science teams so I figured on following the Open Source MS in Data Science for now and apply this knowledge to my projects. There is also a lot of knowledge I have been postponing for…perhaps not lack of motivation but just…life. There are always a million excuses for not doing something. I figured that life is short and instead of finding excuses, perhaps is better to just learn new things. My MS in Data Science curriculum: A few weeks ago I learned about the Data Science Venn Diagram. When I looked at it I figured that I have been following this for the last few years. A good data scientist should have these skills:  Coding skills Math and statistics skills Business domain skillsActually not only a data scientist should have these skills. To build the future of everything. Most people that want to build this future should also have these skills. A doctor will make diagnostics based on data. To really trust on the data, she (or he…I will use she from now) would need to learn how the recommendation was created. She would need to study math, statistics and programming to really dig deep and understand how it works. A marketer will create a marketing campaign based on data. To trust on the recommendations based on the data, she has to follow the same path. The same with other industries. This comparison is a little bit extreme. There are creators and there are users. Just because you trust in buying online doesn’t mean you have to know how it works. But the future will be based on recommendations and trusting these recommendations will take time until we are “recommendation users”. Coding skills: Early in my career I learned C and Java. Then I got involved in Ruby. For data science you must kick ass in R and Python. I have been learning R and I know some Python but I decided to follow a path. It’s easy to develop a bad technique. Math and Statistics skills: This will be the hard part. Programming is not hard to learn. But math is complicated. I studied a lot of it in engineering college and it took me a long time to understand many math subjects. Business domain skills: I think I am strong here. I don’t have business domain in every subject. But I have many years working on business development using the lean startup and I know how to identify problems and solutions. I signed up for this Coursera path: Python for Everybody from University of Michigan: This class has 4 courses:  Intro to Python Python data structures Using Python to access web data Using databases with Python Retrieving, processing and visualizing dataLater on I might take these Coursera:  Data Visualization with Tableau from UC DavisUdemy:  PostgreSQL MongoDB MEAN stack D3. js"
    }, {
    "id": 239,
    "url": "https://www.tomordonez.com/from-wordpress-to-pelican-python/",
    "title": "From Wordpress to Pelican Python",
    "body": "2017/07/17 - I recently moved from Wordpress to Pelican Python. Pelican is a static generator based on Python. Through the years I have used many platforms. I started in blogger. Then drupal. Then wordpress. Then tumblr. Then squarespace. Then Octopress with Github pages. Back to Wordpress. Then Medium. Back to Wordpress again. I have been studying a lot of data science for the last few months and I am focused on Python now. I initially thought of rebuilding my website from scratch using Python and Django, which could be a fun pet project. But I delayed learning Django. Instead I decided on a static generator based on Python. I asked suggestions from my Linkedin network and a few people suggested Pelican. "
    }, {
    "id": 240,
    "url": "https://www.tomordonez.com/previous-directory-vim-commands/",
    "title": "Go to the Previous Directory in Vim",
    "body": "2017/07/01 - When I open a file and I am done with it. I often want to go back to the previous directory. I usually do a :w and then :q. Vim Command to Go to the Previous Directory: If you want to go to the previous directory do :Ex"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>

<div class="search">
    <form onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
        <p><input type="text" class="form-control" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Search" /></p>
    </form>
</div>
<div id="lunrsearchresults">
    <ul></ul>
</div>
        <div class="home"><h2 class="post-list-heading">Posts</h2>
    <ul class="post-list"><li><span class="post-meta">May 7, 2018</span>
        <h3>
          <a class="post-link" href="/sqlite3-cheatsheet/">
            SQLite3 CheatSheet for Python
          </a>
        </h3><p>This is an ongoing SQLite3 cheatsheet for Python.</p>
</li><li><span class="post-meta">Apr 25, 2018</span>
        <h3>
          <a class="post-link" href="/best-time-to-run-according-to-science/">
            The Best Time To Run According To Science
          </a>
        </h3><p>What is the best time to run? According to science it depends on the ozone levels where you live.</p>
</li><li><span class="post-meta">Apr 24, 2018</span>
        <h3>
          <a class="post-link" href="/python-lambda-beautifulsoup/">
            Python Lambda and BeautifulSoup
          </a>
        </h3><p>This Python Lambda is a very weird concept. I almost grok it.</p>
</li><li><span class="post-meta">Apr 19, 2018</span>
        <h3>
          <a class="post-link" href="/python-self/">
            Python Self
          </a>
        </h3><p>Understanding <strong>Python self</strong>.</p>
</li><li><span class="post-meta">Apr 16, 2018</span>
        <h3>
          <a class="post-link" href="/url-encoding-python/">
            URL Encoding and Python
          </a>
        </h3><p>This is not a full reference guide. It’s just a quick reminder for URL encoding.</p>
</li></ul>

    <!-- Pagination links -->
    <div class="pagination">
      
        <a href="/page16" class="previous">
          Previous
        </a>
      
      <span class="page_number">
        Page: 17 of 25
      </span>
      
        <a href="/page18" class="next">Next</a>
      
    </div></div>

      </div>
    </main>
    <script>
//open external links in a new window
function external_new_window() {
    for(var c = document.getElementsByTagName("a"), a = 0;a < c.length;a++) {
        var b = c[a];
        if(b.getAttribute("href") && b.hostname !== location.hostname) {
            b.target = "_blank";
            b.rel = "noopener";
        }
    }
}
external_new_window();
</script><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Tom Ordonez</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">{&quot;name&quot;=&gt;&quot;Tom Ordonez&quot;}</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/tomordonez"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">tomordonez</span></a></li><li><a href="https://www.twitter.com/tomordonez"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">tomordonez</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about analytics, data science, software engineering, and other thoughts.</p>
        <p class="rss-subscribe">subscribe <a href="/feed.xml">via RSS</a></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
